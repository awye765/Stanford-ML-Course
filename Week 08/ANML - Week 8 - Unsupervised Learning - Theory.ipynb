{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Week 8 is all about <b>unsupervised</b> learning algorithms.  All previous weeks have focussed on supervised algorithms, i.e. Linear Regression, Logistic Regression, Neural Networks and SVMs.\n",
    "\n",
    "The term \"<b>unsupervised</b>\" refers to the fact the dataset is unlabelled vs. supervised learning where the dataset <i>is</i> labelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering\n",
    "\n",
    "## 2.1. What is Clustering?\n",
    "\n",
    "Clustering is a commonly used unsupervised technique.  It is intended to identify logical clusters of data in a dataset.\n",
    "\n",
    "The most commonly used clustering technique is the <b>K-Means</b> algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. How to implement Clustering: K-Means Algorithm\n",
    "\n",
    "### 2.2.1. Informal Process\n",
    "\n",
    "1. Randomly initialise two points, known as \"<b>Cluster Centroids</b>\" on the dataset graph.  This is the \"<b>Cluster Initialization</b>\" step.\n",
    "\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Cluster1.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "2. For each training sample, determine whether it is closest to either:\n",
    "\n",
    "    (a) the first centroid; or\n",
    "    \n",
    "    (b) the second centroid,\n",
    "    \n",
    "  and assign it to that centroid.  This is known as the \"<b>Cluster Assignment</b>\" step.\n",
    "  \n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Cluster2.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "  \n",
    "  \n",
    "3. Next, for each centrooid we calculate the mean location of all training samples assigned to that centroid and move the centroid to that location.  This is known as the \"<b>Cluster Move</b>\" step.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Cluster3.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "4. Repeat steps (2) and (3) until the centroids cease moving around the graph:\n",
    "\n",
    "<center><b>Cluster Assignment Iteration 2</b></center>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Cluster4.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "<center><b>Cluster Move Iteration 2</b></center>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Cluster5.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center><b>Cluster Assignment Iteration 3</b></center>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Cluster6.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center><b>Cluster Move Iteration 4</b></center>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Cluster7.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center><b>Clustering Complete!</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Formal Process\n",
    "\n",
    "1. <b>Inputs:</b>\n",
    "\n",
    "    (a) The number of desired clusters, $K$; and\n",
    "    \n",
    "    (b) The unlabelled training set, $\\{x^1, x^2, ... x^m\\}$\n",
    "    \n",
    "  The input vector will be $n$ dimensional, not $n+1$ dimensional, because we do not need to add a bias unit $x_0$ unlike with supervised techniques covered previously.\n",
    "  \n",
    "\n",
    "2. <b>Cluster Initialization Step:</b> randomly initialize $K$ cluster centroids, $\\mu_1, \\mu_2, ...\\mu_K$.\n",
    "\n",
    "\n",
    "3. <b>Cluster Assignment Step:</b> repeat for $i$ = 1 to $m$, i.e. for each example the below steps:\n",
    "\n",
    "    (a) $min_k \\| x^{(i)} - \\mu_k\\|^2$, i.e. find the value of $k$ (the specific cluster centroid) that is nearest to each sample.  The bit in $\\|\\|$ represents the <b>normal</b> or <b>norm</b>, which is simply the straight line distance between the training sample and the particular centroid $k$.\n",
    "    \n",
    "    (b) set the result from (a) as the value of $c^{(i)}$.\n",
    "\n",
    "\n",
    "4. <b>Cluster Move Step:</b> for $k = 1$ to $K$ set $\\mu_k :=$ average of points assigned to cluster $k$ afte step (3).  In other words, it updates each centroid's coorindates to the average coordinates from all training samples assigned to it in step 3.\n",
    "\n",
    "    For example if after step (3) the $2^{nd}$ centroid, $\\mu$ has assigned to it training samples $x^{(1)}, x^{(5)}, x^{(6)}$ and $x^{(10)}$ then the step (4) calculation is as follows:\n",
    "    \n",
    "\\begin{align}\n",
    "\\mu_2 = \\frac{1}{4}(x^{(1)} + x^{(5)} + x^{(6)} + x^{(10)})\n",
    "\\end{align}\n",
    "\n",
    "5. <b>Note:</b> if after step (3) a centroid has no training samples assigned to it then we eliminate that centroid, resulting in $K-1$ clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. K-means Optimisation Objective\n",
    "\n",
    "Previously we saw supervised learning algorithms that had <b>optimization objectives</b>, i.e. a need to optimize parameters to arrive at an accurate model for the data.  K-means also has an optimization objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Notation Recap\n",
    "\n",
    "* $c^{(i)}$ = index of cluster 1, 2, ... $K$ to which a particular example, $x^{(i)}$ is currently assigned.\n",
    "\n",
    "\n",
    "* $\\mu$ = cluster centroid $k$ where $\\mu$ is a real number between $1$ and $K$.\n",
    "\n",
    "\n",
    "* $\\mu_c^{(i)}$ = cluster centroid of cluster to which example $x_{(i)}$ has been assigned.  For instance if $x^{(i)} \\to 5$ (i.e. $x^{(i)}$ has been assigned to centroid 5), then:\n",
    "\n",
    "    $c^{(i)} = 5$; and \n",
    "    \n",
    "    $\\mu_c^{(i)} = \\mu_5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### 2.3.2. Optimization Objective\n",
    "\n",
    "This is as follows: \n",
    "\n",
    "\\begin{align}\n",
    "J(c^{(i)}, ..., c^{(m}), \\mu_1, ... \\mu_k) = \\frac{1}{m}\\sum_{m}^{i = 1} \\|x^{(i)} - \\mu_c^{(i)}\\|^2\n",
    "\\end{align}\n",
    "\n",
    "In other words, it is the average of the squared distance between the location of each example $x^{(i)}$ and the location of the cluster centroid to which $x^{(i)}$ has been assigned.\n",
    "\n",
    "Therefore we can explain that:\n",
    "\n",
    "* K-means is tyring to find parameters $c^{(i)}$ and $\\mu_{(i)}$ to minimize this cost function.\n",
    "\n",
    "\n",
    "* The Cluster Assignment step is minimizing $J$ with respect to $c^{(1)}, c^{(2)}$ and so on up to $c^{(m)}$ whilst keeping cluster centroids $\\mu_1$ up to $\\mu_k$ fixed.  I.e. this step does not change the cluster centroid but only the assignments of each training sample to each centroid.\n",
    "\n",
    "\n",
    "* The Cluster Move step is minimizing $J$ with respect to $\\mu^{(1)}, \\mu^{(2)}$ and so on up to $\\mu_K$.  I.e. this step does not change the assignments of each training sample to each centroid but only the centroid itself.\n",
    "\n",
    "\n",
    "* In this way we can say the K-means algorithm takes two sets of variables, (a) the centroid value and (b) the assignments of centroid to training sample, and partitions them into two halves.  It then minimizes $J$ with respect to $c$ (the Cluster Assignment Step) and then minimizes $J$ with respect to $\\mu$ (the Cluster Move Step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Random Initialization of Centroids\n",
    "\n",
    "### 2.4.1. The Recommended Method\n",
    "\n",
    "There is a recommended method to randomly intialize the centroids:\n",
    "\n",
    "* $K$ should be < $m$.\n",
    "\n",
    "\n",
    "* Randomly pick $K$ training examples, i.e. $K$ number of training examples.\n",
    "\n",
    "\n",
    "* Set $\\mu_1, ... \\mu_K$ equal to these $K$ examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Potential Pitfalls\n",
    "\n",
    "K-means does come with a few issues:\n",
    "\n",
    "* K-means can end up converging on different solutions depending on its initialisation.  \n",
    "\n",
    "\n",
    "* K-means can also get stuck converging on local optima.\n",
    "\n",
    "To avoid the above issues, Andrew Ng suggests randomly initializing and clustering multiple times:\n",
    "\n",
    "1. Randomly intialize K-means.\n",
    "\n",
    "\n",
    "2. Run K-means.\n",
    "\n",
    "\n",
    "3. Compute the cost function.\n",
    "\n",
    "\n",
    "4. Pick clustering that gave lowest cost.\n",
    "\n",
    "However, if $K$ > 10, doing multiple random initializations won't help much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Choosing $K$\n",
    "\n",
    "### 2.5.1. Eyeballing the Data\n",
    "\n",
    "Sometimes eyeballing the data can give an intutition as to the number of clusters, but not always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. Elbow Method\n",
    "\n",
    "The elbow method plots the cost of a clustering function against the number of clusters $K$.  This will plot an \"elbow\" shaped graph.  The \"elbow\", i.e. sharp bend, will indicate the desired number of clusters.  For instance:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\elbowWork.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "However, the elbow method is not used that often because the gradient of the curve will often appear very ambiguous with no clear elbow.  For instance:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\elbowNotWork.PNG\" width=\"40%\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3. Business Purposes\n",
    "\n",
    "Sometimes it's better to start with the end in mind.  For instance, returning to the t-shirt manufacturer example the objective may be to provide more general sizes, e.g. XS, S, M, L and XL, vs. fewer. \n",
    "\n",
    "In this way we have another way to inform our decision re size of $K$: ask how many segments does the business objective desire?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. K-means for non-separated clusters\n",
    "\n",
    "K-means also applies well to non-separated clisters, i.e. where the data groupings don't obviously divide into clusters.  \n",
    "\n",
    "Andrew Ng gives the example of a t-shirt manufacturer trying to assess how to group sizes into S, M and L using data of weights and heights for individuals.  K-means can equally be used for this type of challenge.\n",
    "\n",
    "This type of exercise is also common in market segmentation tasks and can be achieved using K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality Reduction\n",
    "\n",
    "## 3.1. What is dimensionality reduction?\n",
    "\n",
    "Another unsupervised learning algorithm is <b>dimensionality reduction</b>.\n",
    "\n",
    "A simple example might be a 2d dataset where it turns out two features are duplicative, e.g. if height of a person is presented in inches as $x_1$ and in cm as $x_2$.  \n",
    "\n",
    "This would mean half the features are redundant.  As such, we can reduce the dimensionality from 2d to 1d, by eliminating $x_1$ or $x_2$ and instead plot the remaining feature 1d as points on a straight line.\n",
    "\n",
    "### 3.1.1. Example\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\PCA1.PNG\" width=\"80%\"/>\n",
    "</p>\n",
    "\n",
    "Re the above:\n",
    "\n",
    "1. <b>2D to 1D:</b> the PCA algorithm finds a vector, $u^{(1)}$, which create a 1D line onto which we project the 2D data from 2D to 1D.  \n",
    "\n",
    "    By doing so, we need only <b>one</b> number to specify the position of a point on $u^{(1)}$, which we call $z^{(1)}$.\n",
    "\n",
    "\n",
    "2. <b>3D to 2D:</b> the PCA algorithm finds 2 vectors, $u_1$ and $u_2$, which creates a 2D plane onto which we project the 3D data from 3D to 2D.\n",
    "\n",
    "    By doing so, we need only <b>two</b> numbers to specify the position of a point with respect to the plane defined by $u^{(1)}$ and $u^{(2)}$, which we call $z^{(1)}$ and $z^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Uses\n",
    "\n",
    "Dimensionality reduction is typically used to:\n",
    "\n",
    "1. Reduce memory and therefore increase computational speed; and\n",
    "\n",
    "\n",
    "2. Improve our ability to visualise the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (\"<b>PCA</b>\") is a method to reduce dimensions per the above objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Goal of PCA\n",
    "\n",
    "With dimensionality reduction from 2d to 1d PCA aims to find a vector $u^{(1)}$ onto which to project the data so as to minimise the <b>projection error</b>, i.e. the distance of each sample to the new line.\n",
    "\n",
    "With dimensionality reduction from > 2d+ to < 2d PCA aims to find $k$ vectors $u^{(1)}, u^{(2)}, ... u^{(k)}$ onto which to project the data so as to minimize the projection error.  E.g. from 3d to 2d would mean PCA finds a 2d plane onto which to map the data from 3d to 2d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. PCA Algorithm\n",
    "\n",
    "1. <b>Mean Normalization:</b> compute the mean of each feature $x_j^{(i)}$ and replace each feature $x_j^{(i)}$ with $x_j - \\mu_j$.  Therefore, each feature now has zero mean.\n",
    "\n",
    "\n",
    "2. <b>Feature Scaling:</b> scale each feature to have a comparable range of values, i.e. $x_j^{(i)} \\leftarrow \\frac{x_j^{(i)} - \\mu_j}{s_j}$, where $s_j$ is the range of values, e.g. the max - min values or standard deviation.\n",
    "\n",
    "\n",
    "3. <b>Covariance Matrix:</b> compute using this formula: $\\Sigma = \\frac{1}{m}\\sum_{i = 1}^{n}(x^{(i)})(x^{(i)})^T$ where:\n",
    "\n",
    "    (a) $m$ is number of samples.\n",
    "    \n",
    "    (b) $n$ is number of original dimensions.\n",
    "    \n",
    "    (c) $k$ is number of desired dimensions.\n",
    "    \n",
    "    (d) $x^{(i)}$ is an $n$ x $1$ vector.\n",
    "    \n",
    "    (e) $X$ is a $m$ x $n$ matrix (row-wise stored examples).  \n",
    "    \n",
    "    (f) The product of $x^{(i)}$ and $x^{(i)}$ will be an $n$ x $n$ matrix, which are the dimensions of $\\Sigma$.\n",
    "  \n",
    "   \n",
    "   \n",
    "4. <b>Eigen Vectors:</b> compute Eigen Vectors of the matrix $\\Sigma$ using a <b>Singular Value Decomposition</b> (\"<b>SVD</b>\") function, which returns three matrices, $U$, $S$ and $V$. We use only the $U$ matrix which:\n",
    "\n",
    "    (a) is also $n$ x $n$; and\n",
    "    \n",
    "    (b) has its columns being exactly the vectors $u^{(1)},...u^{(2)}, u^{(n)}$ that we want in order to plot a 1D+ surface onto which to project the original data.\n",
    "\n",
    "\n",
    "5. Take the desired $k$ number of Eigen Vectors from Step (4), i.e. $u^{(1)}, u^{(2)},..., u^{(k)}$ out of vectors $u^{(1)} - u^{(n)}$.  Add these as columns $u^{(1)}, u^{(2)}, ...u^{(k)}$ in a new matrix $U_{reduce}$\n",
    "\n",
    "\n",
    "6. <b>Calculate $z$:</b> where $z = U_{reduce}^T X$ where:\n",
    "\n",
    "    (a) $U_{reduce}^T$ transposes the column vectors $u$ into row vectors.\n",
    "    \n",
    "    (b) $U_{reduce}^T$ is $k$ x $n$.\n",
    "    \n",
    "    (c) $X$ is $n$ x $1$.\n",
    "    \n",
    "    (d) The product of $U_{reduce}^T X$ is $k$ x $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Reconstructing the Data\n",
    "\n",
    "We can also increase the number of dimensions back to the data's original dimensionality using this equation:\n",
    "\n",
    "\\begin{align}\n",
    "x^{(1)}_{approx} = U_{reduce} \\cdot z^{(1)}\n",
    "\\end{align}\n",
    "\n",
    "However, note we can only return to <b>approximations</b> of our originla data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Choosing the Number of Principal Components $k$\n",
    "\n",
    "PCA tries to minimize the <b>average square projection error</b>, i.e. the distance between the original data $x^{(i)}$ and $x_{approx}^{(i)}$.\n",
    "\n",
    "Typically $k$ is chosen to be the smallest value so that:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\PCAk.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "Another way to say this is \"99% of variance is retained\".  Although Andrew Ng says this statement is confusing so ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5. Useful Resources\n",
    "\n",
    "- https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors \n",
    "\n",
    "\n",
    "- https://www.youtube.com/watch?v=PFDu9oVAE-g\n",
    "    \n",
    "Andrew Ng kept this as high level as the above... hence lack of mathematical detail.  PCA algorithm is also packaged up in many ML libraries so in practice does not require writing from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Useful Resources\n",
    "\n",
    "- http://setosa.io/ev/principal-component-analysis/\n",
    "\n",
    "\n",
    "- https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
