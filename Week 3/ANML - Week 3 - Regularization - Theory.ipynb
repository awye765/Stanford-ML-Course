{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Regularization - Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting the Scene\n",
    "\n",
    "Sometimes the model we create will either <b>underfit</b> or <b>overfit</b> our data, which in both cases impedes the efficiency and accuracy of the model.\n",
    "\n",
    "## 1.1. What are overfitting and underfitting?\n",
    "\n",
    "1. <b>Overfitting</b> means the model matches the training data so closely that the model fails to create new predictions on new data.  It is accurate on the training set but doesn't perform well on new data.\n",
    "\n",
    "\n",
    "2. <b>Underfitting</b> means the model does not even capture the training dataset, so is unlikely to accurately predict values for new data.  \n",
    "\n",
    "### 1.1.1. Linear Regression Example\n",
    "\n",
    "<img src=\"../Images\\logregressionOverfitting2.png\" width=60%>\n",
    "\n",
    "1. Leftmost figure shows fitting a y = θ<sub>0</sub> + θ<sub>1</sub>x hypothesis to the dataset.  This underfits because the data doesn't really lie on a straight line.\n",
    "\n",
    "\n",
    "2. Middle figure shows fitting a y = θ<sub>0</sub> + θ<sub>1</sub>x + θ<sub>2</sub>x<sup>2</sup> hypothesis to the datatset (i.e. we added another feature, x<sup>2</sup>).  This seems to fit most of the data points well.\n",
    "\n",
    "\n",
    "3. Rightmost figure shows fitting a 5<sup>th</sup> order polynomial, y=∑<sup>5</sup><sub>j = 0</sub> θ<sub>j</sub>x<sup>j</sup>.  This fits the dataset perfectly, however, it is likely to fail at predicting new values.\n",
    "\n",
    "\n",
    "As such, we can say (1) is <b>underfitted</b> in the sense the model cannot capture the underlying trend of the data, and (3) is <b>overfitted</b> because it matches the training data so closely it is unlikely to predict values based on new data.\n",
    "\n",
    "### 1.1.2. Logistic Regression Example\n",
    "\n",
    "<img src=\"../Images\\logregressionOverfitting3.png\" width=60%>\n",
    "\n",
    "1. Leftmost underfits.\n",
    "\n",
    "\n",
    "2. Middle best fits.\n",
    "\n",
    "\n",
    "3. Rightmost overfits.\n",
    "\n",
    "## 1.2. When does overfitting vs. underfitting occur?\n",
    "\n",
    "1. Underfitting (aka <b>high bias</b>) occurs when the hypothesis is <b>too simple</b> or uses <b>too few features</b>.  \n",
    "\n",
    "\n",
    "2. Overfitting (aka <b>high variance</b>) occurs when the hypothesis is an <b>overcomplicated function</b> with <b>too many features</b>.\n",
    "\n",
    "## 1.3. How can we avoid overfitting and underfitting?\n",
    "\n",
    "Answer: <b>Regularization</b>.  Essentially we are aiming to find the optimal number of features and keep those features regularized.  Specifically, to avoid overfitting we must:\n",
    "\n",
    "1. <b>Reduce</b> the number of features by:\n",
    "\n",
    "    (a) Manually selecting which features to keep\n",
    "\n",
    "    OR\n",
    "\n",
    "    (b) Using a model selection algorithm (studied later in the course).\n",
    "\n",
    "\n",
    "2. Use <b>Regularization</b> techniques to keep all the features, but <b>reduce the magnitude of parameters θ<sub>j</sub></b> (note: Regularization works well when we have a lot of slightly useful features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How does regularisation work?\n",
    "\n",
    "## 2.1. Regularisation and the Cost Function\n",
    "\n",
    "### 2.1.1. The basic idea\n",
    "\n",
    "Say we have two hypothesis functions from the same data set, i.e. the linear regression example above: \n",
    "\n",
    "1. the first one is $h_\\theta(x)= \\theta_0+\\theta_1x +\\theta_2x^{2}$ and it works well; \n",
    "\n",
    "\n",
    "2. the third one is $h_\\theta(x)= \\theta_0 + \\theta_1x + \\theta_2x^{2}+ \\theta_3x^{3} + \\theta_4x^{4}$ and it suffers from overfitting. \n",
    "\n",
    "As the training data is the same for both (1) and (2), this means (2) must have something wrong in its formula. It turns out that those two parameters $\\theta_3$ and $\\theta_4$ contribute too much to the curliness of the function.\n",
    "\n",
    "<b>The core idea:</b> penalize those additional parameters and make them very small, so that they will contribute less, or even don't contribute at all to the function shape. \n",
    "\n",
    "### 2.1.2. Option 1 - Manually penalise $\\theta$\n",
    "\n",
    "If we manually set $\\theta_3 ≈0 $  and $\\theta_4 ≈ 0$ (in words: set them to very small values, next to zero) we would basically end up with (1), which fits the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Option 2 - Use Regualrization to penalise $\\theta$\n",
    "\n",
    "Rather than manually penalizing $\\theta$, regularization pushes the idea even further: <b>all parameters' values</b> are reduced by some amount, producing a simpler, or smoother hypothesis function (and it can be proven mathematically).\n",
    "\n",
    "For example, returning to the usual linear regression problem of house price prediction. We have:\n",
    "\n",
    "* 100 features: $x_1,x_2,⋯,x_{100}$ (size, number of floors, ...) that produce...\n",
    "\n",
    "\n",
    "* 100 parameters: $\\theta_0, \\theta_1,⋯, \\theta_{100}$\n",
    "\n",
    "\n",
    "Of course is nearly impossible to know which parameter contributes more or less to the overfitting issue. So in regularization we modify the cost function to <b>shrink all parameters by some amount</b>.\n",
    "\n",
    "The original cost function for linear regression is:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "\\end{align}\n",
    "\n",
    "The regularized version adds an extra term, called regularization term that shrinks all the parameters:\n",
    "\n",
    "\\begin{align}\n",
    "J_{reg}(\\theta) = \\frac{1}{2m} \\bigg[\\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{m} \\theta_j^2\\bigg]\n",
    "\\end{align}\n",
    "\n",
    "The lambda symbol ($\\lambda$) is called the regularization parameter and it is responsible for a trade-off between fitting the training set well and keeping each parameter small. By convention the first parameter $\\theta_0$ is left unprocessed, as the loop in the regularization term starts from 1 (i.e. j=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. But you still need to manually select λ... which can result in problems\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\Regularization1.png\" width=60%>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. An Example: Regularised Linear Regression\n",
    "\n",
    "To build regularised linear regression we have to tweak the original gradient descent algorithm:\n",
    "\n",
    "\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}_j &\n",
    "\\newline \\rbrace \\end{align*}\n",
    "\n",
    "\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\newline \\; & \n",
    "\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)} \\newline \\; & \n",
    "\\theta_2 := \\theta_2 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)} \\newline & \n",
    "\\cdots \n",
    "\\newline \\rbrace \\end{align*}\n",
    "\n",
    "The tweaked version, with regularisation term added, looks like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta_j} J_{reg}(\\theta) = \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j\n",
    "\\end{align}\n",
    "\n",
    "Plugging the above into the gradient descent algorithm looks like this:\n",
    "\n",
    "\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\newline \\; & \n",
    "\\cdots  \\newline \\; & \n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j & \\newline \n",
    "\\rbrace \\end{align*}\n",
    "\n",
    "Note that the first $\\theta_0$ is left unprocessed.  Rearranging the above can make it simpler:\n",
    "\n",
    "\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\newline \\; & \n",
    "\\cdots  \\newline \\; & \n",
    "\\theta_j := \\theta_j(1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} & \\newline \n",
    "\\rbrace \\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General intro, overview and explanation of Logistic Regression:\n",
    "\n",
    "- https://www.internalpointers.com/post/introduction-classification-and-logistic-regression\n",
    "    \n",
    "Detailed explanation of the Logistic Regression Cost Function:\n",
    "    \n",
    "- https://www.internalpointers.com/post/cost-function-logistic-regression\n",
    "\n",
    "Detailed explanation of Regularisation for Logistic Regression:\n",
    "\n",
    "- https://www.internalpointers.com/post/problem-overfitting-machine-learning-algorithms \n",
    "- http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/ \n",
    "\n",
    "Detailed general overview:\n",
    "\n",
    "- https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n",
    "- https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
