{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Logistic Regression - Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the theory behind logistic regression.  A separate notebook details python implementations of various logistic regression examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is an algorithm used in <b>classification</b> problems.\n",
    "\n",
    "## 1.1. What are classification problems?\n",
    "\n",
    "A classification problem is where:\n",
    "\n",
    "1. The input / independent variables are <b>continuous</b> in nature; \n",
    "    \n",
    "    whereas\n",
    "    \n",
    "\n",
    "2. The output / dependent variables are <b>categorical</b>, of which there are three categorical types:\n",
    "\n",
    "    a. <b>Binary:</b> an email is spam (1) or not (0);\n",
    "\n",
    "    b. <b>Multi:</b> Cats, Dogs, Sheep; or\n",
    "\n",
    "    c. <b>Ordinal:</b> Low, Medium or High\n",
    "   \n",
    "   and\n",
    "   \n",
    "   \n",
    "3. We need to predict the category $y$ for given input variables $x$, e.g. if a student scores 90% on test A, 65% on test B and 82% on test C, predict whether they will Pass or Fail (i.e. a binary classification) to get into a given university?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Formally\n",
    "\n",
    "We want to predict:\n",
    "\n",
    "1. <b>Binary:</b> a variable $y \\in\\{0,1\\}$, where 0 is the <b>negative class</b> and 1 is the <b>positive class</b>; or\n",
    "\n",
    "\n",
    "2. <b>Multi:</b> a variable $y \\in\\{0,1, ... n\\}$, where 0 is class 1, 1 is clas 2 and so on.\n",
    "\n",
    "Each of these will be explored separately further below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Linear Regression vs. Logistic Regression\n",
    "\n",
    "Linear Regression tries to predict the value of a <b>continuous</b> variable, e.g.\n",
    "\n",
    "<img src=\"../Images/linreg1.png\" width=40%/>\n",
    "\n",
    "Logistic Regression tries to predict the <b>probability</b>, i.e. a number between 0 and 1 as described above, that a given input (x) belongs to a certain <b>discrete class</b>:\n",
    "    \n",
    "<img src=\"../Images\\linreg2.png\" width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Why not use linear regression?\n",
    "\n",
    "Generally linear regression starts to break down where outliers exist.  For instance, suppose we want to predict whether a tumor is either:\n",
    "\n",
    "1. Malignant (i.e. the positive class); or\n",
    "\n",
    "\n",
    "2. Benign (i.e. the negative class.\n",
    "\n",
    "Using linear regression per the below we could create a threshold classifier at 0.5 such that:\n",
    "\n",
    "1. If $h_\\theta(x) \\geq 0.5$, predict $y = 1$, i.e. \"<b>malignant</b>\"; or\n",
    "\n",
    "\n",
    "2. If $h_\\theta(x) \\leq 0.5$, predict $y = 0$, i.e. \"<b>benign</b>\"\n",
    "\n",
    "This means everything to the left of the green line's vertical is benign, whereas everything to the right of the green line's vertical is \n",
    "\n",
    "<img src=\"../Images\\linRegTumor1.png\" width=40%/>\n",
    "\n",
    "However, if there are outliers we soon run into problems:\n",
    "\n",
    "<img src=\"../Images\\linRegTumor2.png\" width=40%/>\n",
    "\n",
    "Per the above, the outlier:\n",
    "\n",
    "* Significantly alters the line of best fit.\n",
    "\n",
    "\n",
    "* Now everything to the right of the green vertical line is <b>malignant</b> and everything to the left of it is <b>benign</b>, whereas this is clearly incorrect.\n",
    "\n",
    "\n",
    "* Instead, everything to the left of the yellow line should be benign and everything to the right of it should be malignant.\n",
    "\n",
    "For this reason, logistic regression becomes the better choice of classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression ingredients\n",
    "\n",
    "Logistic Regression has the following ingredients:\n",
    "\n",
    "1. A training set of $x$ values mapped to corresponding $y$ values.\n",
    "\n",
    "\n",
    "2. A starting hypothesis.\n",
    "\n",
    "\n",
    "3. A cost function.\n",
    "\n",
    "\n",
    "4. A gradient descent function.\n",
    "\n",
    "These are explained below, and later demonstrated using python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Linear Regression, Logistic Regression requires a labelled training set of inputs $x$ and outputs $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. The starting hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. The Hypothesis\n",
    "\n",
    "The starting hypothesis for logistic regression is:\n",
    "\n",
    "$h_\\theta(x) = g(z) = g(\\theta^{T}x)$</center> = $\\frac{1}{1 +e^{-\\theta^{T}x}}$\n",
    "\n",
    "which is the traditional hypothesis function, $\\theta^{T} x$, processed by a new function $g$, defined as $\\frac{1}{1 +e^{-z}}$, where $z$ is swapped for $\\theta^{T}x$.\n",
    "\n",
    "This new function is called the <b>sigmoid</b> or <b>logistic</b> function.  This takes a set of input variables $X$ and maps predicted values $y$ to probabilities between 0 and 1.  It looks like this:\n",
    "\n",
    "<img src=\"../Images\\sigmoid1.png\" width=40%/>\n",
    "\n",
    "In a sigmoid function as the input $z$:\n",
    "\n",
    "1. goes to $-\\infty$, the output $g(z)$ approaches $0$; and\n",
    "\n",
    "\n",
    "2. goes to $+\\infty$, the output $g(z)$ approaches $1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. So how does it work?\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "The logistic regression hypothesis allows us to convert a continuous input value of $x$ into an predicted probability between $0$ and $1$ that $x$ corresponds to a particular class, e.g. $y = 1$ or $y = 0$.  How we achieve this divides into two parts:\n",
    "\n",
    "#### Part 1: Calculating the <u>% probability</u> that $x$ is / is not a member of a category $y$\n",
    "\n",
    "The logistic regression hypothesis outputs a number between $0$ and $1$, representing the <b>estimated probability</b> that $y = 1$ for a new example $x$ input.  \n",
    "\n",
    "For example, if we want to predict whether a picture $x$ is a cat ($1$) or not cat ($0$):\n",
    "\n",
    "1. If I plug a new picture, $x$, into my hypothesis function and $h_\\theta(x) = 0.9$, this means:\n",
    "\n",
    "    a. It is telling me that for the new picture the probability that $y = 1$ (i.e. is cat) is 0.9, or in other words 90% likely to be a cat; and\n",
    "    \n",
    "    b. To write this formally we would describe this as:\n",
    "\n",
    "<center>$h_\\theta(x) = P(y = 1 \\text{ | } x; \\theta)$</center>\n",
    "\n",
    "2. In words, this says that the hypothesis function tells you the probability that $y = 1$ given $x$ (i.e. given a picture input with a catness represented by the feature $x$), parameterized by $\\theta$.\n",
    "\n",
    "\n",
    "3. To compute the probability of $y = 0$ (i.e. not cat) we simply do the following, which returns that the picture $x$ has a 10% chance of not being a cat:\n",
    "\n",
    "<center>$P(y = 0 \\text{ | } x; \\theta) = 1 - P(y = 1 \\text{ | } x; \\theta)$</center>\n",
    "\n",
    "<center>which becomes the below</center>\n",
    "\n",
    "<center>$P(y = 0 \\text{ | } x; \\theta) = 1 - 0.9 = 0.1$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: The Decision Boundary to assign the input to a given category\n",
    "\n",
    "To be clear, the above only describes how the hypothesis function produces a value between $0$ and $1$ to describe the probability that $y = 1$ for a given input value $x$.  \n",
    "\n",
    "What the above does <b>not</b> describe is those probabilities are then mapped to a discrete class, in this case the binary class cat vs. not cat.  \n",
    "\n",
    "Therefore, to map a probability from the hypothesis function to a discrete class we must determine a <b>decision boundary</b>:\n",
    "\n",
    "* above which we will classify probabilities into class 1, i.e. cat; and\n",
    "\n",
    "\n",
    "* below which we will classify probabilities into class 2, i.e. not cat.\n",
    "\n",
    "For example, if our decision boundary were $0.5$:\n",
    "\n",
    "* if $h_\\theta(x) \\geq 0.5$ predict $y = 1$\n",
    "\n",
    "\n",
    "* if $h_\\theta(x) \\leq 0.5$ predict $y = 0$\n",
    "\n",
    "Looking again at the sigmoid function graph, you'll notice that:\n",
    "\n",
    "* $g(z) \\geq 0.5$ whenever $z \\geq 0$ \n",
    "\n",
    "\n",
    "* $g(z) \\leq 0.5$ whenever $z \\leq 0$ \n",
    "\n",
    "As the hypothesis function for logistic regression is defined as $h_\\theta(x) = g(\\theta^{T}x)$, we can state that $h_\\theta(x) = g(\\theta^{T}x) \\geq 0.5$ whenever $\\theta^{T}x \\geq 0$\n",
    "\n",
    "Therefore, pulling it altogether, we can restate the above assumptions as follows:\n",
    "\n",
    "* if $h_\\theta(x) \\geq 0.5$ (i.e. whenever $\\theta^{T}x \\geq 0$) then predict $y = 1$ (i.e. green quadrant below)\n",
    "\n",
    "\n",
    "* if $h_\\theta(x) \\leq 0.5$ (i.e. whenever $\\theta^{T}x \\leq 0$) then predict $y = 0$ (i.e. red quadrant below)\n",
    "\n",
    "<img src=\"../Images\\sigmoid1c.png\" width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose\n",
    "\n",
    "As with Linear Regression, we need a cost function to measure the difference between:\n",
    "\n",
    "1. the predicted value of $y$; and\n",
    "\n",
    "<p>\n",
    "    \n",
    "2. the actual value of $y$.\n",
    "\n",
    "Returning to the cat picture example, if the hypothesis outputs 0.75 for a given picture $x$, which is in fact a cat (1), then the cost would be 1 - 0.75 = 0.25.\n",
    "\n",
    "#### Explaining the Cost Function\n",
    "\n",
    "The verbose version of the Logistic Regression Cost Function is:\n",
    "    \n",
    "<center>$Cost(h_\\theta(x), y) =\n",
    "\\begin{cases}\n",
    "-\\log(h_\\theta(x)) & \\text{if y = 1} \\\\\n",
    "-\\log(1-h_\\theta(x)) & \\text{if y = 0}\n",
    "\\end{cases}$\n",
    "</center>\n",
    "\n",
    "What this means is this:\n",
    "\n",
    "* <b>Where y = 1</b>, the output (i.e. the cost) approaches $0$ as $h_\\theta(x)$ approaches $1$.  Conversely, the cost grows to $\\infty$ as $h_\\theta(x)$ approaches $0$.  \n",
    "\n",
    "<img src=\"../Images\\costfunctiony1.png\" width=20%/>\n",
    "\n",
    "\n",
    "<p>\n",
    "    \n",
    "* <b>Where y = 0</b>, the output (i.e. the cost) approaches $0$ as $h_\\theta(x)$ approaches $0$.  Conversely, the cost grows to $\\infty$ as $h_\\theta(x)$ approaches 1. \n",
    "\n",
    "<img src=\"../Images\\costfunctiony0.png\" width=20%/>\n",
    "\n",
    "This makes sense: we want a <b>bigger</b> cost penalty as the algorithm predicts a $y$ value that is further away from the actual $y$ value and vice versa.\n",
    "\n",
    "#### Simplifying the formula\n",
    "\n",
    "The above logistic regression cost function can be simplified to the below:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}[-y^{(i)}log(h_\\theta(x^{(i)})) - (1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))]\n",
    "\\end{align}\n",
    "\n",
    "Note that multiplying by $y$ and $(1 - y)$ in the above is a sneaky trick that ensures we only perform the operation we need to perform.  For instance to solve for both $y = 1$ and $y = 0$:\n",
    "\n",
    "* if $y = 0$, the left side cancels out; and\n",
    "\n",
    "\n",
    "* if $y = 1$, the right side cancels out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose\n",
    "\n",
    "As with linear regression, we use gradient descent (albeit a different formulation) to optimise the cost function (i.e. minimise its value to $0$) and thereby discern the optimal parameters (or weights) for our hypothesis that ensures as accurate as possible predictions of y for a given x value.\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_j := \\theta - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)\n",
    "\\end{align}\n",
    "\n",
    "Which can be rewritten as:\n",
    "\n",
    "\\begin{align} \n",
    "\\text{repeat until convergence \\{} \\\\\n",
    "\\theta_j & := \\theta_j - \\alpha \\dfrac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\\\ \n",
    "\\text{\\}}\n",
    "\\end{align}\n",
    "\n",
    "This is <i>almost</i> identical to the gradient descent function for linear regression except that:\n",
    "\n",
    "* the definition of the hypothesis $h_\\theta(x)$ for linear regression was $h_\\theta(x) = \\theta^{T}(x)$\n",
    "        \n",
    "    whereas\n",
    "    \n",
    "\n",
    "* the definition of the hypothesis $h_\\theta(x)$ for logistic regression is $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^{\\top} x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've only explored <b>binary</b> classification in the above examples.  Now let's look at <b>multiclass</b> classification.  Multiclass Classification is also known as <b>one vs. all</b> or <b>one vs. rest</b>.\n",
    "\n",
    "## 3.1. Difference between binary and multi-class classification\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logregressionMulticlass20.png\" width=50%>\n",
    "</p>\n",
    "\n",
    "For binary classification we have $2$ categories and want to calculate the probability that $y = \\{0, 1\\}$.  Whereas for multiclass classification we have $2+$ categories and want to calculate the probability that $y = \\{0, 1, ... n\\}$.  \n",
    "\n",
    "## 3.2. How do we perform multi-class classification?\n",
    "\n",
    "Since $y = \\{0, 1, ... n\\}$, we divide our problem into $n + 1$ binary classification problems.  Note the $+1$ is because the index starts at $0$.  \n",
    "\n",
    "In each one we predict the probability that $y$ is a member of that class, i.e. $y ∈ \\{0, 1, ... n\\}$:\n",
    "\n",
    "\n",
    "1. $h_\\theta^{(0)}(x) = P(y = 0 \\text{ | }x; \\theta)$.\n",
    "\n",
    "\n",
    "2. $h_\\theta^{(1)}(x) = P(y = 1 \\text{ | }x; \\theta)$.\n",
    "\n",
    "   ...\n",
    "   \n",
    "\n",
    "3. $h_\\theta^{(n)}(x) = P(y = n \\text{ | }x; \\theta)$.\n",
    "\n",
    "\n",
    "4. Where the (i) in $h_\\theta^{(i)}$ denotes the class, e.g. $h_\\theta^{(1)}$ describes <b>class 1</b>.\n",
    "\n",
    "\n",
    "5. Prediction = max (i) for $h_\\theta^{(i)}(x)$.\n",
    "\n",
    "We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.\n",
    "\n",
    "### 3.2.1. In practice\n",
    "\n",
    "So for a 3 class multiclass classification we:\n",
    "\n",
    "1. Turn training set into <b>three</b> separate binary classification models:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logregressionMulticlass22.png\" width=50%>\n",
    "</p>\n",
    "\n",
    "2. Which in formula are:\n",
    "\n",
    "    h<sub>θ</sub><sup>(0)</sup>(x) = P(y = 0 | x; θ).\n",
    "\n",
    "    h<sub>θ</sub><sup>(1)</sup>(x) = P(y = 1 | x; θ).\n",
    "\n",
    "    h<sub>θ</sub><sup>(2)</sup>(x) = P(y = 2 | x; θ).\n",
    "\n",
    "    h<sub>θ</sub><sup>(3)</sup>(x) = P(y = 3 | x; θ).\n",
    "    \n",
    "\n",
    "2. And train a logistic regression classifiers, $h_\\theta^{(i)}(x)$ class i to predict the probability that y = i.\n",
    "\n",
    "\n",
    "3. On a new input $x$, to make a prediction, pick the class i that maximises $h_\\theta^{(i)}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General intro, overview and explanation of Logistic Regression:\n",
    "\n",
    "- https://www.internalpointers.com/post/introduction-classification-and-logistic-regression\n",
    "    \n",
    "Detailed explanation of the Logistic Regression Cost Function:\n",
    "    \n",
    "- https://www.internalpointers.com/post/cost-function-logistic-regression\n",
    "\n",
    "Detailed explanation of Regularisation for Logistic Regression:\n",
    "\n",
    "- https://www.internalpointers.com/post/problem-overfitting-machine-learning-algorithms \n",
    "- http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/ \n",
    "\n",
    "Detailed general overview:\n",
    "\n",
    "- https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n",
    "- https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
