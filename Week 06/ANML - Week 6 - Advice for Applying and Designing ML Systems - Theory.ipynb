{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Week 6 is about systematically designing and improving learning algorithms. Specifically Week 6 covers techniques and tips to:\n",
    "\n",
    "1. identify when a learning algorithm is doing poorly, and describe the 'best practices' for how to 'debug' the learning algorithm.\n",
    "\n",
    "\n",
    "2. optimize a machine learning algorithm, including understanding where the biggest improvements can be made and in particular how to understand the performance of a machine learning system with multiple parts, and also how to deal with skewed data; and\n",
    "\n",
    "\n",
    "3. design effective machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Debugging a Learning Algorithm\n",
    "\n",
    "There are various options if a machine learning algorithm returns unacceptably large errors.  These include:\n",
    "\n",
    "1. Acquiring more training samples.\n",
    "\n",
    "\n",
    "2. Trying a smaller set of features.\n",
    "\n",
    "\n",
    "3. Trying a larger set of additional features.\n",
    "\n",
    "\n",
    "4. Adding polynomial features, e.g. $x_1^2$, $x_2^2$, $x_1 x_2$ etc.\n",
    "\n",
    "\n",
    "5. Increasing $\\lambda$.\n",
    "\n",
    "\n",
    "6. Decreasing $\\lambda$\n",
    "\n",
    "Rather than randomly trying one, more or all of the above, there are diagnostic techniques to guide you to the appropriate selection of tweaks necessary to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating a Learning Algorithm\n",
    "\n",
    "Simply minimising the cost function for the training set may not mean the algorithm is optimised.  In fact, it may only mean the algorithm is <b>only</b> optimied for <i>the training set</i>.  \n",
    "\n",
    "In other words, it may be <b>overfitted</b> to the training set.\n",
    "\n",
    "To deploy ML in real life we need to evaluate <b>whether the algorithm generalizes from the training set to other data</b>.  \n",
    "\n",
    "To do so we typically split the initial dataset into different categories of subsets and apply techniques to adjust the algorithm in line with how well it performs over these different subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Splitting into (a) Training Set + (b) Test Set\n",
    "\n",
    "A simple way to begin evaluating a learning algorithm is to split the dataset into:\n",
    "\n",
    "1. A <b>Training Set</b>, $x^{(i)}_{train}, y^{(i)}_{train})$; and\n",
    "\n",
    "\n",
    "2. A <b>Test Set</b>, $x^{(i)}_{test}, y^{(i)}_{test})$.\n",
    "\n",
    "The former is the dataset subset on which the algorithm is <i>trained</i>.  The latter is the dataset subset on which the algorithm is <i>tested</i> to assess its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Applying a Train vs. Test split: Linear Regression\n",
    "\n",
    "Simply do the following:\n",
    "\n",
    "1. Split the dataset into a <b>Training</b> dataset and a Test dataset.\n",
    "\n",
    "\n",
    "2. Learn the paramater(s) $\\theta$ from the training data.\n",
    "\n",
    "\n",
    "3. Compute the error for the <b>Test</b> dataset.\n",
    "\n",
    "For instance, if the algorithm is overfitting the training set we would expect a low error for the training set but a high error for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Applying a Train vs. Test Split: Logistic Regression\n",
    "\n",
    "Simply do the following:\n",
    "\n",
    "1. Split the dataset into a <b>Training</b> dataset and a Test dataset.\n",
    "\n",
    "\n",
    "2. Learn the paramater(s) $\\theta$ from the training data.\n",
    "\n",
    "\n",
    "3. Compute the error for the <b>Test</b> dataset.\n",
    "\n",
    "\n",
    "4. Apply misclassification error (AKA 0/1 classification error):\n",
    "\n",
    "\\begin{align}\n",
    "err(h_\\Theta(x),y) = \\begin{matrix} 1 & \\mbox{if } h_\\Theta(x) \\geq 0.5\\ and\\ y = 0\\ or\\ h_\\Theta(x) < 0.5\\ and\\ y = 1\\newline 0 & \\mbox otherwise \\end{matrix}\n",
    "\\end{align}\n",
    "\n",
    "5. The misclassification error:\n",
    "\n",
    "    (a) returns $1$ if $h_{\\theta}(x) \\geq 0.5$ but mispredicts $y = 0$ or if $h_{\\theta}(x) \\leq 0.5$ but mispredicts $y = 1$; and\n",
    "   \n",
    "    (b) returns $0$ in all other cases. \n",
    "    \n",
    "    \n",
    "6.  The average misclassification error can be found as follows, which returns the proportion of the test data that was misclassified:\n",
    "\n",
    "\\begin{align}\n",
    "Test Error = \\frac{1}{m_{test}}\\sum_{i = 1}^{m_{test}}err(h_\\theta(x^{(i)}_{test}), y^{(i)}_{test}) \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. What is the ideal Train vs. Test split?\n",
    "\n",
    "There are no hard and fast rules.  As a best practice, a $70:30$ split is usually adopted, whereby $70\\%$ of the dataset forms the Training Set and the remaining $30\\%$ forms the Test Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Model Selection & Train / Validation / Test Sets\n",
    "\n",
    "### 3.4.1. The Context\n",
    "\n",
    "When evaluating performance of <b>several different</b> models, e.g. to identify which degree  ($d$) of polynomial to fit to the data, you will do the following:\n",
    "\n",
    "1. Define each polynomial function from $d = 1$ to $d = n$, i.e. your list of potential models.\n",
    "\n",
    "\n",
    "2. For each polynomial function learn the parameter(s) $\\Theta$, e.g. $\\Theta^{1)}, \\Theta^{(2)}$ and so on.\n",
    "\n",
    "\n",
    "3. For each set of $\\Theta$ values, evaluate the cost function, e.g. $J_{test}(\\Theta^{(1)}$ and so on.\n",
    "\n",
    "\n",
    "4. Identify which polynomial degree best fits the data, i.e. lowest error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. The Problem\n",
    "\n",
    "However, the above does not tell you whether the chosen poynomial model generalises well or at all.  Instead it only tells you that it is the best for the train / test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3. The Solution\n",
    "\n",
    "To better assess how well the chosen polynomial generalises we must split the dataset into a <b>third</b> subset rather than two.  Concretely this means we:\n",
    "\n",
    "1. Split $60\\%$ of the dataset into a <b>Training Set</b>;\n",
    "\n",
    "\n",
    "2. Split $20\\%$ of the dataset into a <b>Cross Validation Set</b> (\"$cv$\"); and\n",
    "\n",
    "\n",
    "3. Split $20\\%$ of the dataset into a <b>Test Set</b>.\n",
    "\n",
    "Pulling that altogether we end up evaluating the error over three separate subsets of the total dat\n",
    "aset $m$:\n",
    "\n",
    "* $J_{train}(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "\n",
    "* $J_{cv}(\\theta) = \\frac{1}{2m_{cv}} \\sum_{i = 1}^{m_{cv}}(h_\\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$\n",
    "\n",
    "\n",
    "* $J_{test}(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m_{test}}(h_\\theta(x_{test}^{(i)}) - y_{test}^{(i)})^2$\n",
    "\n",
    "Using this <b>threeway</b> split we repeat steps $1 - 4$ described in 3.4.1. above.  However, this time we evaluate the models using the <b>cross validaton</b> set instead of the test set.  \n",
    "\n",
    "Once the model has been selected after evaluating performance on the cross validation set, we then evaluate the error function of the selected model against the <b>test set</b>.\n",
    "\n",
    "We usually expect $J_{cv}(\\theta)$ to be <b>lower</b> than $J_{test}(\\theta)$ because an extra parameter $d$ (i.e. the degree of polynomial) has been fit to the cross validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bias vs. Variance\n",
    "\n",
    "Most ML problems are due to having one or both of:\n",
    "\n",
    "1. <b>high variance</b>, i.e. <b>overfitting</b>; and / or\n",
    "\n",
    "\n",
    "2. <b>high bias</b>, i.e. <b>underfitting</b>.\n",
    "\n",
    "<img src=\"../Images/fitting.png\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. What are Bias and Variance?\n",
    "\n",
    "### 4.1.1. Bias\n",
    "\n",
    "* Bias = difference between: (a) predicted values; and (b) actual values.  \n",
    "\n",
    "\n",
    "* If the average predicted values are <b>far off</b> the actual values then there is <b>high bias</b>.\n",
    "\n",
    "\n",
    "* If the average predicted values are <b>close</b> to the actual values then there is <b>low bias</b>.\n",
    "\n",
    "\n",
    "* High bias causes the alogrithm to miss relevant relationships between input and output variable, implying the model is <b>too simple</b> and thus <b>underfits</b> the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Variance\n",
    "\n",
    "* Variance = model performs well on trained dataset but not well on a dataset on which it has not been trained, e.g. a test or validation dataset.\n",
    "\n",
    "\n",
    "* If the predicted values are on average scattered <b>close</b> to the actual values then there is <b>high variance</b>.\n",
    "\n",
    "\n",
    "* If the predicted values are on averaged scatted <b>far</b> from the actual values then there is <b>low variance</b>.\n",
    "\n",
    "\n",
    "* High variance causes <b>overfitting</b> that implies the algorithm models random noise present in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Bias / Variance Combinations\n",
    "\n",
    "1. <b>High Bias / Low Variance:</b> models consistent but inaccurate on average.\n",
    "\n",
    "\n",
    "2. <b>High Bias / High Variance:</b> models inconsistent and inaccurate on average.\n",
    "\n",
    "\n",
    "3. <b>Low Bias / Low Variance:</b> models accurate and consistent on average.  This is what we should aim to achieve.\n",
    "\n",
    "\n",
    "4. <b>Low Bias / High Variance:</b> models somewhat accurate but inconsistent on average.  A small change in the data can cause a large error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Diagnosing Bias vs. Variance\n",
    "\n",
    "Given that bias and variance are the most common causes of machine learning problems we need to diagnose whether our algorithm suffers from one or both.\n",
    "\n",
    "To do so we can use <b>learning curves</b> to diagnose our algorithm:\n",
    "\n",
    "<img src=\"../Images/VarianceBias.png\" width=80%/>\n",
    "\n",
    "In other words:\n",
    "\n",
    "1. If $J_{train}(\\theta)$ is <b>high</b> and $J_{cv}$ is approximately equal to $J_{train}(\\theta)$ then there is <b>high bias</b>.  \n",
    "\n",
    "\n",
    "2. Conversely, if $J_{train}(\\theta)$ is <b>low</b> and $J_{cv}$ is <b>much greater than </b>$J_{train}(\\theta)$ then there is <b>high variance</b>.\n",
    "\n",
    "\n",
    "3.  Therefore, we want something roughly in the middle of the two graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Regularization + Bias / Variance\n",
    "\n",
    "Recall that regularistion is a way to correct <b>overfitting</b> by including a penalty $\\lambda$ on the parameters thereby:\n",
    "\n",
    "* reducing the mode's likelihood of fitting the noise in the training dat; and \n",
    "\n",
    "\n",
    "* in turn improve the algorithm's generalisation abilities.\n",
    "\n",
    "We can use the techniques above plus intuitions about the affect of $\\lambda$ on bias and variance to determine whether we need to increase or decrease $\\lambda$ in order to optimise the algorithm.\n",
    "\n",
    "### 4.3.1. Automating the Choice of $\\lambda$\n",
    "\n",
    "1. Split the dataset into three subsets as follows:\n",
    "\n",
    "    (a) $60\\%$ of the dataset into a <b>Training Set</b>;\n",
    "    \n",
    "    (b) $20\\%$ of the dataset into a <b>Cross Validation Set</b> (\"$cv$\"); and\n",
    "\n",
    "    (c) $20\\%$ of the dataset into a <b>Test Set</b>.\n",
    "\n",
    "\n",
    "2. Arbitrarily select a number of $\\lambda$ values, e.g. $0, 0.01, 0.02, 0.04, 0.08 ... 10.24$.  \n",
    "\n",
    "    <b>Note:</b> Andrew Ng usually doubles each $\\lambda$ value when setting potential initial $\\lambda$ values.\n",
    "\n",
    "\n",
    "3. Create a set of models with different degrees of polynomials or any other variants.\n",
    "\n",
    "\n",
    "4. Iterate each model + $\\lambda$ combination to learn the resulting $\\theta$ values.\n",
    "\n",
    "\n",
    "5. For each resulting $\\theta$ value, Min $J_{cv}(\\theta)$ using the <b>non-regularised</b> version of the cost function for each model + $\\lambda$ combination to calculate the resulting error on the cross-validation set.  \n",
    "\n",
    "\n",
    "6. Take the $\\theta$ values learned by the best model + $\\lambda$ combination\n",
    "from step (5) (i.e. the one with the lowest error after cross validation).\n",
    "\n",
    "\n",
    "7. Min $J_{test}(\\theta)$ using the <b>non-regularised</b> version of the cost function to identify if $\\theta$ has a good generalization of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Bias / Variance as a function of the Regularisation Parameter\n",
    "\n",
    "1. If $J_{train}(\\theta)$ is <b>high</b> and $J_{cv}$ is approximately equal to $J_{train}(\\theta)$ then there:\n",
    "\n",
    "   (a) is <b>high bias</b> / <b>underfitting</b>; and \n",
    "    \n",
    "   (b) $\\lambda$ is <b>too large</b>.  \n",
    "\n",
    "\n",
    "2. Conversely, if $J_{train}(\\theta)$ is <b>low</b> and $J_{cv}$ is <b>much greater than </b>$J_{train}(\\theta)$ then there is:\n",
    "\n",
    "    (a) <b>high variance</b> / <b>overfitting</b>; and \n",
    "    \n",
    "    (b) $\\lambda$ is <b>too small</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Images/modelLambdaSelection.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Learning Curves\n",
    "\n",
    "Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. \n",
    "\n",
    "Hence:\n",
    "\n",
    "* as the training set gets larger, the error for a quadratic function increases (i.e. because it's harder and harder to accurately fit  line to all the training samples in a dataset); and\n",
    "\n",
    "\n",
    "* the error value will plateau out after a certain $m$, or training set size because the more samples provided will help the model generalise better.\n",
    "\n",
    "Learning curves (as demonstrated above) are a tool to diagnose if a learning algorithm is suffering from bias, variance or a bit of both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. High Bias / Underfitting\n",
    "\n",
    "### 5.1.1. Context\n",
    "\n",
    "Occurs when:\n",
    "\n",
    "1. <b>Low training set size:</b> causes $J_{train}(\\Theta)$ to be <b>low</b> and $J_{CV}(\\Theta)$ to be <b>high</b>.\n",
    "\n",
    "\n",
    "2. <b>Large training set size:</b> causes both $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ to be <b>high</b> with $J_{train}(\\Theta) \\approx J_{cv}(\\theta)$\n",
    "\n",
    "### 5.1.2. Solution\n",
    "\n",
    "<img src=\"../Images/highBias2.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. High Variance / Overfitting\n",
    "\n",
    "### 5.2.1. Context\n",
    "\n",
    "Occurs when:\n",
    "\n",
    "1. <b>Low training set size:</b> $J_{train}(\\Theta)$ will be <b>low<b> and $J_{CV}(\\Theta)$ will be <b>high</b>.\n",
    "\n",
    "\n",
    "2. <b>Large training set size:</b> $J_{train}(\\Theta)$ increases with training set size and $J_{CV}(\\Theta)$ continues to decrease without leveling off, however, the difference between them remains significant.\n",
    "\n",
    "\n",
    "### 5.2.2. Solution\n",
    "\n",
    "<img src=\"../Images/highVariance2.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary (of the above)\n",
    "\n",
    "When assessing machine learning algorithms our decision process can be broken down as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Generally\n",
    "\n",
    "<img src=\"../Images/summaryBiasVariance.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Neural Networks\n",
    "\n",
    "<b>A neural network with fewer parameters:</b>\n",
    "\n",
    "* Prone to underfitting. \n",
    "\n",
    "* Computationally cheaper.\n",
    "\n",
    "\n",
    "<b>A large neural network with more parameters:</b> \n",
    "\n",
    "* Prone to overfitting. \n",
    "\n",
    "* Computationally expensive. \n",
    "\n",
    "* In this case you can use regularization (increase λ) to address the overfitting.\n",
    "\n",
    "\n",
    "<b>Number of Layers?</b>\n",
    "\n",
    "* Using a single hidden layer is a good starting default. \n",
    "\n",
    "* You can train your neural network on a number of hidden layers using your cross validation set. \n",
    "\n",
    "* You can then select the one that performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Model Complexity Effects\n",
    "\n",
    "* <b>Lower-order polynomials (low model complexity):</b> have high bias and low variance. In this case, the model fits poorly consistently.\n",
    "\n",
    "\n",
    "* <b>Higher-order polynomials (high model complexity):</b> fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.\n",
    "\n",
    "\n",
    "* In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. The Power of Large Data\n",
    "\n",
    "High Variance and High Bias can in theory be avoided if you:\n",
    "\n",
    "1. Use a learning algorithm with <b>many parameters</b> (e.g. logistic regression / linear regression with many features) or a neural network with many hidden units).  This:\n",
    "\n",
    "    (a) creates low bias algorithms, reducing the chance of underfitting; and\n",
    "    \n",
    "    (b) means $J_{train}(\\theta)$ will be small.\n",
    "\n",
    "\n",
    "2. Use a very large training set.  This:\n",
    "\n",
    "    (a) creates low variance algorithms, reducing the chance of overfitting; and\n",
    "    \n",
    "    (b) means $J_{train}(\\theta) \\approx J_{test}(\\theta)$ and thereofre  $J_{test}(\\theta)$ will also be small.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Error Analysis\n",
    "\n",
    "## 7.1. Manual Error Analysis\n",
    "\n",
    "The recommended approach to solving machine learning problems is to:\n",
    "\n",
    "1. Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.\n",
    "\n",
    "\n",
    "2. Plot learning curves to decide if more data, more features, etc. are likely to help.\n",
    "\n",
    "\n",
    "3. Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.\n",
    "\n",
    "For example:\n",
    "\n",
    "* Assume that we have 500 emails and our algorithm misclassifies a 100 of them. \n",
    "\n",
    "\n",
    "* We could manually analyze the 100 emails and categorize them based on what type of emails they are. \n",
    "\n",
    "\n",
    "* We could then try to come up with new cues and features that would help us classify these 100 emails correctly. \n",
    "\n",
    "* Hence, if most of our misclassified emails are those which try to steal passwords, then we could find some features that are particular to those emails and add them to our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. \"Accuracy\" is not enough\n",
    "\n",
    "For instance, a system makes $1,000$ predictions like so to determine whether a plane passenger is terrorist (<b>positive</b>) or non-terrorist (<b>negative</b>):\n",
    "\n",
    "<img src=\"../Images/confusionMatrix2.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the one hand we can say that the system correctly predicted 998 / 1000 results and is therefore 99.9% accurate.  Instinctively most people assume this is incredible, and the only measure of success their algorithm needs.\n",
    "\n",
    "On the other hand, the one result the system missed 50% of the total terrorists, i.e. the individual predicted as \"Negative\" but in fact \"Positive\" (see lower left corner of the above).  Much less impressive, and much more alarming!\n",
    "\n",
    "This neatly illustrates why the better, and indeed first, question is not whether the system is \"accurate\" but specifically whether there is a need for:\n",
    "\n",
    "1. <b>High Recall</b> a <b>high</b> cost for missing a true positive (e.g. a terrorist, cancer diagnosis or someone infected with a zombie virus!) and a <b>low</b> cost of mislabelling an actual true negative as a true positive (e.g. saying someone is a terrorist who is in fact not a terrorist)\n",
    "\n",
    "    or\n",
    "\n",
    "\n",
    "2. <b>High Precision:</b> a <b>low</b> cost for missing a true positive (e.g. a criminal in relation to a capital offence) and a <b>high</b> cost of mislabelling an actual true negative as a true positive (e.g. saying an innocent person is guilty of a capital offence).\n",
    "\n",
    "\n",
    "\n",
    "These issues become readily apparent in many situations where the number of observations belonging to one class are significantly lower than those belonging to the other classes (as in the above diagram), also known as <b>imbalanced classification problems</b>.  \n",
    "\n",
    "Real life examples include:\n",
    "\n",
    "* Fraud detection\n",
    "\n",
    "\n",
    "* Terrorism detection\n",
    "\n",
    "\n",
    "* Rare disease detection\n",
    "\n",
    "\n",
    "* Zombie infection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Precision & Recall\n",
    "\n",
    "### 7.3.1. Precision\n",
    "\n",
    "* <b>Intutition:</b> Of the <i>predicted</i> true positives, how many did we correctly predict.  \n",
    "\n",
    "\n",
    "* <b>Mathematically:</b> precision = $\\frac{\\text{true positives}}{\\text{true positives + false positives}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2. Recall (AKA \"Sensitivity\")\n",
    "\n",
    "* <b>Intutition:</b> Of the <i>actual</i> true positives, how many did we correctly predict.  \n",
    "\n",
    "\n",
    "* <b>Mathematically:</b> recall = $\\frac{\\text{true positives}}{\\text{true positives + false negatives}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3. Confusion Matrix\n",
    "\n",
    "The resulting spread of true positives / false positives to actual positives / actual negatives is typically represented in a confusion matix like so:\n",
    "\n",
    "<img src=\"../Images/confusionMatrix1.png\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Trading off precision and recall\n",
    "\n",
    "### 7.4.1. Increasing Precision\n",
    "\n",
    "* <b>Why?</b>: where incorrectly predicting a true positive has a high cost, e.g. predicting someone guilty of a capital offence when in fact they are innocent.\n",
    "\n",
    "\n",
    "* <b>How?</b>: increasing the decision boundary, e.g. modifiying predict 1 if $h_\\theta(x) \\geq 0.5$ to $h_\\theta(x) \\geq 0.7$.\n",
    "\n",
    "However, increasing precision reduces recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2. Increasing Recall\n",
    "\n",
    "* <b>Why?</b>: where incorrectly predicting a true negative has a high cost, e.g. predicting someone does not have cancer when in fact they do have cancer.\n",
    "\n",
    "\n",
    "* <b>How?</b>: reducing the decision boundary, e.g. modifiying predict 1 if $h_\\theta(x) \\geq 0.7$ to $h_\\theta(x) \\geq 0.5$. \n",
    "\n",
    "However, increasing recall reduces precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. Measuring Precision and Recall\n",
    "\n",
    "* Precision and Recall should be measured on the <b>cross validation</b> set.  \n",
    "\n",
    "\n",
    "* The value of the threshold should be that which maximises the $F_1$ score, i.e. results in the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. Choosing the Appropriate Precision / Recall Ratio\n",
    "\n",
    "<b>Problem:</b> If we are evaluating precision and recall scores for different algorithms we need to then evaluate which algorithm produces the optimal ratio of precision and recall.\n",
    "\n",
    "<b>Solution:</b> the $F_1$ Score.  Mathematically this is:\n",
    "\n",
    "\\begin{align}\n",
    "2 \\frac{PR}{P + R}\n",
    "\\end{align}\n",
    "\n",
    "<b>Difference between $F_1$ score and Accruacy:</b> as described in 7.2 above, a high \"accuracy\" score can be largely contributed to by a large number of True Negatives which in most business circumstances we do not focus on, whereas False Negatives and False Positives usually have business consequences, good or bad.  Therefore $F_1$ scores are potentially better if we needto balance between Precision and Recall and there is an uneven class distribution, e.g. a large number of True Negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Useful Resources\n",
    "\n",
    "- http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n",
    "\n",
    "- https://medium.com/datadriveninvestor/bias-and-variance-in-machine-learning-51fdd38d1f86\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
