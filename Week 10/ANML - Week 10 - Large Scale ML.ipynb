{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Large Scale Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In Week 10 Andrew Ng explains how the principles and techniques from weeks 1 - 9 can be modified to cope with large scale datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Large Scale ML Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Tip #1 - Increase or Reduce Sample Size\n",
    "\n",
    "Reducing the sample size can reduce computational load and thereby save time.  Conversely, increasing sample size may increase performance albeit increasing computational load and therefore time.  \n",
    "\n",
    "However, neither technique will work in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Determing whether to reduce sample size\n",
    "\n",
    "With reference to Week 6, plot $J_{cost}(\\theta)$ and $J_{train}(\\theta)$ to determine variance.\n",
    "\n",
    "If there is <b>high</b> variance, adding extra training samples is <b>likely</b> to improve performance:\n",
    "\n",
    "<p align = \"center\">\n",
    "  <img src=\"../Images\\lowVariance.png\" width=30%>\n",
    "</p>\n",
    "\n",
    "If there is <b>low</b> variance, adding extra training samples is <b>unlikely</b> to improve performance:\n",
    "\n",
    "<p align = \"center\">\n",
    "  <img src=\"../Images\\highVariance.png\" width=30%>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Tip #2 - Other Types of Gradient Descent\n",
    "\n",
    "With gradient descent throughout Weeks 1 - 9, we've run gradient descent on every sample during each iteration, also known as <b>Batch Gradient Descent</b>, where the batch size is $m$.  With large datasets this is computationally draining!\n",
    "\n",
    "For <b>linear regression</b> it's possible to switch out batch gradient descent for <b>Stochastic Gradient Descent</b> or <b>Mini-Batch Gradient Descent</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Stochastic Gradient Descent\n",
    "\n",
    "SGD requires:\n",
    "\n",
    "1. A <b>random shuffle</b> of the dataset; and\n",
    "\n",
    "\n",
    "2. Running gradient descent on a <b>single</b> sample per iteration (vs. all samples per iteration as is the case for batch gradient descent).\n",
    "\n",
    "Note SGD won't neatly step down toward the minimum, instead it will dart about toward the minimum and then hover around it indefinitely.\n",
    "\n",
    "To identify convergence, every 1,000 iterations or so plot $cost(\\theta, (x^{(i)}, y^{(i)}))$ averaged over the last 1,000 examples processed by the algorithm.  If it is decreasing and starting to plateau then it is likely converging; in all other scenarios it is probably not converging. \n",
    "\n",
    "Reducing the Learning Rate may help, and in some uses of SGD, the learning rate is deliberately and slowly decreased over time if we want $\\theta$ to converge, e.g. by defining the learning rate as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha = \\frac{const1}{iterationNumber + const2}\n",
    "\\end{align}\n",
    "\n",
    "However, you will need to fiddle with $const1$ and $const2$, so it does involve some extra effort and guesswork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Mini-Batch Gradient Descent\n",
    "\n",
    "MBGD requires running gradient descent on a <b>mini-batch</b>, $b$, of samples per iteration.  Using MBGD allows you to effectively paralellise your computations via vectorization vs. SGD, e.g. if you ran MBGD with $b = 10$ vs. a SGD implementation across $10$ samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Tip #3 - Reduce Learning Rate\n",
    "\n",
    "Reducing the learning rate may also improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Online Learning\n",
    "\n",
    "If you have a continuous stream of data, with large volumes, you can train on each sample in real time and simply delete the sample after training the algorithm with that sample.  Depending on your data flow, it may make more sense to save large amounts of data and train the algorithm offline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Map-Reduce & Data Parallelism\n",
    "\n",
    "Map-Reduce splits the training set into subsets, which are then run separately on different machines.  This reduces the amount of computations per machine, speeding up total time to completion.\n",
    "\n",
    "The master server will then combine the results from each machine to find the optimal parameters from the entire dataset.\n",
    "\n",
    "E.g. if a total sample set $m = 400$ we can split this into $4$ x $100$ datasets and run each on $4$ separate machines so each machine runs $100$ computations each vs. $400$ if we ran all computations on one machine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
