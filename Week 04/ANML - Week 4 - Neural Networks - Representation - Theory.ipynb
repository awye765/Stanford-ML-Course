{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook explains what a neural network is and how we represent it in a machine learning model.  A neural network in machine learning is an algorithmic architecture designed to mimic biological neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Biological Neural Networks\n",
    "\n",
    "## 2.1. Neurons\n",
    "\n",
    "### 2.1.1. What are they?\n",
    "\n",
    "A neuron is the brain's <b>basic computational unit</b>.  \n",
    "\n",
    "### 2.1.2. What do they do?\n",
    "\n",
    "It receives and integrates chemical signals from other neurons via <b>dendrites</b> and, depending on a number of factors, it either does nothing or generates an electrical signal output via <b>axons</b>, which in turn signals other connected neurons via synapses:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\neurons.PNG\" width=\"80%\"/>\n",
    "</p>\n",
    "\n",
    "Source: https://becominghuman.ai/making-a-simple-neural-network-2ea1de81ec20\n",
    "\n",
    "### 2.1.3. How do they work?\n",
    "\n",
    "In essence, the cell acts as a <b>function</b> into which we provide inputs (via the dendrites) that are churned into an output via the axon terminals.  \n",
    "\n",
    "### 2.1.4. Relevance to machine learning?\n",
    "\n",
    "Neural networks in machine learning aim to represent this function and connect neurons in a useful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression vs. Neural Networks\n",
    "\n",
    "## 3.1. Logistic Regression Representation\n",
    "\n",
    "In logistic regression, we composed a linear model $h_\\theta(x)$ with the logistic function $g(z)$ to form our predictions.  This linear model was a combination of feature inputs $x_i$ and weights $\\theta_i$:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logisticRegressionNN.PNG\" width=60%>\n",
    "</p>\n",
    "\n",
    "Represented like this, we can describe that:\n",
    "\n",
    "1. The firts layer contains a node for each value in our input feature vector $x^1$, i.e. $x_0$, $x_1$ and $x_2$.\n",
    "\n",
    "\n",
    "2. These input features are scaled by their corresponding weights stored in the $\\theta^1$ vector, i.e. $\\theta_0$, $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "\n",
    "3. The values from (1) and (2) - a single linear combination of all the inputs - are then passed into a single output node, which processes these values via the logistic (aka sigmoid) function.\n",
    "\n",
    "\n",
    "4. In neural network parlance, we can say (3) is the \"<b>activation unit</b>\".  It controls whether or not this node, or \"<b>neuron</b>\" fires\n",
    "\n",
    "\n",
    "5. Recall the $x_0$ bias unit has been added.  Recall this is added to enable matrices operations but also to ensure our model isn't fixed through the origin, i.e. $(0, 0)$.  Without this, our model would necessarily pass through the origin and cause problems like so:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logisticRegressionOrigin.PNG\" width=60%>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Difference to a simple Neural Network\n",
    "\n",
    "At a high level, they're practically identical - the main difference being the activation function, $g(z)$, used to control neuron firing. The perceptron activation is a <b>step-function</b> from 0 (when the neuron doesn't fire) to 1 (when the neuron fires) while the logistic regression model has a smoother activation function with values ranging from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Why can't we simply use linear or logistic regression?\n",
    "\n",
    "Even for a relatively simple problem such as predicting house prices, if there are 100 features it may become incredibly complicated to fit a linear or logistic function to the data.  \n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks1.PNG\" width=70%>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What are neural networks in machine learning?\n",
    "\n",
    "The below is a great video from [3Blue1Brown](http://www.3blue1brown.com/) introducing and explaining neural newtworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aircAruvnKk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen> </iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aircAruvnKk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen> </iframe></center>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Network Ingredients + Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with this example from the Andrew Ng course we will explain the function of each component, plus its corresponding notation.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks6.PNG\" width=80%>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Nodes\n",
    "\n",
    "A neural network is a network of computational units, also known as nodes.  These nodes store and / or process information passing through the neural network from its input later to its output layer.\n",
    "\n",
    "In the above, each node is represented by each circle.  There are different types of nodes, including:\n",
    "\n",
    "* Input nodes\n",
    "\n",
    "\n",
    "* Activiation Units\n",
    "\n",
    "\n",
    "* Output nodes\n",
    "\n",
    "These, and their notation, are explored below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Layers $j$\n",
    "\n",
    "### 5.2.1. What do they do?\n",
    "\n",
    "Layers are simply a collection of nodes organised together but separate from other layers of nodes.  There are different types of layers:\n",
    "\n",
    "* <b>Input Layer:</b> the first layer in the neural network where each node represents a value for each feature $x_i$.\n",
    "    \n",
    "    \n",
    "* <b>Hidden Layers:</b> subsequent layers intermediate between the input layer and output layer.\n",
    "\n",
    "\n",
    "* <b>Output Layer:</b> the last layer in the neural network where each node represents a value corresponding to an output value $y$.\n",
    "\n",
    "\n",
    "### 5.2.2. Notation\n",
    "\n",
    "* Current layer = $j$.\n",
    "\n",
    "\n",
    "* Preceding layer = $j - 1$.\n",
    "\n",
    "\n",
    "* Subsequent layer = $j + 1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Input Nodes $x$\n",
    "\n",
    "### 5.3.1. What does it do?\n",
    "\n",
    "Stores a feature value for $x_i$.  For instance, if we were teaching a neural network to classify 28 x 28 pixel images (i.e. 784 pixels total) of handwritten digits to their corresponding numerical values we might:\n",
    "\n",
    "1. Represent each <b>pixel</b> by its grayscale value, i.e. a number between $0$ (white) and $1$ (black);\n",
    "\n",
    "\n",
    "2. Store this information in a single input node, e.g. node 1 = grayscale value of pixel 1 and so on from $x_1, x_2, ... x_{784}$; and\n",
    "\n",
    "\n",
    "3. Therefore each <b>image</b>, $x^i$, would be represented by a $784$ x $1$ vector of pixel grayscale values.\n",
    "\n",
    "### 5.3.2. Notation\n",
    "\n",
    "Simply:\n",
    "\n",
    "* $x_i$ = the $i^{th}$ feature; and\n",
    "\n",
    "\n",
    "* $x^i$ = the $i^{th}$ sample.\n",
    "\n",
    "\n",
    "<b>Example:</b> $x_1^{(2)}$ = feature $1$ in the $2^{nd}$ sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.4. Activation Units $a$\n",
    "\n",
    "### 5.4.1. What does it do?\n",
    "\n",
    "Each activation unit:\n",
    "\n",
    "1. Takes as its input the sum of: (i) each value from each input node, $x_i$, multiplied (ii) by the corresponding weight, $w_i$.\n",
    "\n",
    "\n",
    "2. The information from (1) is then processed by an <b>activation function</b>, tradtionally the <b>sigmoid / logit</b> function although in recent times the <b>ReLU</b> function is preferred.  Both functions squish that number into a value between $0$ and $1$.\n",
    "\n",
    "\n",
    "3. The value from (2) is then compared against a <b>threshold value</b>.  If the linear sum of inputs and weights is higher than the threshold the neuron fires, and if less than the threshold it does not fire.\n",
    "\n",
    "### 5.4.2. Notation\n",
    "\n",
    "1. $_i$ the activation unit number.\n",
    "\n",
    "\n",
    "2. $^j$ the layer number used to reference the current layer from:\n",
    "\n",
    "    (a) $j + 1$, the <b>subsequent</b> layer; and\n",
    "    \n",
    "    (b) $j - 1$, the <b>previous</b> layer.\n",
    "\n",
    "\n",
    "3. $a_i^{(j)}$ = activation unit $i$ in layer $j$.  \n",
    "\n",
    "    <b>Example:</b> $a_1^{(2)}$ = activation unit $1$ in the $2^{nd}$ layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Output Nodes\n",
    "\n",
    "### 5.4.2. What does it do?\n",
    "\n",
    "Is an activation unit that computes the final output for the hypothesis.\n",
    "\n",
    "### 5.4.3. Notation\n",
    "\n",
    "Same as for activation units above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## 5.5. Parameters Matrix $\\Theta$ \n",
    "\n",
    "### 5.5.1. What does it do?\n",
    "\n",
    "The parameters matrix controlls the function mapping from layer $j$ to layer $j + 1$.  In other words, these are the dials and knobs that the algorithm tweaks to learn the optimum values and combinations of weights to accurately predict outputs from inputs.\n",
    "\n",
    "### 5.5.2. Notation\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetwork10.PNG\" width=100%>\n",
    "</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3. Dimensionality of the $\\Theta$ Parameters Matrix\n",
    "\n",
    "The $\\Theta$ matrix's dimensionality is expressed as: $s_{j + 1}$ x $(s_j + 1)$ (rows x columns), where: \n",
    "\n",
    "* $s_{j + 1}$ = number of units in layer $j + 1$, i.e. the follwoing layer; and\n",
    "\n",
    "\n",
    "* $(s_j + 1)$ = number of units in the current layer $+ 1$, i.e. the additional unit to represent the bias unit, which is always $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: $\\Theta^{1}$\n",
    "\n",
    "* <b>Row length</b> = number of units in the <b>following</b> layer, i.e. $3$.\n",
    "\n",
    "\n",
    "* <b>Column length</b> = number of units in the <b>current</b> layer $+ 1$ (because of the bias unit), i.e. $4$.\n",
    "  \n",
    "  \n",
    "* <b>Resulting Dimensionality</b> = $\\Theta^{1}$ is of $3$ x $4$ dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: $\\Theta^{2}$\n",
    "\n",
    "* <b>Row length</b> = number of units in the <b>following</b> layer, i.e. $1$.\n",
    "\n",
    "\n",
    "* <b>Column length</b> = number of units in the <b>current</b> layer $+ 1$ (because of the bias unit), i.e. $4$.\n",
    "\n",
    "    \n",
    "* <b>Resulting Dimensionality</b> = $\\Theta^{2}$ is of $1$ x $4$ dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How is information forward processed from one layer to another?\n",
    "\n",
    "In the below the value of $a_o^{(1)}$ is equal to the sigmoid of: the weighted sum of each activation unit in the preceding layer + the bias unit.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\NNProcessing.PNG\" width=80%>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\NNProcessing3.PNG\" width=80%>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Forward Propagation \n",
    "\n",
    "## 7.1. What is forward propagation?\n",
    "\n",
    "In neural networks, you propagate <b>forward</b> from the inital input feature values in layer 1 to the output values of the final layer.  This is done to get the output and compare it with the real value to get the error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. What's clever about forward propagation?\n",
    "\n",
    "The clever thing is the difference in what a neural network can learn vs. logistic regression:\n",
    "\n",
    "* <b>Logistic Regression:</b> output layer learns new parameters based on the original input features.\n",
    "\n",
    "\n",
    "* <b>Logistic Regression:</b> output layer learns <b>both</b> new parameters <b>and</b> new features based on inputs from intermediate layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. Logistic Regression\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks14.PNG\" width=40%>\n",
    "</p>\n",
    "\n",
    "Here Layer 3 is simply a logistic regression node where the hypothesis output is simply: g(Ɵ<sub>10</sub><sup>2</sup> a<sub>0</sub><sup>2</sup> + Ɵ<sub>11</sub><sup>2</sup> a<sub>1</sub><sup>2</sup> + Ɵ<sub>12</sub><sup>2</sup> a<sub>2</sub><sup>2</sup> + Ɵ<sub>13</sub><sup>2</sup> a<sub>3</sub><sup>2</sup>.\n",
    "\n",
    "The model is constrained to processing the input featurs scaled by the weights.  It cannot learn new features, only new parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### 7.2.2. Neural Networks\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks12.PNG\" width=40%>\n",
    "</p>\n",
    "\n",
    "Here, unlike the above, Layer 3 learns both: \n",
    "\n",
    "1. new parameters; and\n",
    "\n",
    "\n",
    "2. new features!\n",
    "\n",
    "So it's as if the neural network, instead of being constrained to feed the features x<sub>1</sub>, x<sub>2</sub> and x<sub>3</sub> into logistic regression, it instead gets to learn its own features a<sup>(1)</sup>, a<sup>(2)</sup> and a<sup>(3)</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3. Why is this helpful?\n",
    "\n",
    "It means the neural network can learn some interesting and complex features (e.g. edges or combinations of edges in images of handwritten digits as in the 3Brown1Blue video) and therefore end up with better hypotheses than if you were constrained to use the raw features x<sub>1</sub>, x<sub>2</sub> and x<sub>3</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. An Example\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks11.PNG\" width=70%>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Backward Propagation\n",
    "\n",
    "## 8.1. What is backward propagation?\n",
    "\n",
    "To minimize the error, you propagate <b>backwards</b> by finding the derivative of error with respect to each weight and then subtracting this value from the weight value.\n",
    "\n",
    "\n",
    "## 8.2. Back Propagation\n",
    "\n",
    "See week 5 notes for full detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Neural Networks and Logic Gates\n",
    "\n",
    "## 9.1. Why Demonstrate This?\n",
    "\n",
    "Demonstrating the below helps build an intuitive understanding of how neural networks can be configured to replicate logic gates to perform increasingly complex decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. AND gate\n",
    "\n",
    "### 9.2.1. Defining the Problem\n",
    "\n",
    "We want to create an AND logic gate with a neural network:  \n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logicGateDiagramAND.PNG\" width=40%>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\ANDLogicGate.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "This is a function that will output 1 if and only if <b>both inputs are equal to 1</b>.\n",
    "\n",
    "### 9.2.2. A Neural Network Solution\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks15.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "To achieve this, we choose a negative value of x<sub>0</sub> that is greater than each of x<sub>1</sub> and x<sub>2</sub>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. OR gate\n",
    "\n",
    "### 9.3.1. Defining the Problem\n",
    "\n",
    "We want to create an OR logic gate with a neural network:  \n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logicGateDiagramOR.PNG\" width=40%>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\ORLogicGate.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "This is a function that will output 1 if and only if <b>one or both of the inputs are equal to 1</b>.\n",
    "\n",
    "### 9.3.2. Demonstrating the Solution\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks16.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "To achieve this, we choose a negative value of x<sub>0</sub> that is less than x<sub>1</sub> and x<sub>2</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. NOT gate\n",
    "\n",
    "### 9.4.1. Defining the Problem\n",
    "\n",
    "We want to create a NOT logic gate with a neural network.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logicGateDiagramNOT.PNG\" width=40%>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\NOTLogicGate.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "This is a function that will output 1 if and only if <b>the input is equal to 0</b>.\n",
    "\n",
    "### 9.4.2. Demonstrating the Solution\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks17.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "To achieve this, we choose a positive value of x<sub>0</sub> that is less than x<sub>1</sub> but also negate x<sub>1</sub>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5. NAND gate\n",
    "\n",
    "### 9.5.1. Defining the Problem\n",
    "\n",
    "We want to create a NAND logic gate with a neural network.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logicGateDiagramNAND.PNG\" width=40%>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\NANDLogicGate.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "This is a function that will output 1 if and only if <b>one or more of the inputs are equal to 0</b>.\n",
    "\n",
    "### 9.5.2. Demonstrating the Solution\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks21.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "To achieve this, we choose a positive value of x<sub>0</sub> that is greater than x<sub>1</sub> and x<sub>2</sub> and also negate x<sub>1</sub> and x<sub>2</sub>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6. NOR gate\n",
    "\n",
    "### 9.6.1. Defining the Problem\n",
    "\n",
    "We want to create a NOR logic gate with a neural network.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logicGateDiagramNOR.PNG\" width=40%>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\NORLogicGate.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "This is a function that will output 1 if and only if <b> both inputs are equal to 0</b>.\n",
    "\n",
    "### 9.6.2. Demonstrating the Solution\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks19.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "To achieve this, we choose a positive value of x<sub>0</sub> that is less than x<sub>1</sub> and x<sub>2</sub> and also negate both of x<sub>1</sub> and x<sub>2</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7. A Limitation of Neural Networks\n",
    "\n",
    "The above examples are possible because it allows us to separate positive examples from negative examples linearly.  Unfortunately, for <b>XOR</b> and <b>XNOR</b> logic gates we need a non-linear decision boundary, which rules out further permutations on the above basic structures.  For example:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\XORXNORproblem.GIF\" width=50%>\n",
    "</p>\n",
    "\n",
    "Instead, we must combine these structures to create a more complex neural network with a hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.8. XOR gate\n",
    "\n",
    "### 9.8.1. Defining the Problem\n",
    "\n",
    "We want to create a XOR logic gate with a neural network.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logicGateDiagramXOR.PNG\" width=40%>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\XORLogicGate.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "This is a function that will output 1 if and only if <b> one but not both inputs are equal to 1</b>.\n",
    "\n",
    "### 9.8.2. Demonstrating the Solution\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\XORneuralnetwork.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "To solve for a XNOR neural network we must combine:\n",
    "\n",
    "1. an NOR neural network (ORANGE); AND\n",
    "\n",
    "\n",
    "2. a NAND neural network (BLUE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.10 XNOR gate\n",
    "\n",
    "### 9.10.1. Defining the Problem\n",
    "\n",
    "We want to create a XNOR logic gate with a neural network.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\logicGateDiagramXNOR.PNG\" width=40%>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\XNORLogicGate.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "This is a function that will output 1 if and only if <b> both inputs are the same, i.e. both 1 or both 0</b>.\n",
    "\n",
    "### 9.10.11. Demonstrating the Solution\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\XNORneuralnetwork.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "To solve for a XNOR neural network we must combine:\n",
    "\n",
    "1. an AND neural network (RED);\n",
    "\n",
    "\n",
    "2. a NOR neural network (BLUE); AND\n",
    "\n",
    "\n",
    "3. an OR neural network (GREEN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.11. Useful Resources\n",
    "\n",
    "- http://www.ee.surrey.ac.uk/Projects/CAL/digital-logic/gatesfunc/index.html\n",
    "\n",
    "- https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b (re the XOR and XNOR problems for neural networks re inability to linearly separate this types of classification)\n",
    "\n",
    "- https://www.quora.com/How-can-we-design-a-neural-network-that-acts-as-an-XOR-gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Multiclass Classifcation\n",
    "\n",
    "As with logistic regression we can apply neural networks to multiclass classification problems. \n",
    "\n",
    "## 10.1. How does this work?\n",
    "\n",
    "To do so we will have multiple output nodes, each output node corresponding to a particularl classification.  E.g.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\multiclassneuralnetwork2.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "* <b>Node 1</b> = Pedestrian.\n",
    "\n",
    "\n",
    "* <b>Node 2</b> = Car.\n",
    "\n",
    "\n",
    "* <b>Node 3</b> = Motorcycle.\n",
    "\n",
    "\n",
    "* <b>Node 4</b> = Truck.\n",
    "\n",
    "This resulting set of classes can be defined as $y$:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\multiclassneuralnetworkYClasses.PNG\" width=20%>\n",
    "</p>\n",
    "\n",
    "And if our resulting hypothesis for one set of inputs looks like this:\n",
    "\n",
    "$h_\\Theta(x) =\\begin{bmatrix}0 \\newline 0 \\newline 1 \\newline 0 \\newline\\end{bmatrix}$\n",
    "\n",
    "Then the resulting class for the input image is the third one down, or $h_\\Theta(x)$, which represents the motorcycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Why use neural networks\n",
    "\n",
    "To solve large and complicated problems.  For instance, a computer vision problem to identify cars from non-cars:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralnetworks2.PNG\" width=50%>\n",
    "</p>\n",
    "\n",
    "In this type of problem, the number of features is huge.  In such circumstances, there is no simple way to build decent classifiers.  \n",
    "\n",
    "Instead, we need a non-linear hypothesis to separate the classes.  Neural networks perform much better in this regard than simply logistic regression.\n",
    "\n",
    "<b>Explanation re Quadratic Terms:</b>\n",
    "\n",
    "1. The ~3,000,000 quadratic features figure is the number of unique pairs of x<sub>i</sub> and x<sub>j</sub>.\n",
    "\n",
    "\n",
    "2. This can be calculated as follows: n(n+1)/2.\n",
    "\n",
    "\n",
    "3. Therefore, this equates to: 2,500 x (2501 + 1) / 2.\n",
    "\n",
    "\n",
    "4. 6,252,500 / 2 = 3,126,250.\n",
    "\n",
    "See also here:\n",
    "\n",
    "- https://www.coursera.org/learn/machine-learning/discussions/weeks/4/threads/zWn1oshFEeWRfg5WVr61Uw\n",
    "\n",
    "\n",
    "- https://www.mathsisfun.com/combinatorics/combinations-permutations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Popular Neural Network Architectures\n",
    "\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"../Images\\neuralNetworksArchitectires.PNG\" width=70%>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Useful Resources\n",
    "\n",
    "- https://www.jeremyjordan.me/intro-to-neural-networks/\n",
    "\n",
    "\n",
    "- http://www.asimovinstitute.org/neural-network-zoo/\n",
    "\n",
    "\n",
    "- https://media.readthedocs.org/pdf/ml-cheatsheet/latest/ml-cheatsheet.pdf\n",
    "\n",
    "\n",
    "- https://bigtheta.io/2016/02/24/intro-to-neural-networks.html\n",
    "\n",
    "\n",
    "- https://medium.com/@erikhallstrm/backpropagation-from-the-beginning-77356edf427d\n",
    "\n",
    "\n",
    "- https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html\n",
    "\n",
    "\n",
    "- https://d3c33hcgiwev3.cloudfront.net/_106ac679d8102f2bee614cc67e9e5212_deep-learning-notation.pdf?Expires=1543708800&Signature=jRrjLR7FHWycjGsg5b24zlDfEfeOZdm9mqq~j1BQ7Zj2QUgG-Zbk3DvjxKJxgo6j-043b~EQK433DRu7Db3u~hpTYlak331V8oc-CB~lyWEPtLB6zv6JMknrGuoGbwOG4hVy~JLD9msqX9bo~QiAuC0iuFMJhJ7s74bCuuZ8UWY_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/under-the-hood-of-neural-network-forward-propagation-the-dreaded-matrix-multiplication-a5360b33426\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883 \n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f\n",
    "\n",
    "\n",
    "- https://media.readthedocs.org/pdf/ml-cheatsheet/latest/ml-cheatsheet.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
