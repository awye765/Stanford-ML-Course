{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Week 4 focused on the representation, notation and architecture for simple feed forward (aka forward propagation) neural networks.  Week 5 focuses on how neural networks <b>learn</b> via backpropagration.\n",
    "\n",
    "This week we will cover:\n",
    "\n",
    "1. The neural network cost function;\n",
    "\n",
    "\n",
    "2. The neural network gradient descent function;\n",
    "\n",
    "\n",
    "3. The idea and intuition for backpropagation; \n",
    "\n",
    "\n",
    "4. How to best initialise parameters; and\n",
    "\n",
    "\n",
    "5. How all of the above, plus theory from Week 4, comes together to create a functioning neural network capable of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Intuition: what is backpropagation in a nutshell?\n",
    "\n",
    "I am going to try and restate the intuition from [this](https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3) awesome 3Blue1Brown video regarding backpropagation, staritng with how it works for a single example.\n",
    "\n",
    "1. <b>Goal:</b> at a high level backpropagation is the algorithm for determining how a single training example would like to adjust the weights and biases up or down and in what relative proportions to those changes to cause the most rapid decrease to the cost.  \n",
    "\n",
    "\n",
    "2. <b>How:</b> \n",
    "\n",
    "    (a) First, a training instance is fed into the network and the output of every neuron from the input to output layer is calculated (the \"<b>Forward Pass</b>\").\n",
    "    \n",
    "    (b) Second, the \"<b>Output Error</b>\" is calculated for each output node, i.e. the difference between (i) the desired output and (ii) the actual output of the network.\n",
    "    \n",
    "    (c) Third, a calculation is made to compute how much each neuron in the previous layer (i.e. $L - 1$) contributed to the Output Error.\n",
    "    \n",
    "    (d) Fourth, a calculation is made to compute how much each neuron in the layer previous to $L - 1$ (i.e. $L - 2$) contributed to the Output Error... and so on until the algorithm reaches the input layer.\n",
    "    \n",
    "    We describe each of (c) and (d) as the \"<b>Reverse Pass</b>\".  The Reverse Pass efficiently measures the <b>error gradient</b> across all the connection weights in the network by propagating (i.e. passing back) the error gradient backward through the network.\n",
    "\n",
    "\n",
    "3. <b>Result:</b> completing the above adjusts each weight in the network in proportion to how much it contributes to the overall error based on a single training example.  If we iteratively reduce each weightâ€™s error over multiple training exampes we will eventually have a series of weights the produce good predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Example Context\n",
    "\n",
    "In the above video we have:\n",
    "\n",
    "* $5000$ images of handwritten digits, labelled to their corresponding number; \n",
    "\n",
    "\n",
    "* Each image is a $28$ x $28$ pixel image, i.e. $784$ total pixels;\n",
    "\n",
    "\n",
    "* $10$ classes, i.e. one for each number $0 - 9$;\n",
    "\n",
    "\n",
    "* 13,000 weights / 2 biases (one bias for each hidden layer);\n",
    "\n",
    "\n",
    "* $4$ layers:\n",
    "<br>\n",
    "<br>\n",
    "    * Layer 1 = input layer of 784 nodes, one for each pixel (i.e. feature);\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "    * Layer 2 = hidden layer no. 1;\n",
    "<br>\n",
    "<br>\n",
    "    * Layer 3 = hidden layer no. 2; and\n",
    "<br>\n",
    "<br>\n",
    "    * Layer 4 = output layer of 10 nodes, one node for each class, e.g. node 0 = digit 0, node 1 = digit 1 and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. The Backpropagation Process (No Maths)\n",
    "\n",
    "1. First, we feed in an input image - i.e. values for each of the 784 nodes in layer 1.  This information is forward propagated (see Week 4 notes) to calculate the probabilities that the input image corresponds to digit 0, 1, 2, ... 9.\n",
    "\n",
    "\n",
    "2. To assess accuracy of the network given a single image we calculate the cost, e.g. for an image corresponding to the number $3$, we compare:\n",
    "    \n",
    "    (a) The output of the neural network, i.e. the probabilities described in (1)\n",
    "\n",
    "        with\n",
    "\n",
    "    (b) The output you wanted the neural network to give, i.e. the actual label corresponding to the image, in this case digit 3.\n",
    "\n",
    "\n",
    "3. Then add up the <b>square</b> of the differences, i.e. difference between 2(a) and (b):\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNCostIntuition4.PNG\" width=\"80%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "4. Recall that:\n",
    "\n",
    "    (a) Each leftmost number bounded within $()$ in the above describes the probability that the input image belongs to each class; and\n",
    "\n",
    "    (b) Each rightmost number bounded within $()$ in the above describes the actual label for that input image, in this case the number $3$, hence only the $4th$ node is equal to $1$ and all other nodes are equal to $0$.\n",
    "\n",
    "\n",
    "5. The resulting number, i.e. the cost, is either:\n",
    "\n",
    "    (a) <b>large</b> when the image is classified <b>incorrectly</b>, e.g. per the above example; or\n",
    "    \n",
    "    (b) <b>small</b> when it is classified <b>correctly</b>, e.g. per the below example:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNCostIntuition5.PNG\" width=\"80%\"/>\n",
    "</p>\n",
    "\n",
    "6. In other words, the above <b>only</b> calculates the cost function for a <b>single</b> input image.  In turn this tells us how accurate the network is at classifying the digit 3 when fed an input image.\n",
    "\n",
    "\n",
    "7. If the network is inaccurate, i.e. output node corresponding to the correct label is not firing higher than all other output nodes, then the network needs adjusting so that (for this example):\n",
    "\n",
    "    (a) the output node for digit 3 is <b>increased</b> in value; \n",
    "    \n",
    "    (b) the output nodes for labels 0, 1, 2, 4, 5, 6, 7, 8 and 9 are <b>descreased</b> in value; and\n",
    "    \n",
    "    (c) the size of the adjustments described in 7(a) and (b) should be <b>proportional</b> to the distance away from the desired values to accurately classify the particular input image, i.e. per the below diagram (note this diagram describes an input image of a number 2, not an input image of a number 3 per the above):\n",
    "    \n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNCostIntuition6.PNG\" width=\"80%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "8. Per the above image, we can say that the value of the digit 2 output node is a product of the weighted sum of the activation nodes of the previous layer plus a bias, which is altogether processed through a sigmoid or ReLU function.\n",
    "\n",
    "\n",
    "9. Therefore, the digit 2 output node's value is influenced by adjustments to the:\n",
    "\n",
    "    (a) <b>bias</b> unit;\n",
    "    \n",
    "    (b) each <b>weight</b>, <i>in proportion to the related activation unit in the previous layer</i>; and\n",
    "    \n",
    "    (c) each <b>activation unit</b> in the previous layer, <i>in proportion to the related weight</i>.\n",
    "\n",
    "\n",
    "10. With regard to the activation units, we can interpret that the digit 2 output node's value is influenced: \n",
    "\n",
    "    (a) <b>more</b> by the <b>higher</b> value activation nodes in the previous layer; and\n",
    "    \n",
    "    (b) <b>less</b> by the <b>lower</b> value activation nodes in the previous layer,\n",
    "    \n",
    "    and therefore the weights connecting (a) to the output node will have more impact because they are multiplying larger values than the weights connecting (b) to the output node.  \n",
    "    \n",
    "    Therefore, increasing the weights connecting a high value activation node to the output node has a <b>stronger</b> influence than increasing / decreasing the weights of lower value input nodes.\n",
    "    \n",
    "\n",
    "11. Not only do we need to adjust the ingredients based on increasing the value of the digit 2 output node, but we also need to adjust the ingredients based on decreasing the value of the non-digit 2 output nodes.  The adjustments to do the latter will differ from those required to achieve the former because they aim to alter the output nodes in an opposite direction.  \n",
    "\n",
    "    Intuitively, 3Blue1Brown describe this as each output node for each image sample having its own \"thoughts\" about how to adjust the previous layer's bias unit, weights and activation unit's values.  \n",
    "    \n",
    "    \n",
    "12. Therefore, each output node's suggested changes to the previous layer are added together like so:\n",
    "    \n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNCostIntuition7.PNG\" width=\"80%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "13. After the above is complete, you recursively apply the same process backward through each previous layer.  This is from where the term \"back propagation\" derives.\n",
    "\n",
    "\n",
    "14. But <b>remember</b>, this only describes the back propagation routine for a <b>single</b> sample image.  Therefore, you repeat the above process for each image in your data set to generate the average adjustments to each weight and bias in the network.  Roughly speaking, this is the negative gradient of the cost function for all weights and biases in the network.\n",
    "\n",
    "\n",
    "15. In practice, this is <b>not</b> repeated for every example in a dataset all at once.  Instead, the dataset is randomly shuffled and sliced into mini datasets, known as \"<b>Mini Batches</b>\".  These mini data sets are processed one by one, unlike in a true gradient descent step whereby all weights and biases for all examples would be process simultaneously!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Maths\n",
    "\n",
    "To keep this simple, I am again going to follow the awesome intuition presented by 3Blue1Brown in [this](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4) video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Example Context\n",
    "\n",
    "Assume a simple neural network with:\n",
    "\n",
    "1. Four layers, each comprising one node:\n",
    "\n",
    "    (a) Layer 1 - input layer ($L - 3$);\n",
    "    \n",
    "    (b) Layer 2 - hidden layer 1 ($L - 2$);\n",
    "    \n",
    "    (c) Layer 3 - hidden layer 2 ($L - 1$); and\n",
    "    \n",
    "    (d) Layer 4 - output layer ($L$).\n",
    "    \n",
    "    \n",
    "2. Where the connection between each layer is influenced by a weight, $w$.\n",
    "\n",
    "\n",
    "3. Our goal is to reduce the cost of the network's function, which is to say we want to optimise the weights and biases affecting the neural connections and thereby ensure the network is as accurate as possible.  \n",
    "\n",
    "\n",
    "4.  For these purposes, to keep things consistent, let's assume this network classifies whether an image of a handwritten digit is actually a number 2 or not a number 2.\n",
    "\n",
    "\n",
    "5. Finally, to keep things simple, we will focus on the connection between the last two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.  Feeding Forward to Calculate the <u>Value</u> of the Output Neuron\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNCalculus0.PNG\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "In the above:\n",
    "\n",
    "1. $y$ is the value of the <b>last activation</b> unit we want the network to output.  In other words, if the image is of a number 2, this activation unit should, when fed that image, output $1.0$, i.e. a 100% probability it is a number 2.\n",
    "\n",
    "\n",
    "2. $L$ is the index of the layer we are currently computing.  We count back from $L$ to reference other layers.  For instance, as we are calculating the value of the output neuron this layer is $L$ and the one previous is labelled $L - 1$.\n",
    "\n",
    "\n",
    "3. $a$ is each activation unit.\n",
    "\n",
    "\n",
    "4. $C_0$ is the cost for a single training example, in this case, the very first training example.  It is simply $a^{(L)} - y)^2$, i.e. the difference between the  predicted value $a^{(L)}$ and the desired value for this activation unit.\n",
    "\n",
    "\n",
    "5. Each activation unit's value is determined by: $\\sigma(w^{(L)}a^{(L - 1)} + b^{(L)})$.  This breaks down as follows:\n",
    "\n",
    "    (a) the weight $w^{(L)}$\n",
    "    \n",
    "    multipled by\n",
    "    \n",
    "    (b) the value of the previous layer's activation unit $a^{(L - 1)}$ \n",
    "    \n",
    "    plus \n",
    "    \n",
    "    (c) the bias $b^{(L)}$\n",
    "    \n",
    "    squished into a value bounded between $1$ and $0$ by\n",
    "    \n",
    "    (d) passing (a) - (c) into the signmoid function $\\sigma$.\n",
    "\n",
    "\n",
    "6. We can use shorthand to describe 5(a) - (b), i.e. the weighted sum of the previous activation unit plus bias unit.  This shorthand is $z^{(L)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Computing the <u>Cost</u> of the Output Neuron\n",
    "\n",
    "### 3.3.1. Visualising the Relationships between inputs and cost function\n",
    "\n",
    "The cost computation can be described by this representation, demonstrating how each element - whether a constant (e.g. $w^{(L)}$ or $b^{(l)}$) or a computation ($a^{(L)}$) - feeds into each other, and ultimately into the cost function:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNCalculus2.PNG\" width=\"15%\"/>\n",
    "</p>\n",
    "\n",
    "We can also describe this mathematically as: $C_0 = (a^{(L)}-y)^{2} = (\\sigma(z^{(L)}) - y)^{2} = (\\sigma(w^{(L)}a^{(L - 1)} + b^{(L)}) - y)^{2}$.  This formula will calculate the cost of our network's function for a single example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Sensitivity of the Cost function to each element\n",
    "\n",
    "Next our goal is to determine how <b>sensitive</b> the cost functon is to small changes in each of the above tree's topmost ingredients, i.e.:\n",
    "\n",
    "1. $w^{(L)}$, which can be mathematically determined by the derivative of $C_0$ with respect to $w^{(L)}$</b>, i.e. $\\frac{\\partial C_0}{\\partial w^{(L)}}$;\n",
    "\n",
    "\n",
    "2. $a^{(L)}$, which can be mathematically determined by the derivative of $C_0$ with respect to $a^{(L)}$</b>, i.e. $\\frac{\\partial C_0}{\\partial a^{(L)}}$; and\n",
    "\n",
    "\n",
    "3. $b^{(L)}$, which can be mathematically determined by the derivative of $C_0$ with respect to $b^{(L)}$</b>, i.e. $\\frac{\\partial C_0}{\\partial b^{(L)}}$.\n",
    "\n",
    "A neat intuition from 3Blue1Brown is to think of each element in this process having its own slider per the below image, where tiny changes to each slider have a corresponding influence that waterfalls down the chain, and ultimately influences the cost function:  \n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNCalculus1.PNG\" width=\"50%\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.1. Sensitivity between pairs of elements down the chain\n",
    "\n",
    "1. Taking the sensitivity of $C_0$ with respect to $w^{(L)}$ we can conceptualise the relationships from $w^{(L)}$ down the chain to $C_0$ as follows:\n",
    "\n",
    "    (a) a nudge to $w^{((L)}$ causes...\n",
    "    \n",
    "    (b) a nudge to $z^{(L)}$, which causes...\n",
    "    \n",
    "    (c) a nudge to $a^{(L)}$, which causes...\n",
    "    \n",
    "    (d) an increase or decrease in $C_0$.\n",
    "\n",
    "    We can describe these interrelationships mathematically as follows:\n",
    "\n",
    "\n",
    "2. The ratio of the resulting change in $z^{(L)}$ with respect to a change in $w^{(L)}$ is the derivative of $z^{(L)}$ with respect to $w^{(L)}$, which can be represented as $\\frac{\\partial z^{(L)}}{\\partial w^{(L)}}$.\n",
    "\n",
    "\n",
    "3. The ratio of the resulting change in $a^{(L)}$ with respect to a change in $z^{(L)}$ is the derivative of $a^{(L)}$ with respect to $z^{(L)}$, which can be represented as $\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}$.\n",
    "\n",
    "\n",
    "4. The ratio of the resulting change in $C_0$ with respect to a change in $a^{(L)}$ is the derivative of $C_0$ with respect to $a^{(L)}$, which can be represented as $\\frac{\\partial C_0}{\\partial a^{(L)}}$.\n",
    "\n",
    "\n",
    "5. Altogether, we can combine (2) - (3) as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial C_0}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\frac{\\partial C_0}{\\partial a^{(L)}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "7. This is the \"<b>Chain Rule</b>\": by multiplying the ratios it gives us the sensitivity of $C_0$ to changes in $w^{(L)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.2. Computing the Derivatives\n",
    "\n",
    "1. Next we compute each of these derivaties:\n",
    "\n",
    "    (a) $\\frac{\\partial C_0}{\\partial a^{(L)}} = 2(a^{(L)} - y)$\n",
    "    \n",
    "    Notice: it's size is proportionatal to the different between the network's output and what it should be.\n",
    "    \n",
    "    (b) $\\frac{\\partial a^{(L)}}{\\partial z^{(L)}} = \\sigma'(z^{(L)})$\n",
    "    \n",
    "    Notice: the $'$ signifies that it is simply the derivative of the sigmoid function.\n",
    "    \n",
    "    (c) $\\frac{\\partial z^{(L)}}{\\partial w^{(L)}} = a^{(L - 1)}$\n",
    "    \n",
    "    Recall: the amount that the small change in $w^{(L)}$ changes the cost function depends on the size of $a^{(L - 1)}$.\n",
    "\n",
    "\n",
    "2. The above process, is described for a <b>single example image</b> example and for a <b> single input element</b>, and can be described as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial C_0}{\\partial w^{(L)}} = a^{(L - 1)} \\cdot \\sigma'(z^{(L)}) \\cdot 2(a^{(L)} - y)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "3. This process is then repeated with regard to each of $a^{(L)}$ and $b^{(L)}$, computing the constituent derivatives of the constituent relationships down the chain to $C_0$, i.e.\n",
    "\n",
    "    (a) Sensitivity of $C_0$ to the bias unit is as follows: $\\frac{\\partial C_0}{\\partial b^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial b^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\frac{\\partial C_0}{\\partial a^{(L)}}$,  and can be expaned as follows: \n",
    "    \n",
    "    $\\frac{\\partial C_0}{\\partial b^{(L)}} = 1 \\cdot \\sigma'(z^{(L)}) \\cdot 2(a^{(L)} - y)$\n",
    "    \n",
    "    (b) Sensitivity of $C_0$ to the activation unit of the previous layer is as follows: $\\frac{\\partial C_0}{\\partial a^{(L - 1)}} = \\frac{\\partial z^{(L)}}{\\partial a^{(L - 1)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\frac{\\partial C_0}{\\partial a^{(L)}}$, which can be expanded as follows:\n",
    "    \n",
    "    $\\frac{\\partial C_0}{\\partial a^{(L)}} = w^{(L)} \\cdot \\sigma'(z^{(L)}) \\cdot 2(a^{(L)} - y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.3. Working backward through each layer\n",
    "\n",
    "This same set of processes can be used up the chain from layer to layer in order to work out $C_0$'s sensitivities to all elements in the previous layer, i.e. $L - 2$, and so on all the way back to the first hidden layer.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\ChainRule2.PNG\" width=\"60%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Calculating the Total Cost\n",
    "\n",
    "The above process is repeated for every sample.  The derivative of the full cost function is therefore equal to the average of the derivative cost function for all samples:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{n}\\sum_{k=0}^{n -1} \\frac{\\partial C_k}{\\partial w^{(L)}}\n",
    "\\end{align}\n",
    "\n",
    "But it gets more complicated! $\\frac{1}{n}\\sum_{k=0}^{n -1} \\frac{\\partial C_k}{\\partial w^{(L)}}$ is simply <b>one</b> component of the gradient vector, which also includes the derivatives of the cost function and each bias:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNCalculus3.PNG\" width=\"20%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Zooming out... what do these chain rule expressions do?\n",
    "\n",
    "These chain rule expressions provide the derivatives that determine each component in the gradient vector that helps minimize the cost by repeatedly stepping downhill to converge on a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. A Worked Example\n",
    "\n",
    "This example is based on [this](https://maviccprp.github.io/a-neural-network-from-scratch-in-just-a-few-lines-of-python-code/) implementation and article.  My aim is to:\n",
    "\n",
    "1. Consolidate the theory described above and in my Week 4 notes, and if necessary update both to clarify any misunderstandings / errors in intutition.\n",
    "\n",
    "\n",
    "2. Work through step by step, math alongside code, each step in the feed forward and backpropagation algorithms, how they relate and how they optimise and \"train\" a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Training a Neural Network\n",
    "\n",
    "Training a neural network involves the following steps:\n",
    "\n",
    "1. Choose a network architecture, including:\n",
    "\n",
    "    (a) Number of <b>input units</b>, which will = the number of <b>features</b> of $x^{(i)}$ (aka $x^{(i)}$'s dimensions.\n",
    "    \n",
    "    (b) Number of <b>output units</b>, which will = the number of <b>classes</b> $K$.\n",
    "    \n",
    "    (c) Number of <b>hidden units</b>, which can be any number. Usually the more the better, but need to balance with the increased computation cost.\n",
    "    \n",
    "    By default it is suggested that you:\n",
    "    \n",
    "    * include at least 1 hidden layer; and\n",
    "    <br>\n",
    "    <br>    \n",
    "    * if you have +1 hidden layers you have the same number of units in every hidden layer (although more complex models diverge from this guidance.\n",
    "\n",
    "\n",
    "2. Set up the coding environment, including loading any libraries / dependencies and the dataset.\n",
    "\n",
    "\n",
    "3. Randomly initialize the weights, $\\theta$, connecting each node and summarised in the $\\Theta$ matrix.  \n",
    "\n",
    "    <b>Note:</b> The reason for this is that setting all parameters to 0 does <b>not</b> work for neural networks as each unit will update to the same value repeatedly, which is not what we want!  Randomly initialising the parameters is known as \"<b>Symmetry Breaking</b>.\n",
    "\n",
    "\n",
    "4. Implement forward propagation to get $h_\\Theta(x^{(i)})$ for any $x^{(i)}$.\n",
    "\n",
    "\n",
    "5. Implement the cost function to compute the difference between $h_\\Theta(x^{(i)})$ and $y$ for any $x^{(i)}$.\n",
    "\n",
    "\n",
    "6. Implement backpropagation to compute partial derivatives of each weight in the network.\n",
    "\n",
    "\n",
    "7. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.\n",
    "\n",
    "\n",
    "8. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Implementing a Learning Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Choosing the Architecture\n",
    "\n",
    "The chosen architecture is the following:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNTut1.PNG\" width=\"70%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Set up the Environment and Load the Dataset\n",
    "\n",
    "### (a) Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Define the Dataset\n",
    "\n",
    "The toy dataset used represents an $XOR$ function, i.e. $y = 1$ where <b>one but not both</b> of $x_1$ and $x_2$ = 1.  In all other scenarios, $y = 0$.  \n",
    "\n",
    "In the below:\n",
    "\n",
    "* each row = a sample; and \n",
    "\n",
    "\n",
    "* each column = a feature (or label in the case of $y$).\n",
    "\n",
    "There are a total of $4$ samples:\n",
    "    \n",
    "| $x_1$ | $x_2$ | $y$|Sample|\n",
    "|-------|-------|----|------|\n",
    "| 1     | 1     | 0  | $x_0$|\n",
    "| 1     | 0     | 1  | $x_1$|\n",
    "| 0     | 1     | 1  | $x_2$|\n",
    "| 0     | 0     | 0  | $x_3$|\n",
    "\n",
    "As before, we need to add a bias unit, $x_0 = 1$ for each sample to ensure the linear function is not required to pass through the origin.  Updating the dataset this way looks like this:\n",
    "\n",
    "| $x_0$ | $x_1$ | $x_2$ | $y$ |Sample|\n",
    "|-------|-------|-------|-----|------|\n",
    "| 1     | 1     | 1     | 0   | $x_0$|\n",
    "| 1     | 1     | 0     | 1   | $x_1$|\n",
    "| 1     | 0     | 1     | 1   | $x_2$|\n",
    "| 1     | 0     | 0     | 0   | $x_3$|\n",
    "\n",
    "We can easily create the dataset of features and bias unit like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data set XOR\n",
    "X = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 0],\n",
    "])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly sense check the dimensions of $X$ and $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (4, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:\n",
    "\n",
    "* $X$ is a matrix with $4$ rows and $3$ columns; and\n",
    "\n",
    "\n",
    "* $y$ is a matrix with $4$ rows and $1$ column.\n",
    "\n",
    "\n",
    "* Each row in $X$ and $y$ represent a single sample.  \n",
    "\n",
    "\n",
    "* Each column in $X$ represents a single feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Define a Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Define the number of Epochs for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Initialise the Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialise the weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 5), (5, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialises matrix of weights to map from layer 0 (input layer) to layer 1 (hidden layer).\n",
    "w01 = np.ones((len(X[0]), 5))\n",
    "\n",
    "# Initialises matrix of weights to map from layer 1 (hidden layer) to layer 2 (output layer).\n",
    "w12 = np.ones((5, 1))\n",
    "\n",
    "w01.shape, w12.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose of `w01` and `w12`\n",
    "\n",
    "* `w01` is a matrix of weights connecting the input nodes to the hidden layer nodes, i.e. from layer $0$ to $1$.\n",
    "\n",
    "\n",
    "* `w12` is a matrix of weights connecting the hidden layer nodes to the output node, i.e. from layer $1$ to $2$.\n",
    "\n",
    "\n",
    "* Note, usually neural networks use random values for the intial weights.  However, to keep calculations simple for the purpose of intuition we've initialised the matrixes with $1$'s.\n",
    "\n",
    "#### Dimensionality of `w01` and `w12`\n",
    "\n",
    "* `w01` has $3$ rows and $5$ columns, where:\n",
    "<br>\n",
    "<br>\n",
    "    * the number of rows = the number of input nodes / features $x_0$, $x_1$ and $x_2$, i.e. $3$; and\n",
    "<br>\n",
    "<br>\n",
    "    * the number of columns = the number of connections from each input node to each L+1 node, i.e. $5$.\n",
    "\n",
    "\n",
    "* `w12` has $5$ rows and $1$ column, where:\n",
    "<br>\n",
    "<br>\n",
    "    * the number of rows = the number of hidden layer nodes $h_1$, $h_2$, $h_3$, $h_4$ and $h_5$, i.e. $5$; and\n",
    "<br>\n",
    "<br>\n",
    "    * the number of columns = the number of output nodes $a_0$, i.e. $1$.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Implementing Forward Propagation\n",
    "\n",
    "Recall forward propagation is the algorithm that processes the input variables from input node(s) to output node(s) via each hidden layer of intermediary nodes.  This is possible because:\n",
    "\n",
    "1. Each input feature, $x_{(i)}$ is fed into a separate input node, $x_1$ and $x_2$, along with an additional input node, $x_0$ to represent the bias unit, which is = 0.\n",
    "\n",
    "\n",
    "2. Each neuron in the hidden layer $h_1 - h_5$ receives as its input the weighted sum of the input values from each of $x_0$, $x_1$ and $x_2$.\n",
    "\n",
    "\n",
    "3. Therefore each hidden neuron's input can be expressed as $z_{h_j} = \\sum_{i=0}^3 x_i*w_{ji}$.  This simply means the weighted sum of each input feature.\n",
    "\n",
    "\n",
    "4. Each neuron in the hidden layer $h_1 - h_5$ computes it activation function.  This function:\n",
    "\n",
    "    (a) is the sigmoid function, takes as <i>its</i> input $z_{h_j}$, is expressed as $a_h = \\sigma(z_o)$; and\n",
    "    \n",
    "    (b) calculates the activation value for that neuron, a number ranging from between $0 - 1$.\n",
    "    \n",
    "\n",
    "5. The above process described in steps (3) and (4) is repeated to compute the output neuron $O$.  In other words:\n",
    "\n",
    "    (a) $O$ takes as its input the weighted su of the hidden layer's activation values: $z_o = \\sum_{i=1}^5 h_i*w_{1i}$; and\n",
    "    \n",
    "    (b) calculates its own activation value via the sigmoid function: $a_o = \\sigma(z_o)$.\n",
    "    \n",
    "\n",
    "6. The output value from step (6) is the final classification result for the entire network.\n",
    "\n",
    "Let's implement each of the above steps in code to enable forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Defining the Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derive=False):\n",
    "    if derive:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Line $3$ is the <b>derivative</b> of the sigmoid function.  This is needed for backpropagation.\n",
    "\n",
    "\n",
    "* Line $4$ is the sigmoid function: $\\sigma(x) = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Defining the Feed Forward Pass\n",
    "\n",
    "#### (i) Calculating the Input Values from Layer 0 to Layer 1\n",
    "\n",
    "Recall that the feed forward pass can be vectorised as follows:\n",
    "<p style='text-align: center;'> \n",
    "$\\begin{equation}\n",
    "    z_h = \\begin{pmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 0 \\\\\n",
    "    1 & 0 & 1 \\\\\n",
    "    1 & 0 & 0 \n",
    "    \\end{pmatrix}\n",
    "    \\cdot\n",
    "   \\begin{pmatrix}\n",
    "    1 & 1 & 1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 & 1 & 1\n",
    "   \\end{pmatrix}\n",
    "\\end{equation}$ \n",
    "</p>\n",
    "\n",
    "In the above:\n",
    "\n",
    "* The leftmost matrix is the sample matix $X$, where each row represents a sample and each column represents a feature.\n",
    "\n",
    "\n",
    "* The rightmost matrix is the weights matrix $\\Theta^1$, where each row represents the $5$ weights mapping each of the $3$ input neurons to each of the $5$ hidden neurons.\n",
    "\n",
    "After matrix multiplication the resulting matrix is as follows:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\begin{equation}\n",
    "z_h = \n",
    "\\begin{pmatrix}\n",
    "3 & 3 & 3 & 3 & 3 \\\\\n",
    "2 & 2 & 2 & 2 & 2 \\\\\n",
    "2 & 2 & 2 & 2 & 2 \\\\\n",
    "1 & 1 & 1 & 1 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}$ \n",
    "</p>\n",
    "\n",
    "In the above:\n",
    "\n",
    "* Each row represents the weighted input values to <b>each</b> hiden neuron, $z_{h1} - z_{h5}$, for a <b>single<b> sample.\n",
    "\n",
    "\n",
    "* Each column represents the weighted input values to a <b>single</b> hidden neuron for <b>all</b> samples.\n",
    "\n",
    "In code this computation $z_h = X$ x $\\Theta^1$ is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  3.  3.  3.  3.]\n",
      " [ 2.  2.  2.  2.  2.]\n",
      " [ 2.  2.  2.  2.  2.]\n",
      " [ 1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "z_h = np.dot(X, w01)\n",
    "print(z_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Calculating the Activation Values of Layer 1\n",
    "\n",
    "Now we use our sigmoid function $\\sigma$ to process the weighted input values for each hidden neuron, $z_h$:\n",
    "<p>\n",
    "\\begin{align}\n",
    "\\sigma(z_h) = \\frac{1}{1+e^{-z_h}}\n",
    "\\end{align}\n",
    "</p>\n",
    "\n",
    "Which returns the corresponding activation values of $a_{h1} - a_{h5}$.\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\begin{equation}\n",
    "a_h = \n",
    "\\begin{pmatrix}\n",
    "0.952574 & 0.952574 & 0.952574 & 0.952574 & 0.952574 \\\\\n",
    "0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "0.731059 & 0.731059 & 0.731059 & 0.731059 & 0.73105\n",
    "\\end{pmatrix}\n",
    "\\end{equation}$ \n",
    "</p>\n",
    "\n",
    "In the above:\n",
    "\n",
    "* Each row represents the sigmoid of the weighted input values to <b>each</b> hiden neuron, $a_{h1} - a_{h5}$, for a <b>single<b> sample.\n",
    "\n",
    "\n",
    "* Each column represents the sigmoid of the weighted input values to a <b>single</b> hidden neuron for <b>all</b> samples.\n",
    "\n",
    "In code this computation $a_h = \\sigma(z_h)$ is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95257413  0.95257413  0.95257413  0.95257413  0.95257413]\n",
      " [ 0.88079708  0.88079708  0.88079708  0.88079708  0.88079708]\n",
      " [ 0.88079708  0.88079708  0.88079708  0.88079708  0.88079708]\n",
      " [ 0.73105858  0.73105858  0.73105858  0.73105858  0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "a_h = sigmoid(z_h)\n",
    "print(a_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat steps (i) and (ii) to feed forward from layer 1 to layer 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Calculating the Input Values from Layer 1 to Layer 2\n",
    "\n",
    "The output neuron receives as its input the weighted sum of the activation values from the previous layer like so:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\begin{equation}\n",
    "z_o = \n",
    "\\begin{pmatrix}\n",
    "0.952574 & 0.952574 & 0.952574 & 0.952574 & 0.952574 \\\\\n",
    "0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "0.731059 & 0.731059 & 0.731059 & 0.731059 & 0.73105\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}$ \n",
    "</p>\n",
    "\n",
    "In the above:\n",
    "\n",
    "* The leftmost matrix is the $a_h$, the sigmoided outputs of the weighted inputs to the hidden neurons per the prior step above.\n",
    "\n",
    "\n",
    "* The rightmost matrix is the weights matrix $\\Theta^2$, where each row represents the $5$ weights mapping each of the $5$ hidden neurons to the single output neuron.\n",
    "\n",
    "Which returns the following matrix:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\n",
    "z_o = \n",
    "\\begin{pmatrix}\n",
    "4.762870 \\\\\n",
    "4.403985 \\\\\n",
    "4.403985 \\\\\n",
    "3.655293\n",
    "\\end{pmatrix}$\n",
    "</p>\n",
    "\n",
    "In the above:\n",
    "\n",
    "* Each row in the above corresponds to the weighted inputs to the output neuron, $z_{o1} - z_{o4}$ for a <b>single</b> example.\n",
    "\n",
    "\n",
    "* The entire vector represents each of the weighted inputs to the output neuron for <b>all</b> examples.\n",
    "\n",
    "In code this computation $a_h$ x $\\Theta^3$ is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.76287063]\n",
      " [ 4.40398539]\n",
      " [ 4.40398539]\n",
      " [ 3.65529289]]\n"
     ]
    }
   ],
   "source": [
    "z_o = np.dot(a_h, w12)\n",
    "print(z_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Calculating the Activation Value of Layer 2\n",
    "\n",
    "Now we use our sigmoid function $\\sigma$ to process the weighted input values for each hidden neuron, $z_o$:\n",
    "<p>\n",
    "\\begin{align}\n",
    "\\sigma(z_o) = \\frac{1}{1+e^{-z_o}}\n",
    "\\end{align}\n",
    "</p>\n",
    "\n",
    "Which returns the activation value for $a_o$:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\n",
    "a_o =\n",
    "\\begin{pmatrix} \n",
    "0.991531 \\\\\n",
    "0.987919 \\\\\n",
    "0.987919 \\\\\n",
    "0.974798\n",
    "\\end{pmatrix}$\n",
    "</p>\n",
    "\n",
    "In the above:\n",
    "\n",
    "* Each row represents the sigmoid of the weighted input values to the output neuron, $a_{o}$, for a <b>single<b> sample.\n",
    "\n",
    "\n",
    "* The entire vector represents the sigmoid of the weighted input values to the output neuron for <b>all</b> samples.\n",
    "\n",
    "In code this computation $\\sigma(a_o)$ looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99153128]\n",
      " [ 0.98791922]\n",
      " [ 0.98791922]\n",
      " [ 0.97479766]]\n"
     ]
    }
   ],
   "source": [
    "a_o = sigmoid(z_o)\n",
    "print(a_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats the first forward pass done! Note, we've used vectorised implementation to:\n",
    "\n",
    "* Process all x 4 samples from our dataset at once; and\n",
    "\n",
    "\n",
    "* In doing so, calculated the activation values (or predictions) our network suggests for each sample in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Calculating the Network's Error / Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After step 4 we have the final output for the entire dataset.  Therefore we are ready to calculate the network's error.\n",
    "\n",
    "First we need to define the error function.  Recall that the error function is: $E_{a_o}(x)= \\frac{1}{2}(h(x) - y_{x})^2$.  For instance, the error for the first sample $x^0$ would be: \n",
    "<br>\n",
    "<br>\n",
    "<center>$E_{a_o}(x_0)= \\frac{1}{2}(0.991531 - 0)^2 = 0.491567$</center>  \n",
    "\n",
    "Again, we can vectorise this to compute the error for each sample simultaneously, which is:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\begin{equation}\n",
    "E_{a_0} = \\frac{1}{2}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "0.991531 \\\\\n",
    "0.987919 \\\\\n",
    "0.987919 \\\\\n",
    "0.974798\n",
    "\\end{pmatrix}\n",
    "-\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "^2\n",
    "\\end{equation}$ \n",
    "</p>\n",
    "\n",
    "Which returns:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\begin{equation}\n",
    "E_{a_0} = \n",
    "\\begin{pmatrix}\n",
    "0.491567 \\\\\n",
    "0.000073 \\\\\n",
    "0.000073 \\\\\n",
    "0.475116\n",
    "\\end{pmatrix}\n",
    "\\end{equation}$\n",
    "</p>\n",
    "\n",
    "In the above:\n",
    "\n",
    "* Each row represents the $E$ for a <b>single</b> sample; and\n",
    "\n",
    "\n",
    "* The entire vector represents the $E$ for <b>all</b> samples.\n",
    "\n",
    "In code this computation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.91567136e-01]\n",
      " [  7.29725915e-05]\n",
      " [  7.29725915e-05]\n",
      " [  4.75115235e-01]]\n"
     ]
    }
   ],
   "source": [
    "a_o_error = ((1 / 2) * (np.power((a_o - y), 2)))\n",
    "print(a_o_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we interpret from the above? Recall that the expected values for each sample are as follows:\n",
    "\n",
    "| $x_0$ | $x_1$ | $x_2$ | $y$ | $h(x) $  | $E_x$    |Sample|\n",
    "|-------|-------|-------|-----|----------|----------|------|\n",
    "| 1     | 1     | 1     | 0   | 0.991531 | 0.491567 |$x^0$ |\n",
    "| 1     | 1     | 0     | 1   | 0.987919 | 0.000073 |$x^1$ |\n",
    "| 1     | 0     | 1     | 1   | 0.987919 | 0.000073 |$x^2$ |\n",
    "| 1     | 0     | 0     | 0   | 0.974798 | 0.475116 |$x^3$ |\n",
    "\n",
    "As a we can see from the above, the network produced:\n",
    "\n",
    "1. A very <b>accurate</b> prediction for $x_1 $ and $x_2$, i.e. because $h(x)$ is close to 1 when it should be 1; and\n",
    "\n",
    "\n",
    "2. A very <b>inaccurate</b> prediction for $x_0$ and $x_3$, i.e. because $h(x)$ is close to 1 when it should be 0.\n",
    "\n",
    "In other words, our network's accuracy is pretty mixed \"as is\".  To improve accuracy we need to optimise the weights across each layer in the network to minimize this error.  This is the process of backpropation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Implementing Backpropagation\n",
    "\n",
    "Recall that the essential idea of backpropagation is the <b>backward pass</b>. It takes the error and passes it backward through the whole network, to find out, how much the weights contributed to the error and therefore the consequential adjustments necessary to minimize the error.\n",
    "\n",
    "This process can be visualised as follows:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNTut3.PNG\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "There's a lot going on here.  Let's break it down and explain each backward step one by one and then put it all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Calculate the Update Matrix for the <u>Weights<u> of the <u>Output Layer<u>\n",
    "\n",
    "Identify how much the weights, $w_{12}$ linking the hidden layer to the output layer contribute to the current error $E$.\n",
    "\n",
    "Mathematically we can represent the necessary equation as follows:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$\\frac{\\delta E}{\\delta w12} = \\frac{\\delta E}{\\delta a_{o}} * \\frac{\\delta a_{o}}{\\delta z_{o}} * \\frac{\\delta z_{o}}{\\delta w}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Step 1 - Find $\\frac{\\delta E}{\\delta a_o}$\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "Find the derivative of the error function $E$ w.r.t. the output of the output neuron $a_o$ to identify how much the output neuron's activation value contributes to the total error.  This step circled red:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNTut4.PNG\" width=\"30%\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "1. First, find the derivative of the error function $E$ with respect to the output of the output neuron $a_0$:\n",
    "<br>\n",
    "<center>$\\frac{\\delta E}{\\delta a_{o}} = 2*\\frac{1}{2}(f(x) - y)^{2-1}$</center>\n",
    "<br>\n",
    "<center>which becomes</center>\n",
    "<br>\n",
    "<center>$\\frac{\\delta E}{\\delta a_{o}} = f(x) - y$</center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "2. Second, we use the derivative function above to calculate the sensitivity of the error function to changes in the output neuron's activiation value for sample $x_o$:\n",
    "\n",
    "<center>$\\frac{\\delta E(x_0)}{\\delta a_{o}}= 0.9915317 - 0 = 0.991531$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "Altogether, for each sample the results are as follows:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\frac{\\delta a_o}{\\delta z_{o}} =\n",
    "\\begin{pmatrix}\n",
    "    0.991531 \\\\\n",
    "    {-}0.012081 \\\\\n",
    "    {-}0.012081\\\\\n",
    "    0.974798\n",
    "\\end{pmatrix}$\n",
    "</p>\n",
    "\n",
    "In code this computation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99153128]\n",
      " [-0.01208078]\n",
      " [-0.01208078]\n",
      " [ 0.97479766]]\n"
     ]
    }
   ],
   "source": [
    "delta_a_o_error = a_o - y\n",
    "print(delta_a_o_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Step 2 - Find $\\frac{\\delta a_{o}}{\\delta z_{o}}$\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "Find the derivative of the $a_o$ w.r.t. its input $z_o$ to identify how much the output neuron's activation value $a_o$ is affected by changes in the weighted inputs to it, i.e. $z_o$.  This step circled red:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNTut5.PNG\" width=\"30%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "1. First, find the derivative of the output neuron's activation value $a_o$ w.r.t the weighted input values to it, i.e. $z_o$, which is $\\frac{\\delta a}{\\delta z_{o}} = a_o * (1 - a_o)$.\n",
    "\n",
    "\n",
    "2. Second, we use the derivative function above to calculate the sensitivity of the activation value of the output neuron to changes in the weighted input values $z_o$ for that neuron for sample $x_o$:\n",
    "\n",
    "<center>$\\frac{\\delta a_o}{\\delta z_{o}}= 0.991531 * (1 - 0.991531) = 0.008398$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "Altogether, for each sample the results are as follows:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\frac{\\delta a_o}{\\delta z_{o}} =\n",
    "\\begin{pmatrix}\n",
    "    0.008398 \\\\\n",
    "    0.011935 \\\\\n",
    "    0.011935 \\\\\n",
    "    0.024567\n",
    "\\end{pmatrix}$\n",
    "</p>\n",
    "\n",
    "In code, this computation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.008397  ]\n",
      " [ 0.01193483]\n",
      " [ 0.01193483]\n",
      " [ 0.02456719]]\n"
     ]
    }
   ],
   "source": [
    "delta_z_o = sigmoid(a_o,derive=True)\n",
    "print(delta_z_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Step 3 - Find $\\frac{\\delta z_{o}}{\\delta w_{o}}$\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "Find the derivative of the $z_o$ w.r.t. its weights $w_o$ to identify how much do the weighted inputs $z_o$ change with respect to $w_{11} - w_{15}$.  This step circled red:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNTut6.PNG\" width=\"30%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "1. First, find the derivative of the output neuron's weighted input values $z_o$ w.r.t the weights, i.e. $w_o$, which are:\n",
    "\n",
    "    (a) $\\frac{\\delta z_{o}}{\\delta w_{11}} = 1 * a_{h_1} * w_{11}^{1-1} + 0 + 0 + 0 + 0 = a_{h_1}$\n",
    "    \n",
    "    (b) $\\frac{\\delta z_{o}}{\\delta w_{12}} = 0 + 1 * a_{h_2} * w_{12}^{1-1} + 0 + 0 + 0 = a_{h_2}$\n",
    "    \n",
    "    (c) $\\frac{\\delta z_{o}}{\\delta w_{13}} = 0 + 0 + 1 * a_{h_3} * w_{13}^{1-1} + 0 + 0 = a_{h_3}$\n",
    "    \n",
    "    (d) $\\frac{\\delta z_{o}}{\\delta w_{14}} = 0 + 0 + 0 + 1 * a_{h_4} * w_{14}^{1-1} + 0= a_{h_4}$\n",
    "    \n",
    "    (e) $\\frac{\\delta z_{o}}{\\delta w_{15}} = 0 + 0 + 0 + 0 + 1 * a_{h_5} * w_{15}^{1-1} = a_{h_5}$\n",
    "\n",
    "\n",
    "2. Second, the values for $a_h$ have already been calculated the previous steps, so we can copy them from before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\frac{\\delta a_o}{\\delta z_{o}} =\n",
    "\\begin{pmatrix}\n",
    "    0.952574 & 0.952574 & 0.952574 & 0.952574 & 0.952574 \\\\\n",
    "    0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "    0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "    0.731059 & 0.731059 & 0.731059 & 0.731059 & 0.731058\n",
    "\\end{pmatrix}$\n",
    "</p>\n",
    "\n",
    "Where each row in the above represents the activation values of each node in the the hidden layer per sample.\n",
    "\n",
    "In code, this is computed by assigning the $a_h$ values to a new variable, `delta_w12` which represents $\\frac{\\delta z_{o}}{\\delta w_{o}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95257413  0.95257413  0.95257413  0.95257413  0.95257413]\n",
      " [ 0.88079708  0.88079708  0.88079708  0.88079708  0.88079708]\n",
      " [ 0.88079708  0.88079708  0.88079708  0.88079708  0.88079708]\n",
      " [ 0.73105858  0.73105858  0.73105858  0.73105858  0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "delta_w12 = a_h\n",
    "print(a_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) Step 4 - Calculate gradients for the Output Layer for each Weight\n",
    "\n",
    "Now we are ready to return to our original equation for calculating the gradients for the Output Layer for each weight, i.e. this:\n",
    "<br>\n",
    "<br>\n",
    "<center>$\\frac{\\delta E}{\\delta w12} = \\frac{\\delta E}{\\delta a_{o}} * \\frac{\\delta a_{o}}{\\delta z_{o}} * \\frac{\\delta z_{o}}{\\delta w}$</center>\n",
    "\n",
    "However, because of matrices dimensions not being aligned, we need to rewrite this equation as follows to ensure a final output is possible via matrix multiplication:\n",
    "\n",
    "<center>\n",
    "$\\frac{\\delta E}{\\delta w} = \\frac{\\delta z_{o}}{\\delta w}.T * (\\frac{\\delta E}{\\delta a_{o}} * \\frac{\\delta a_{o}}{\\delta z_{o}})$\n",
    "</center>\n",
    "\n",
    "Expanding the above with the matrices from the previous steps, this equation is as follows:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\begin{equation}\n",
    "\\frac{\\partial E}{\\partial w} = \n",
    "\\begin{pmatrix}\n",
    "0.952574 & 0.952574 & 0.952574 & 0.952574 & 0.952574 \\\\\n",
    "0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "0.731059 & 0.731059 & 0.731059 & 0.731059 & 0.731058\n",
    "\\end{pmatrix}^{T}\n",
    "*\n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0.991531 \\\\\n",
    "- 0.012081 \\\\\n",
    "- 0.012081 \\\\\n",
    "0.974798\n",
    "\\end{pmatrix}\n",
    "\\circ\n",
    "\\begin{pmatrix}\n",
    "    0.008398 \\\\\n",
    "    0.011935 \\\\\n",
    "    0.011935 \\\\\n",
    "    0.024567\n",
    "\\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}$ \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> $\\frac{\\delta E}{\\delta a_{o}}$ and $\\frac{\\delta a_{o}}{\\delta z_{o}}$ are multiplied using the <b>hadamard product</b> ($\\circ$).\n",
    "\n",
    "The result is as follows, which is the matrix of updates to the output layer:\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "$\\frac{\\delta E}{\\delta w} = \n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "0.025184 \\\\\n",
    "0.025184 \\\\\n",
    "0.025184 \\\\\n",
    "0.025184 \\\\\n",
    "0.025184 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}$\n",
    "    \n",
    "We will use this matrix to update our weights $w_{11} - w_{15}$ in the last part of this implementation.\n",
    "\n",
    "In code, this computation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02518446]\n",
      " [ 0.02518446]\n",
      " [ 0.02518446]\n",
      " [ 0.02518446]\n",
      " [ 0.02518446]]\n"
     ]
    }
   ],
   "source": [
    "delta_output_layer = np.dot(delta_w12.T,(delta_a_o_error * delta_z_o))\n",
    "print(delta_output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Calculate the Update Matrix for the <u>Weights</u> of the <u>Hidden Layer</u>\n",
    "\n",
    "Essentially we repeat the above process, albeit moving from the hidden layer to the input layer.  Similarly, this is to identify how much the weights, $w_{01}$ linking the input layer to the hidden layer contribute to the current error $E$.\n",
    "\n",
    "Mathematically we can represent the necessary equation as follows:\n",
    "<br>\n",
    "<br>\n",
    "<center>$\\frac{\\delta E}{\\delta w01} = \\frac{\\delta E}{\\delta a_{h}} * \\frac{\\delta a_h}{\\delta z_h} * \\frac{\\delta z_{h}}{\\partial w}$</center>\n",
    "\n",
    "We can visualise this process as follows:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNTut7.PNG\" width=\"50%\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Step 1 - Find $\\frac{\\delta E}{\\delta a_{h}}$\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "Find the derivative of the $E_{a_{o}}$ w.r.t. the output of the hidden neuron $a_h$ to identify how much the hidden neuron's activation value contributes to the total error.  This step circled red:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNTut8.PNG\" width=\"60%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "We need to calculate $\\frac{\\delta E}{\\delta a_{h}} = \\frac{\\delta E}{\\delta z_{o}} * \\frac{\\delta z_{o}}{\\delta a_{h}}$.  We need to break this down into further steps:\n",
    "\n",
    "\n",
    "1. To find the <b>first term</b> in the above, $\\frac{\\delta E}{\\delta z_{o}} = \\frac{\\delta E}{\\delta a_{o}} * \\frac{\\delta a_{o}}{\\delta z_{o}}$, we:\n",
    "\n",
    "    (a) take the first term of <i>that</i> equation, $\\frac{\\delta E}{\\delta a_{o}}$, from a previous step, which was as follows:\n",
    "    <br>\n",
    "    <center>\n",
    "    $\\frac{\\delta E}{\\delta a_{o}} = \\begin{pmatrix}\n",
    "    0.991531 \\\\\n",
    "    - 0.012081 \\\\\n",
    "    - 0.012081 \\\\\n",
    "    0.974798\n",
    "    \\end{pmatrix}$\n",
    "    </center>\n",
    "    \n",
    "    (b) take the second term of <i>that</i> equation, $\\frac{\\delta a_{o}}{\\delta z_{o}}$ from a previous step, which were as follows:\n",
    "<br>\n",
    "<center>\n",
    "$\\frac{\\delta a_{o}}{\\delta z_{o}} = \\begin{pmatrix}\n",
    "    0.008398 \\\\\n",
    "    0.011935 \\\\\n",
    "    0.011935 \\\\\n",
    "    0.024567\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "2. Multiply using the hadamard product, i.e. $\\frac{\\delta E}{\\delta a_{o}} \\circ \\frac{\\delta a_{o}}{\\delta z_{o}}$ to produce:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "$\\frac{\\delta E_{o}}{\\delta z_{o}} = \\begin{pmatrix}\n",
    "    0.008327\\\\\n",
    "    - 0.000144\\\\\n",
    "    - 0.000144\\\\\n",
    "    0.023948\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "3. To find the <b>second term</b> in the above, $\\frac{\\delta z_{o}}{\\delta a_{h}}$, we do the following:\n",
    "\n",
    "<center>$z_o = w_{11}*a_{hidden} + w_{12}*a_{hidden} + w_{13}*a_{hidden} + w_{14}*a_{hidden} + w_{15}*a_{hidden}$</center>\n",
    "<br>\n",
    "\n",
    "<center>which becomes</center>\n",
    "<br>\n",
    "\n",
    "<center>$\\frac{\\delta z_{o}}{\\delta a_{h}} = w$</center>\n",
    "<br>\n",
    "<center>which gives us this vector</center>\n",
    "<br>\n",
    "<center> w =   \n",
    "$\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{pmatrix}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "Now we can calculate the full equation $\\frac{\\delta E}{\\delta a_{h}}$ via matrix multiplication.  Again, to ensure dimensionality we need to transpose $w$:\n",
    "<center>\n",
    "$\\begin{pmatrix}\n",
    "0.008327 \\\\\n",
    "- 0.000144 \\\\\n",
    "- 0.000144 \\\\\n",
    "0.023948\n",
    "\\end{pmatrix}\n",
    "*\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{pmatrix}.T\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0.008326 & 0.008326 & 0.008326 & 0,008326 & 0,008326 \\\\\n",
    "- 0.000144 & - 0.000144 & - 0.000144 & - 0.000144 & - 0.000144 \\\\\n",
    "- 0.000144 & - 0.000144 & - 0.000144 & - 0.000144 & - 0.000144 \\\\\n",
    "0.023948 & 0.023948 & 0.023948 & 0.023948 & 0.023948 \n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "In code this computation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00832589  0.00832589  0.00832589  0.00832589  0.00832589]\n",
      " [-0.00014418 -0.00014418 -0.00014418 -0.00014418 -0.00014418]\n",
      " [-0.00014418 -0.00014418 -0.00014418 -0.00014418 -0.00014418]\n",
      " [ 0.02394804  0.02394804  0.02394804  0.02394804  0.02394804]]\n"
     ]
    }
   ],
   "source": [
    "delta_a_h = np.dot(delta_a_o_error * delta_z_o, w12.T)\n",
    "print(delta_a_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Step 2 - Find $\\frac{\\delta a_h}{\\delta z_h}$\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "Find the derivative of the $a_h$ w.r.t. the weighted inputs to it $z_h$ to identify how much the hidden neuron's weighted inputs contribute to the total error.  This step circled red:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNTut9.PNG\" width=\"60%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "1. First, find the derivative of $\\frac{\\delta a_h}{\\delta z_h}$, which is $a_h * (1-a_h)$.\n",
    "\n",
    "\n",
    "2. Calculate the derivative's value, which is as follows:\n",
    "\n",
    "<center>\n",
    "$\\begin{pmatrix}\n",
    "    0.952574 & 0.952574 & 0.952574 & 0.952574 & 0.952574 \\\\\n",
    "    0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "    0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "    0.731059 & 0.731059 & 0.731059 & 0.731059 & 0.731058\n",
    "\\end{pmatrix}\n",
    "*\n",
    "\\begin{pmatrix}\n",
    "1 -\n",
    "\\begin{pmatrix}\n",
    "    0.952574 & 0.952574 & 0.952574 & 0.952574 & 0.952574 \\\\\n",
    "    0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "    0.880797 & 0.880797 & 0.880797 & 0.880797 & 0.880797 \\\\\n",
    "    0.731059 & 0.731059 & 0.731059 & 0.731059 & 0.731058\n",
    "\\end{pmatrix}\n",
    "\\end{pmatrix}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "The above matric multiplication results in the following:\n",
    "<br>\n",
    "<br>\n",
    "<center>$ \\frac{\\delta a_h}{\\delta z_h} =\n",
    "\\begin{pmatrix}\n",
    "0.045177 & 0.045177 & 0.045177 & 0.045177 & 0.045177 \\\\\n",
    "0.104993 & 0.104993 & 0.104993 & 0.104993 & 0.104993 \\\\\n",
    "0.104993 & 0.104993 & 0.104993 & 0.104993 & 0.104993 \\\\\n",
    "0.196612 & 0.196612 & 0.196612 & 0.196612 & 0.196612\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "In code this computation is `a_h * (1 - a_h`.  As that represents the derivative of $\\sigma(a_h)$ and because our sigmoid function already allows us to return either the sigmoid value (e.g. if the argument `derive=False`) or the sigmoid's derivative's value (e.g. if the argument `derive=True`) we can compute this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04517666  0.04517666  0.04517666  0.04517666  0.04517666]\n",
      " [ 0.10499359  0.10499359  0.10499359  0.10499359  0.10499359]\n",
      " [ 0.10499359  0.10499359  0.10499359  0.10499359  0.10499359]\n",
      " [ 0.19661193  0.19661193  0.19661193  0.19661193  0.19661193]]\n"
     ]
    }
   ],
   "source": [
    "delta_z_h = sigmoid(a_h,derive=True)\n",
    "print(delta_z_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Step 3 - Find $\\frac{\\delta z_{h}}{\\partial w}$\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "Find the derivative of the $z_h$ w.r.t. its weights $w_h$ to identify how much do the weighted inputs $z_h$ change with respect to $w_{01} - w_{03}$.  This step circled red:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\NNTut10.PNG\" width=\"60%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "Find the derivative of $\\frac{\\delta z_h}{\\delta w_h}$, which is $z_{h} = x_0 * w + x_1 * w$, or in other words:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$\\frac{\\delta z_{h}}{\\partial w} = x$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "The result is simply our dataset $X$:\n",
    "\n",
    "<center>\n",
    "$\\frac{\\delta z_{h}}{\\partial w} =\n",
    "\\begin{pmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 0 \\\\\n",
    "    1 & 0 & 1 \\\\\n",
    "    1 & 0 & 0 \n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "In code we assign this to a new variable, `delta_w01` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "delta_w01 = X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) Step 4 - Calculate gradients for the Hidden Layer for each Weight\n",
    "\n",
    "Now we are ready to return to our original equation for calculating the gradients for the Output Layer for each weight, i.e. this:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>$\\frac{\\delta E}{\\delta w01} = \\frac{\\delta E}{\\delta a_{h}} * \\frac{\\delta a_h}{\\delta z_h} * \\frac{\\delta z_{h}}{\\partial w}$</center>\n",
    "\n",
    "\n",
    "However, because of matrices dimensions not being aligned, we need to rewrite this equation as follows to ensure a final output is possible via matrix multiplication:\n",
    "<center>   \n",
    "$\\frac{\\delta E}{\\delta w} =\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 \n",
    "\\end{pmatrix}.T\n",
    "*\n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0.008327 & 0.008327 & 0.008327 & 0,008327 & 0,008327 \\\\\n",
    "- 0.000144 & - 0.000144 & - 0.000144 & - 0.000144 & - 0.000144 \\\\\n",
    "- 0.000144 & - 0.000144 & - 0.000144 & - 0.000144 & - 0.000144 \\\\\n",
    "0.023948 & 0.023948 & 0.023948 & 0.023948 & 0.023948 \n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "\\circ\n",
    "\\begin{pmatrix}\n",
    "0.045177 & 0.045177 & 0.045177 & 0.045177 & 0.045177 \\\\\n",
    "0.104993 & 0.104993 & 0.104993 & 0.104993 & 0.104993 \\\\\n",
    "0.104993 & 0.104993 & 0.104993 & 0.104993 & 0.104993 \\\\\n",
    "0.196612 & 0.196612 & 0.196612 & 0.196612 & 0.196612\n",
    "\\end{pmatrix}\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "Solving the hadamard product simplifies this to:\n",
    "<br>\n",
    "<br>\n",
    "<center>   \n",
    "$\\frac{\\delta E}{\\delta w} = \n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 \n",
    "\\end{pmatrix}.T\n",
    "*\n",
    "\\begin{pmatrix}\n",
    "0.000376 & 0.000376 & 0.000376 & 0.000376 & 0.000376 \\\\\n",
    "- 0.000015 & - 0.000015 & - 0.000015 & - 0.000015 & - 0.000015 \\\\\n",
    "- 0.000015 & - 0.000015 & - 0.000015 & - 0.000015 & - 0.000015 \\\\\n",
    "0.004708 & 0.004708 & 0.004708 & 0.004708 & 0.004708 \\\\\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "\n",
    "And returns the following update matrix:\n",
    "<br>\n",
    "<br>\n",
    "<center>   \n",
    "$\\frac{\\delta E}{\\delta w} = \n",
    "\\begin{pmatrix}\n",
    "0.00505433 & 0.00505433 & 0.00505433 & 0.00505433 & 0.00505433 \\\\\n",
    "0.000361 & 0.000361 & 0.000361 & 0.000361 & 0.000361 \\\\\n",
    "0.000361 & 0.000361 & 0.000361 & 0.000361 & 0.000361 \\\\\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "In code this computation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00505433  0.00505433  0.00505433  0.00505433  0.00505433]\n",
      " [ 0.000361    0.000361    0.000361    0.000361    0.000361  ]\n",
      " [ 0.000361    0.000361    0.000361    0.000361    0.000361  ]]\n"
     ]
    }
   ],
   "source": [
    "delta_hidden_layer = np.dot(delta_w01.T, delta_a_h * delta_z_h)\n",
    "print(delta_hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Update the Weights Matrices for the entire network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above we have calculated the update matrices that will update the weights between layer 0 and 1 and between 1 and 2.  The only remaining step is to now update the weights matrix using those matrices.  This is as follows:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$w12 = w12 - (eta * \\frac{\\delta E}{\\delta w12})$\n",
    "<br>\n",
    "<br>\n",
    "$w01 = w01 - (eta * \\frac{\\delta E}{\\delta w01})$\n",
    "</center>\n",
    "\n",
    "<b>Note:</b> recall that $eta$ is the learning rate, set to 3 in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Updating the weight matrix for the Output Layer\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$w12 = \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{pmatrix}\n",
    "- (3*\n",
    "\\begin{pmatrix}\n",
    "0.025184 \\\\\n",
    "0.025184 \\\\\n",
    "0.025184 \\\\\n",
    "0.025184 \\\\\n",
    "0.025184 \\\\\n",
    "\\end{pmatrix}\n",
    ") =\n",
    "\\begin{pmatrix}\n",
    "0.92444663 \\\\\n",
    "0.92444663 \\\\\n",
    "0.92444663 \\\\\n",
    "0.92444663 \\\\\n",
    "0.92444663\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "In code this computation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92444663],\n",
       "       [ 0.92444663],\n",
       "       [ 0.92444663],\n",
       "       [ 0.92444663],\n",
       "       [ 0.92444663]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w12 = w12 - eta * delta_output_layer\n",
    "w12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Updating the weight matrix for the Hidden Layer\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$w01 = \n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1\n",
    "\\end{pmatrix}\n",
    "- (3*\n",
    "\\begin{pmatrix}\n",
    "0.000361 & 0.000361 & 0.000361 & 0.000361 & 0.000361 \\\\\n",
    "0.000361 & 0.000361 & 0.000361 & 0.000361 & 0.000361 \\\\\n",
    "0.00505433 & 0.00505433 & 0.00505433 & 0.00505433 & 0.00505433 \\\\\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "\n",
    "Which results in:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$w01 =\\begin{pmatrix}\n",
    "0.98483701 & 0.98483701 & 0.98483701 & 0.98483701 & 0.98483701 \\\\\n",
    "0.99891701 & 0.99891701 & 0.99891701 & 0.99891701 & 0.99891701 \\\\\n",
    "0.99891701 & 0.99891701 & 0.99891701 & 0.99891701 & 0.99891701 \\\\\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "In code this computation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98483701,  0.98483701,  0.98483701,  0.98483701,  0.98483701],\n",
       "       [ 0.99891701,  0.99891701,  0.99891701,  0.99891701,  0.99891701],\n",
       "       [ 0.99891701,  0.99891701,  0.99891701,  0.99891701,  0.99891701]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w01 = w01 - eta * delta_hidden_layer\n",
    "w01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Make a Prediction with the updated Weight Matrices\n",
    "\n",
    "### (a) Calculate the activation values of the hidden layer\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$a_h = \\sigma (\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 0\\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "*\n",
    "\\begin{pmatrix}\n",
    "0.99891701 & 0.99891701 & 0.99891701 & 0.99891701 & 0.99891701 \\\\\n",
    "0.99891701 & 0.99891701 & 0.99891701 & 0.99891701 & 0.99891701 \\\\\n",
    "0.98483701 & 0.98483701 & 0.98483701 & 0.98483701 & 0.98483701\n",
    "\\end{pmatrix} )$\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$a_h =\n",
    "\\begin{pmatrix}\n",
    "0.95178509  & 0.95178509 & 0.95178509 & 0.95178509 & 0.95178509 \\\\\n",
    "0.87908077  & 0.87908077 & 0.87908077 & 0.87908077 & 0.87908077 \\\\\n",
    "0.87908077  & 0.87908077 & 0.87908077 & 0.87908077 & 0.87908077 \\\\\n",
    "0.72806693  & 0.72806693 & 0.72806693 & 0.72806693 & 0.72806693\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "In code this computation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95178509  0.95178509  0.95178509  0.95178509  0.95178509]\n",
      " [ 0.87908077  0.87908077  0.87908077  0.87908077  0.87908077]\n",
      " [ 0.87908077  0.87908077  0.87908077  0.87908077  0.87908077]\n",
      " [ 0.72806693  0.72806693  0.72806693  0.72806693  0.72806693]]\n"
     ]
    }
   ],
   "source": [
    "z_h2 = np.dot(X, w01)\n",
    "a_h2 = sigmoid(z_h2)\n",
    "print(a_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Calculate the activation values of the output layer\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$a_o = \\sigma(\n",
    "\\begin{pmatrix}\n",
    "0.95178509  & 0.95178509 & 0.95178509 & 0.95178509 & 0.95178509 \\\\\n",
    "0.87908077  & 0.87908077 & 0.87908077 & 0.87908077 & 0.87908077 \\\\\n",
    "0.87908077  & 0.87908077 & 0.87908077 & 0.87908077 & 0.87908077 \\\\\n",
    "0.72806693  & 0.72806693 & 0.72806693 & 0.72806693 & 0.72806693\n",
    "\\end{pmatrix}\n",
    "*\n",
    "\\begin{pmatrix}\n",
    "0.92444663 \\\\\n",
    "0.92444663 \\\\\n",
    "0.92444663 \\\\\n",
    "0.92444663 \\\\\n",
    "0.92444663\n",
    "\\end{pmatrix})$\n",
    "</center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "$a_o =\n",
    "\\begin{pmatrix}\n",
    "0.98786405 \\\\\n",
    "0.98309866 \\\\\n",
    "0.98309866 \\\\\n",
    "0.96660214\n",
    "\\end{pmatrix}$\n",
    "</center>\n",
    "\n",
    "In code this computation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.98786405]\n",
      " [ 0.98309866]\n",
      " [ 0.98309866]\n",
      " [ 0.96660214]]\n"
     ]
    }
   ],
   "source": [
    "z_o2 = np.dot(a_h2, w12)\n",
    "a_o2 = sigmoid(z_o2)\n",
    "print(a_o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Checking our $1^{st}$ prediction vs. our $2^{nd}$ prediction\n",
    "\n",
    "As a reminder, the results for $a_o$ after the first prediction were the following:\n",
    "\n",
    "| $x_0$ | $x_1$ | $x_2$ | $y$ | $h(x)^{1} $  | ${E_{x}}^1$|Sample|\n",
    "|-------|-------|-------|-----|--------------|------------|------|\n",
    "| 1     | 1     | 1     | 0   | 0.991531     | 0.491567   |$x^0$ |\n",
    "| 1     | 1     | 0     | 1   | 0.987919     | 0.000073   |$x^1$ |\n",
    "| 1     | 0     | 1     | 1   | 0.987919     | 0.000073   |$x^2$ |\n",
    "| 1     | 0     | 0     | 0   | 0.974798     | 0.475116   |$x^3$ |\n",
    "\n",
    "To update the above with the $2^{nd}$ set of predictions we take the revised $a_o$ values from the above computations and calculate the error $E$ again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.87937686e-01]\n",
      " [  1.42827725e-04]\n",
      " [  1.42827725e-04]\n",
      " [  4.67159847e-01]]\n"
     ]
    }
   ],
   "source": [
    "a_o2_error = ((1 / 2) * (np.power((a_o2 - y), 2)))\n",
    "print(a_o2_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for $a_o$ after the second prediction using the updated weights is the below:\n",
    "\n",
    "| $x_0$ | $x_1$ | $x_2$ | $y$ | $h(x)^{1} $ | ${E_{x}}^1$|$h(x)^{2} $ |${E_{x}}^2$|Sample|\n",
    "|-------|-------|-------|-----|-------------|------------|------------|-----------|------|\n",
    "| 1     | 1     | 1     | 0   | 0.991531    | 0.491567   | 0.98786405 |0.487937686|$x^0$ |\n",
    "| 1     | 1     | 0     | 1   | 0.987919    | 0.000073   | 0.98309866 |0.000014282|$x^1$ |\n",
    "| 1     | 0     | 1     | 1   | 0.987919    | 0.000073   | 0.98309866 |0.000014282|$x^2$ |\n",
    "| 1     | 0     | 0     | 0   | 0.974798    | 0.475116   | 0.96660214 |0.000046715|$x^3$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not especially amazing, but we can draw the following interpretations:\n",
    "\n",
    "* The accuracy is increasing, i.e. the predictions are becoming slightly more accurate for each example; and\n",
    "\n",
    "\n",
    "* The error is reducing, i.e. the error rate has reduced slightly for each example.\n",
    "\n",
    "This is typical of a neural network - it needs to update its operations many times to arrive at weights that minimise the error and improve accuracy of output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Running the Neural Network with Random Weights + 20,000 Epochs!\n",
    "\n",
    "The final code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: \n",
      " [[  7.25523919e-06]\n",
      " [  6.51078122e-06]\n",
      " [  6.18936256e-06]\n",
      " [  3.50692311e-06]]\n",
      "Final a_o: \n",
      " [[ 0.00380926]\n",
      " [ 0.99639146]\n",
      " [ 0.99648166]\n",
      " [ 0.00264837]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(x, derive=False):\n",
    "    if derive:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the data set XOR\n",
    "X = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 0],\n",
    "])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]\n",
    "             ])\n",
    "\n",
    "# Define a learning rate\n",
    "eta = 3\n",
    "# Define the number of epochs for learning\n",
    "epochs = 20000\n",
    "\n",
    "# Initialize the weights with random numbers\n",
    "w01 = np.random.random((len(X[0]), 5))\n",
    "w12 = np.random.random((5, 1))\n",
    "\n",
    "# Start feeding forward and backpropagate *epochs* times.\n",
    "for epoch in range(epochs):\n",
    "    # Feed forward\n",
    "    z_h = np.dot(X, w01)\n",
    "    a_h = sigmoid(z_h)\n",
    "\n",
    "    z_o = np.dot(a_h, w12)\n",
    "    a_o = sigmoid(z_o)\n",
    "\n",
    "    # Calculate the error\n",
    "    a_o_error = ((1 / 2) * (np.power((a_o - y), 2)))\n",
    "\n",
    "    # Backpropagation\n",
    "    ## Output layer\n",
    "    delta_a_o_error = a_o - y\n",
    "    delta_z_o = sigmoid(a_o,derive=True)\n",
    "    delta_w12 = a_h\n",
    "    delta_output_layer = np.dot(delta_w12.T,(delta_a_o_error * delta_z_o))\n",
    "\n",
    "    ## Hidden layer\n",
    "    delta_a_h = np.dot(delta_a_o_error * delta_z_o, w12.T)\n",
    "    delta_z_h = sigmoid(a_h,derive=True)\n",
    "    delta_w01 = X\n",
    "    delta_hidden_layer = np.dot(delta_w01.T, delta_a_h * delta_z_h)\n",
    "\n",
    "    w01 = w01 - eta * delta_hidden_layer\n",
    "    w12 = w12 - eta * delta_output_layer\n",
    "    \n",
    "print('Final Error: \\n {0}'.format(a_o_error))\n",
    "print('Final a_o: \\n {0}'.format(a_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the neural network for 20,000 epochs it's a lot more accurate:\n",
    "\n",
    "| $x_0$ | $x_1$ | $x_2$ | $y$ | $h(x)^{1} $ | ${E_{x}}^1$|$h(x)^{2} $ |${E_{x}}^2$|$h(x)^{3} $ |${E_{x}}^3$|Sample|\n",
    "|-------|-------|-------|-----|-------------|------------|------------|-----------|------------|-----------|------|\n",
    "| 1     | 1     | 1     | 0   | 0.991531    | 0.491567   | 0.98786405 |0.487937686|0.00388388  |0.000000754|$x^0$ |\n",
    "| 1     | 1     | 0     | 1   | 0.987919    | 0.000073   | 0.98309866 |0.000014282|0.99628807  |0.000000688|$x^1$ |\n",
    "| 1     | 0     | 1     | 1   | 0.987919    | 0.000073   | 0.98309866 |0.000014282|0.99633215  |0.000000672|$x^2$ |\n",
    "| 1     | 0     | 0     | 0   | 0.974798    | 0.475116   | 0.96660214 |0.000046715|0.00222113  |0.000000246|$x^3$ |\n",
    "\n",
    "As you can see, the neural network has:\n",
    "\n",
    "* <b>significantly</b> improved its accuracy with respect to predicting the correct result for samples $x_0$ and $x_3$; and \n",
    "\n",
    "\n",
    "* <b>marginally</b> improved its accuracy with respect to predicting the correct result for samples $x_1$ and $x_2$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
