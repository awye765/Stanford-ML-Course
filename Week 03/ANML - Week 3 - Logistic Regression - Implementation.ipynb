{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Logistic Regression - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Logistic Regression - Binary Classification\n",
    "\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this exercise, we'll build a logistic regression model to predict whether a student gets admitted to a university:\n",
    "\n",
    "1. Suppose that you are the administrator of a university department and you want to determine each applicant's chance of admission based on their results on two exams. \n",
    "\n",
    "\n",
    "2. You have historical data from previous applicants that you can use as a training set for logistic regression.  \n",
    "\n",
    "\n",
    "3. For each training example, you have the applicant's scores on two exams and the admissions decision.  \n",
    "\n",
    "To accomplish this, we're going to build a classification model that estimates the probability of admission $y$ based on the exam scores $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "      <th>Admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Exam 1     Exam 2  Admitted\n",
       "0  34.623660  78.024693         0\n",
       "1  30.286711  43.894998         0\n",
       "2  35.847409  72.902198         0\n",
       "3  60.182599  86.308552         1\n",
       "4  79.032736  75.344376         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd() + '\\data\\ex2data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Exam 1', 'Exam 2', 'Admitted'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Do some initial analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "      <th>Admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>65.644274</td>\n",
       "      <td>66.221998</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.458222</td>\n",
       "      <td>18.582783</td>\n",
       "      <td>0.492366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>30.058822</td>\n",
       "      <td>30.603263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.919511</td>\n",
       "      <td>48.179205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.032988</td>\n",
       "      <td>67.682381</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>80.212529</td>\n",
       "      <td>79.360605</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99.827858</td>\n",
       "      <td>98.869436</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Exam 1      Exam 2    Admitted\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean    65.644274   66.221998    0.600000\n",
       "std     19.458222   18.582783    0.492366\n",
       "min     30.058822   30.603263    0.000000\n",
       "25%     50.919511   48.179205    0.000000\n",
       "50%     67.032988   67.682381    1.000000\n",
       "75%     80.212529   79.360605    1.000000\n",
       "max     99.827858   98.869436    1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a scatter plot of the two scores and use color coding to visualize if the example is positive (admitted) or negative (not admitted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Exam 2 Score')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positive = data[data['Admitted'].isin([1])]\n",
    "\n",
    "# Returns only the examples whereby the student was admitted, i.e. there is a 1 in the 'Admitted' column.\n",
    "\n",
    "negative = data[data['Admitted'].isin([0])]\n",
    "\n",
    "# Returns only the examples whereby the student was rejected, i.e. there is a 0 in the 'Admitted' column.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')\n",
    "ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Exam 1 Score')\n",
    "ax.set_ylabel('Exam 2 Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is a clear decision boundary between the two classes.  Now we need to implement logistic regression so we can train a model to predict the outcome.  The equations implemented in the following code samples are detailed in \"ex2.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Define the Training Dataset & Target Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to do some setup, similar to what we did in exercise 1 for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Enable Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Linear Regression, we need to add a column of ones to the training set to enable matrix multiplication for cost and gradient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a ones column - this makes the matrix multiplication work out easier\n",
    "data.insert(0, 'Ones', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means our dataset now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "      <th>Admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones     Exam 1     Exam 2  Admitted\n",
       "0     1  34.623660  78.024693         0\n",
       "1     1  30.286711  43.894998         0\n",
       "2     1  35.847409  72.902198         0\n",
       "3     1  60.182599  86.308552         1\n",
       "4     1  79.032736  75.344376         1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Define the Training Data & Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some variable intialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "\n",
    "cols = data.shape[1]\n",
    "# returns the value 4, i.e. to reflect that there are x 4 columns in our table.\n",
    "\n",
    "X = data.iloc[:,0:cols-1]\n",
    "# i.e. X includes ALL rows but only the FIRST 3 columns.  Note: index 0 is the column of \"Ones\".  In other words, X is a matrix of ALL INPUT VARIABLES, i.e. x0 (Ones), x2 (Exam 1) and x3 (Exam 2).  \n",
    "\n",
    "y = data.iloc[:,cols-1:cols]\n",
    "# i.e. y includes ALL rows but only the last column \"Admitted\".  In other words, y is a matrix of all OUTPUT VARIABLES, i.e. all Addmitted / Not Addmitted results for each training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Converting X and y to numpy matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays and initalize the parameter array theta\n",
    "X = np.array(X.values)\n",
    "y = np.array(y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Initialise theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Checking the matrices shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the shape of our arrays to make sure everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:\n",
    "    \n",
    "* X = 100 rows x 3 columns;\n",
    "\n",
    "\n",
    "* y = 100 rows x 1 column; and\n",
    "\n",
    "\n",
    "* $\\theta$ = 1 row x 3 column \n",
    "\n",
    "Note `.shape` will confirm the dimension of the array, and if the array is 1D, will simply return the number of columns in the format `(row,)`.  If the array is 2D, it will return in the format `(row, column`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Define the Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that this function is $h_\\theta(x) = g(\\theta^{(T)}x)$ where g is the sigmoid function $g(z) = \\frac{1}{1 +e^{-z}}$.\n",
    "\n",
    "The code for this is pretty simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to make sure the function is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.arange(-10, 10, step=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(nums, sigmoid(nums), 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 - Define the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write the cost function to evaluate a solution.  Recall that the cost function in logistic regression is:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}[-y^{(i)}log(h_\\theta(x^{(i)})) - (1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, X, y):\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n",
    "    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n",
    "    return np.sum(first - second) / (len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Intuition: mapping formula to the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`first = np.multiply(-y, np.log(sigmoid(X * theta.T)))` is equivalent to $-y^{(i)}log(h_\\theta(x^{(i)}))$, which calculates the cost where $y = 1$.\n",
    "\n",
    "`second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))` is equivalent to $(1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))$, which calculates the cost where $y = 0$.\n",
    "\n",
    "and `return np.sum(first - second) / len(X))` executes the two parts of the formula together, sums it up and applies the fraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Running the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 - Define the Gradient Descent Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good.  Next we need a function to compute the gradient (parameter updates) given our training data, labels, and some parameters theta.  Recall that the gradient of the cost is a vector of the same length as $\\theta$ where the j<sup>th</sup> element (for j = 0, 1, ... n) is defined as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_j := \\theta - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)\n",
    "\\end{align}\n",
    "\n",
    "Which can be rewritten as:\n",
    "\n",
    "\\begin{align} \n",
    "\\text{repeat until convergence \\{} \\\\\n",
    "\\theta_j & := \\theta_j - \\alpha \\dfrac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\\\ \n",
    "\\text{\\}}\n",
    "\\end{align}\n",
    "\n",
    "This is almost identical to the linear regression gradient descent function, except rather than swapping $\\theta^{T}x$ for $h_\\theta(x)$ in the above, we instead swap $\\frac{1}{1 +e^{-\\theta^{T}x}}$ in for $h_\\theta(x)$ in the above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    grad = np.zeros(parameters)\n",
    "    \n",
    "    error = sigmoid(X * theta.T) - y\n",
    "    # Calculates difference between predicted y value and actual y value for a given input x.  \n",
    "    # Produces column vector of error scores for each example.\n",
    "    \n",
    "    for i in range(parameters):\n",
    "        term = np.multiply(error, X[:,i])\n",
    "        # Iterates through each column in the X matrix one by one, i.e. the Ones col, the Exam 1 col and Exam 2 col.  \n",
    "        # In each such iteration each row of each such column is multipled by the corresponding row of the error vector.     \n",
    "               \n",
    "        grad[i] = np.sum(term) / len(X)\n",
    "        # Sums all values from term (i.e. the particular column of the X matrix under iteration) and divides by number of\n",
    "        # examples, in this case 100 (i.e. because m = 100).  This value is then assigned to the index of grad corresponding\n",
    "        # to the iteration number.\n",
    "    \n",
    "    return grad\n",
    "    # Returns matrix of updated parameters after one gradient step.\n",
    "\n",
    "gradient(theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Breaking down the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Line 2\n",
    "\n",
    "`theta = np.matrix(theta)` creates a numpy matrix containing our theta values, i.e. `matrix([0., 0., 0.])`.\n",
    "\n",
    "#### (ii) Lines 3 & 4\n",
    "\n",
    "`X = np.matrix(X)` creates a numpy matrix containing our X values.\n",
    "\n",
    "`y = np.matrix(y)` creates a numpy matrix containign our y values.\n",
    "\n",
    "#### (iii) Line 5\n",
    "    \n",
    "`parameters = int(theta.ravel().shape[1])`\n",
    "\n",
    "`.ravel()` \"<b>flattens</b>\" the theta np matrix to a single row matrix, i.e. this `[[0., 0., 0.]]` becomes this `[0., 0., 0.]`\n",
    "\n",
    "`.shape[1]` returns the <b>number of columns</b> of that matrix, i.e. 3.\n",
    "\n",
    "`.int`returns the integer value of that in brackets, i.e. 3.\n",
    "\n",
    "All of which is assigned to the variable, `parameters`.\n",
    "\n",
    "<b>Purpose:</b> is the number of weights, in our case only three, i.e. $\\theta_0$, $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "#### (iv) Line 6\n",
    "\n",
    "`grad = np.zeros(parameters)` creates a np matrix with a number of zeros equal to the number of parameters, i.e. 3 zeros so this: `[0., 0., 0.]`\n",
    " \n",
    "#### (v) Line 7\n",
    " \n",
    "`error = sigmoid(X * theta.T) - y` creates a new variable equivalent to $g(h_\\theta(x^{(i)}) - y^{(i)})$.  This is calculating the cost for each training example.  Note that we take the sigmoid of the cost.\n",
    "\n",
    "#### (vi) Line 8\n",
    "  \n",
    "`for i in range(parameters):` iterates through each column of the X matrix one by one.  As parameters = 3 this means perform all operations below it for index items 0, 1 and 2 of the X matrix, i.e. Ones col, Exam 1 col and Exam 2 col respectively.\n",
    "\n",
    "#### (vii) Line 9\n",
    "\n",
    "`term = np.multiply(error, X[:,i])` is the equivalent of $(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "To recap, this multiples each row of the errors vector (`error`) against each of the corresponding $x^{i}$ feature rows (`X[:,j]`) in X. \n",
    "\n",
    "So in each sub-iteration (i.e. within the second for loop `for j in range(parameters)`):\n",
    "\n",
    "1. This is `error` x `X[:,0]`, which is each row of the error vector multiplied by each row of the $x_0$ vector (i.e. the \"ones\"); \n",
    "\n",
    "\n",
    "2. This is `error` x `X[:,1]`, which is each row of the error vector multiplied by each row of the $x_1$ vector (i.e. the \"Exam 1\" figures); and\n",
    "\n",
    "\n",
    "3. This is `error` x `X[:,2]`, which is each row of the error vector multiplied by each row of the $x_2$ vector (i.e. the \"Exam 2\" figures).\n",
    "\n",
    "#### (viii) Line 10\n",
    "\n",
    "`grad[i] = np.sum(term) / len(X)` is the equivalent of $\\frac{1}{m}\\sum J(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "We don't actually perform gradient descent in this function - we just compute a <b>single</b> gradient step.  \n",
    "\n",
    "In the Andrew Ng exercise, an Octave function called \"fminunc\" is used to optimize the parameters given to the cost and gradient functions.  \n",
    "\n",
    "Since we're using Python, we can use SciPy's \"optimize\" namespace to do the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9 - Running Gradient Descent (a single step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a single call to the gradient method using our data and initial paramter values of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gradient(theta, X, y)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 - Using SciPy's TNC to optimise parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use SciPy's truncated newton (TNC) implementation to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain these results:\n",
    "\n",
    "1. <b>`[-25.16131872,   0.20623159,   0.20147149]`</b> are the optimised parameters.\n",
    "<p>\n",
    "    \n",
    "2. <b>36</b> represents the number of function evaluations (aka gradient \"steps\" or \"iterations\") necessary to arrive at the optimal parameters.\n",
    "    \n",
    "<p>\n",
    "3. <b>0</b> is a \"return code\" that means \"Local minimum reached\".  Other Return codes can be found [here](http://pageperso.lif.univ-mrs.fr/~francois.denis/IAAM1/scipy-html-1.0.0/generated/scipy.optimize.fmin_tnc.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11 - Checking the cost function with optimised parameters\n",
    "\n",
    "\n",
    "Let's see what the our cost looks like with this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(result[0], X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12 - Define Decision Boundary Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a function that can map the probability that given an input $x$ it corresponds to $y = 1$ to a corresponding class, i.e. pass or fail.\n",
    "\n",
    "To do so, we calculate the probability that $y = 1$ for each sample, and then use a decision boundary to map that value to either 1 (pass) or 0 (fail):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    probability = sigmoid(X * theta.T)\n",
    "    # Calculates probability that y = 1 for a given set of x values.\n",
    "    return [1 if x >= 0.5 else 0 for x in probability]\n",
    "    # Sets the decision boundary, i.e. mapping the probability to a binary class, in this case 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13 - Plot the Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = data[data['Admitted'].isin([1])]\n",
    "\n",
    "# Returns only the examples whereby the student was admitted, i.e. there is a 1 in the 'Admitted' column.\n",
    "\n",
    "negative = data[data['Admitted'].isin([0])]\n",
    "\n",
    "# Returns only the examples whereby the student was rejected, i.e. there is a 0 in the 'Admitted' column.\n",
    "\n",
    "dec_boundary_plot_x = [np.min(X[:,1]-2), np.max(X[:,2]+2)]\n",
    "\n",
    "dec_boundary_plot_y = -1/(result[0][2])*(result[0][0] + np.dot(result[0][1],dec_boundary_plot_x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')\n",
    "ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')\n",
    "decision_boundary = plt.plot(dec_boundary_plot_x, dec_boundary_plot_y)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Exam 1 Score')\n",
    "ax.set_ylabel('Exam 2 Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 14 - Scoring the Accuracy of our Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this function to score the training accuracy of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_min = np.matrix(result[0])\n",
    "# Creates matrix of the optimised parameters.\n",
    "\n",
    "predictions = predict(theta_min, X)\n",
    "# Takes the optimised paramters and X as its arguments and produces a vector of 0s and 1s corresponding to the predicted\n",
    "# values of y our algorithm generates.\n",
    "\n",
    "correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]\n",
    "# Compares result in predictions vector (a) against the actual y values (b).  If they match, returns 1.  If they do not\n",
    "# match, returns 0.  Produces new vector of 0s and 1s representing whether our algorithm was correct / incorrect for each \n",
    "# prediction. See also here: https://www.codecademy.com/en/forum_questions/526269b2f10c604fc500b53c\n",
    "\n",
    "accuracy = (sum(map(int, correct)) % len(correct))\n",
    "# Calculates what % of our predicted y results were in fact correct when compared with the actual y values.  Concretely\n",
    "# sum(map(int, correct)) counts the number of results that were correct, i.e. all the 1s, which returns a total of 89. \n",
    "# And len(correct) returns the length of the correct vector, i.e. 100 items.  % returns 89 as a % of 100, i.e. 89%\n",
    "\n",
    "print ('accuracy = {0}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression classifer correctly predicted if a student was admitted or not 89% of the time.  Not bad!  Keep in mind that this is training set accuracy though.  We didn't keep a hold-out set or use cross-validation to get a true approximation of the accuracy so this number is likely higher than its true perfomance (this topic is covered in a later exercise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Regularised Logistic Regression\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the Scene\n",
    "\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests.  From these two tests, you would like to determine whether the microchips should be accepted or rejected.  To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Import the Data\n",
    "\n",
    "Similar to part 1, let's start by visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd() + '\\data\\ex2data2.txt'\n",
    "data2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which reflects the following dimensions:\n",
    "    \n",
    "* Test 1 (i.e. $x_1$) = 100 rows x 1 column;\n",
    "\n",
    "\n",
    "* Test 2 (i.e. $x_2$) = 100 rows x 1 column; \n",
    "\n",
    "\n",
    "* Test 1 and Test 2 (i.e. $X$) = 100 rows x 2 columns; and\n",
    "\n",
    "\n",
    "* Accepted (i.e. $y$) = 100 rows x 1 column.\n",
    "\n",
    "Note, once we add the additional column of ones for the $x_0$ feature (per later step below), the dimensions for $X$ alter as follows: \n",
    "\n",
    "* 100 rows x 2 columns &rightarrow; 100 rows x 3 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Visualise the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = data2[data2['Accepted'].isin([1])]\n",
    "negative = data2[data2['Accepted'].isin([0])]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(positive['Test 1'], positive['Test 2'], s=50, c='b', marker='o', label='Accepted')\n",
    "ax.scatter(negative['Test 1'], negative['Test 2'], s=50, c='r', marker='x', label='Rejected')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Test 1 Score')\n",
    "ax.set_ylabel('Test 2 Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data looks a bit more complicated than the previous example.  In particular, you'll notice that there is no linear decision boundary that will perform well on this data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Create Polynomial Features\n",
    "\n",
    "One way to deal with this using a linear technique like logistic regression is to construct features that are derived from polynomials of the original features.  Let's start by creating a bunch of polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 7\n",
    "\n",
    "x1 = data2['Test 1']\n",
    "# The x1 values\n",
    "\n",
    "x2 = data2['Test 2']\n",
    "# The x2 values\n",
    "\n",
    "data2.insert(3, 'Ones', 1)\n",
    "# .insert(3, 'Ones', 1) is adding as new index 3, a column headed 'Ones' with each element to be added being eqaul to 1.  # I.e. it adds to x0 = 1 values for each sample.\n",
    "\n",
    "data2\n",
    "for i in range(1, degree):\n",
    "#Iterate from 1 - (degree-1), i.e. from 1 - 5 (inclusive)\n",
    "\n",
    "    for j in range(0, i+1):\n",
    "    # Iterate from 0 - (i-1) (inclusive)\n",
    "        data2['F' + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)\n",
    "        # First decides exponent for x1 using current values of (i) and (j)\n",
    "        # Second decides exponent for x2 using current value of (j)\n",
    "        # Third multiplies x1^n . x2^n to provide a new polynomial feature value\n",
    "        # Fourth value assigned to new column headed with current value \n",
    "\n",
    "data2.drop('Test 1', axis=1, inplace=True)\n",
    "data2.drop('Test 2', axis=1, inplace=True)\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which equates to the below, where $F10 = x_1$, $F11 = x_2$, $F20 = x_1^{2}$, ... $F66 = x_2^{6}$\n",
    "\n",
    "<img src=\"../Images/polynomials.png\" width=100%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Initialise the Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables like we did in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X and y (remember from above that we moved the label to column 0)\n",
    "cols = data2.shape[1]\n",
    "\n",
    "X2 = data2.iloc[:,1:cols]\n",
    "# Creates array of all X feature values\n",
    "\n",
    "y2 = data2.iloc[:,0:1]\n",
    "# Creates array of all y label values\n",
    "\n",
    "# Below converts X2 and y2 from arrays to numpy arrays and initalize the parameter array theta\n",
    "X2 = np.array(X2.values)\n",
    "y2 = np.array(y2.values)\n",
    "theta2 = np.zeros(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the shape of `X2`, `y2` and `theta2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X2.shape)\n",
    "print(y2.shape)\n",
    "print(theta2.shape)\n",
    "\n",
    "# X2_shape = np.array([X2.shape])\n",
    "# y2_shape = np.array([y2.shape])\n",
    "# theta2_np = np.array([[theta2]])\n",
    "# theta2_shape = theta2_np.shape\n",
    "# theta2_shape[2]\n",
    "\n",
    "\n",
    "# print(\"X2 has {0} rows and {1} columns\".format(X2_shape[0][0], X2_shape[0][1]))\n",
    "# print(\"y2 has {0} rows and {1} columns\".format(y2_shape[0][0], y2_shape[0][1]))\n",
    "# print(\"theta2 has {0} rows and {1} columns\".format(theta2_shape[1], theta2_shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:\n",
    "    \n",
    "* X2 has 118 rows and 28 columns\n",
    "\n",
    "\n",
    "* y2 has 118 rows and 1 column\n",
    "\n",
    "\n",
    "* theta2 has 1 row and 28 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Include the Regularization Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to modify the cost and gradient functions from part 1 to include the regularization term.  First the cost function:\n",
    "\n",
    "## (a) Regularizing the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costReg(theta, X, y, learningRate):\n",
    "    # Below three lines format everything to numpy matrices\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # Below is the left side of the cost function equation for where y = 1\n",
    "    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n",
    "    \n",
    "    # Below is the right side of the cost function for where y = 1\n",
    "    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n",
    "    \n",
    "    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))\n",
    "    # Is the regularisation term, explained below.\n",
    "    \n",
    "    return np.sum(first - second) / (len(X)) + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `reg` term in the equation, which is equivalent to the $+\\lambda \\sum_{j=1}^{m} \\theta_j^2$ at the far right in the below:\n",
    "\n",
    "\\begin{align}\n",
    "J_{reg}(\\theta) = \\frac{1}{2m} \\bigg[-y^{(i)}log(h_\\theta(x^{(i)})) - (1 - y^{(i)})log(1 - h_\\theta(x^{(i)})) + \\lambda \\sum_{j=1}^{m} \\theta_j^2\\bigg]\n",
    "\\end{align}\n",
    "\n",
    "To recap, the `reg` term works as follows:\n",
    "\n",
    "1. Recall that by convention we do not process $x_0$, hence `theta[:,1]` avoids index $0$ of the $\\theta$ matrix, i.e. $x_0$, and instead returns index $1$ of the $\\theta$ matrix, which corresponds to $x_1$.  \n",
    "\n",
    "\n",
    "2. `theta.shape[1]` returns the number of columns in the $\\theta$ matrix, in this case $28$.\n",
    "\n",
    "\n",
    "3. Altogether then, `np.power(theta[:,1:theta.shape[1]], 2))` squares every $\\theta$ value from index $1$ to index $n$.\n",
    "\n",
    "\n",
    "4. `np.sum(np.power(theta[:,1:theta.shape[1]], 2))` sums each squared valued of $\\theta$ resulting from step (3) above.\n",
    "\n",
    "\n",
    "5. `learningRate` is $\\lambda$.  This is a hyperparameter, i.e. a configuration external to the model and whose value cannot be estimated from the data, and is often tuned by the human.  It controls the effectiveness of the regularization term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Regularizing the Gradient Function\n",
    "\n",
    "Now we need to add regularization to the gradient function.  Recall that it looks like this:\n",
    "\n",
    "\n",
    "Plugging the above into the gradient descent algorithm looks like this:\n",
    "\n",
    "\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\newline \\; & \n",
    "\\cdots  \\newline \\; & \n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j & \\newline \n",
    "\\rbrace \\end{align*}\n",
    "\n",
    "Note that the first $\\theta_0$ is left unprocessed.  Rearranging the above can make it simpler:\n",
    "\n",
    "\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\newline \\; & \n",
    "\\cdots  \\newline \\; & \n",
    "\\theta_j := \\theta_j(1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} & \\newline \n",
    "\\rbrace \\end{align*}\n",
    "\n",
    "This is implemented at lines 16 - 19 below (and also the above lines where they calculate the constituent ingredients obviously!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientReg(theta, X, y, learningRate):\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    grad = np.zeros(parameters)\n",
    "    \n",
    "    error = sigmoid(X * theta.T) - y\n",
    "    \n",
    "    for i in range(parameters):\n",
    "        term = np.multiply(error, X[:,i])\n",
    "        \n",
    "        # As with the regularised cost function above, by convention we similarly don't process x_0 when applying\n",
    "        # regularised gradient descent\n",
    "        if (i == 0):\n",
    "            grad[i] = np.sum(term) / len(X)\n",
    "        else:\n",
    "            grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i])\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Initialise the Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our learning rate to a sensible value.  We can play with this later if necessary (i.e. if the penalization is too strong or not strong enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 - Running our Regularized functions!\n",
    "\n",
    "Now let's try calling our new regularized functions with the default (0) values for theta to make sure the calculations are working.\n",
    "\n",
    "## (a) Running the Regularized Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costReg(theta2, X2, y2, learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Running the Regularized Gradient Descent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientReg(theta2, X2, y2, learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 - Optimising our Functions\n",
    "\n",
    "Now we can use the same optimization function from part 1 to compute the optimal solution for all $theta$ values, i.e. from $\\theta_0$ to $\\theta_{28}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = opt.fmin_tnc(func=costReg, x0=theta2, fprime=gradientReg, args=(X2, y2, learningRate))\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9 - Predicting our $y$ values and scoring accuracy!\n",
    "\n",
    "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_min = np.matrix(result2[0])\n",
    "predictions = predict(theta_min, X2)\n",
    "correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\n",
    "accuracy = (sum(map(int, correct)) % len(correct))\n",
    "\n",
    "print(\"accuracy = {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 - Using scikit-learn\n",
    "\n",
    "Although we implemented these algorithms from scratch, it's worth noting that we could also use a high-level python library like scikit-learn to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression(penalty='l2', C=1.0)\n",
    "model.fit(X2, y2.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score(X2, y2)\n",
    "print(\"accuracy = {:.0%}%\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is much lower than what we just computed, but keep in mind this result is using the default parameters provided by scikit-learn.  We'd likely need to do some parameter tuning to get the same accuracy that we obtained with our earlier result.\n",
    "\n",
    "That's all for Exercise 2!  Stay tuned for the next exercise where we'll tackle multi-class image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General intro, overview and explanation of Logistic Regression:\n",
    "\n",
    "- https://www.internalpointers.com/post/introduction-classification-and-logistic-regression\n",
    "    \n",
    "Detailed explanation of the Logistic Regression Cost Function:\n",
    "    \n",
    "- https://www.internalpointers.com/post/cost-function-logistic-regression\n",
    "\n",
    "Detailed explanation of Regularisation for Logistic Regression:\n",
    "\n",
    "- https://www.internalpointers.com/post/problem-overfitting-machine-learning-algorithms \n",
    "- http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/ \n",
    "\n",
    "Detailed general overview:\n",
    "\n",
    "- https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n",
    "- https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102\n",
    "\n",
    "Useful explainer when results go awry due to output of `X * theta.T` produces either very large or very small values, which results in NaN or Infinity values, and errors the code:\n",
    "\n",
    "- https://stackoverflow.com/questions/35419882/cost-function-in-logistic-regression-gives-nan-as-a-result\n",
    "\n",
    "Another python example\n",
    "\n",
    "- https://medium.com/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-1-1a666f049ad6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
