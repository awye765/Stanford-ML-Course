{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1. What is a neuron?\n",
    "\n",
    "A neuron is the brain's <b>basic computational unit</b>.  \n",
    "\n",
    "It receives and integrates chemical signals from other neurons via <b>dendrites</b> and, depending on a number of factors, it either does nothing or generates an electrical signal output via <b>axons</b>, which in turn signals other connected neurons via synapses:\n",
    "\n",
    "<img src=\"../Images/neuralnetworks7.PNG\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "![](images/neuralnetworks7.PNG)\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\neuralnetwork3.PNG\" width=100%>\n",
    "</p>\n",
    "\n",
    "Source: https://becominghuman.ai/making-a-simple-neural-network-2ea1de81ec20\n",
    "\n",
    "## 1.2. What is a neural network?\n",
    "\n",
    "A neural network is <b>simply a network of neurons</b>.\n",
    "\n",
    "## 1.3. How do we represent a neural network algorithmically?\n",
    "\n",
    "### 1.3.1. A simple neural network\n",
    "\n",
    "The below is a simple neural network.  Note x<sub>0</sub> is our \"<b>bias unit</b>\" and always equal to 1.  In neural networks we use the same logistic function as in logistic regression, i.e. the sigmoid (logistic) activation function described below:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=../Images\\neuralnetworks4.PNG width=100%>\n",
    "</p>\n",
    "\n",
    "In the above, the input layer (layer 1) feeds into the other layer (layer 2), which finally outputs the hypothesis function, known as the \"<b>Output Layer</b>\".\n",
    "\n",
    "### 1.3.2. A more complex neural network\n",
    "\n",
    "The below is a more complex neural network, and includes a \"<b>Hidden Layer</b>\":\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=../Images\\neuralnetworks5.PNG width=100%>\n",
    "</p>\n",
    "\n",
    "Explanation of Notation:\n",
    "\n",
    "1. The <b>input</b> layer is denoted by x<sub>0</sub>, x<sub>1</sub>, x<sub>2</sub>, and x<sub>3</sub>.\n",
    "\n",
    "2. The <b>hidden</b> layer is denoted by a<sub>1</sub><sup>2</sup>, a<sub>2</sub><sup>2</sup> and a<sub>3</sub><sup>2</sup>.  Hidden layers have the following characteristics:\n",
    "\n",
    "    (a) Intermediate layer(s) of nodes between the input and output layers.\n",
    "\n",
    "    (b) Cannot observe the values processed in the hidden layer(s).\n",
    "\n",
    "    (c\\) Can have many hidden layers.\n",
    "\n",
    "3. The <b>output</b> layer is simply hƟ(x).\n",
    "\n",
    "# 2. Neural Networks Notation\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=../Images\\neuralnetworks6.PNG width=100%>\n",
    "</p>\n",
    "\n",
    "The notation for neural networks can get confusing. It is explained as follows:\n",
    "\n",
    "1. a<sub>i</sub><sup>(j)</sup> = activation unit i in layer j.\n",
    "\n",
    "    <b>Example:</b> a<sub>1</sub><sup>(2)</sup> = activation unit 1 in the second layer.\n",
    "\n",
    "2. Ɵ<sup>(j)</sup> = matrix of parameters controlling function mapping from layer j to layer j+1.\n",
    "\n",
    "    <b>Example:</b>\n",
    "\n",
    "    (a) If network has s<sub>j</sub> units in layer j; and\n",
    "\n",
    "    (b) If network has s<sub>j+1</sub> units in layer j+1\n",
    "\n",
    "    (c\\) Then Ɵ<sup>(j)</sup> will be of dimensions s<sub>j+1</sub> x s<sub>j</sub> + 1, because:\n",
    "\n",
    "    (i) s<sub>j+1</sub> is equal to the number of units in layer (j + 1);\n",
    "\n",
    "    and\n",
    "\n",
    "    (ii) s<sub>j</sub> + 1 is equal to the number of units in layer j, plus an additional unit to represent the bias unit, which is always 1.\n",
    "\n",
    "3. For the Ɵ matrix:\n",
    "\n",
    "    (a) <b>Row length</b> = number of units in the <b>following</b> layer.\n",
    "\n",
    "    (b) <b>Column length</b> = number of units in the <b>current</b> layer + 1 (because of the bias unit).\n",
    "\n",
    "4. Example of the Ɵ matrix:\n",
    "\n",
    "    (a) Layer 1 = 3 input nodes, i.e. s<sub>j</sub> = 3\n",
    "\n",
    "    (b) Layer 2 = 3 activation nodes, i.e. s<sub>j+1</sub> = 3\n",
    "\n",
    "    (c\\) Dimension of Ɵ<sup>1</sup> is 3 x 4, i.e. s<sub>j+1</sub> x (s<sub>j</sub> + 1)\n",
    "\n",
    "5. We have to compute the activation for each node, which depends on:\n",
    "\n",
    "    (a) the input(s) to the node;\n",
    "\n",
    "    AND\n",
    "\n",
    "    (b) the parameter associated with that node (from the Ɵ vector associated with that layer).\n",
    "\n",
    "6. Therefore, we calculate each of the layer 2 activations in the above example based on the input values plus the bias term (which is equal to 1).\n",
    "\n",
    "7. The activation value of each layer 2 node is equal to the <b>sigmoid function applied to the linear combination of inputs</b>.\n",
    "\n",
    "8. For the <b>three input units</b> (x<sub>1</sub>, x<sub>2</sub> and x<sub>3</sub>):\n",
    "\n",
    "    (a) Ɵ<sup>(1)</sup> is the matrix of parameters governing the mapping of the input layer to the hidden layer.\n",
    "\n",
    "    (b) Ɵ<sup>(1)</sup> here is a 3 x 4 matrix.\n",
    "\n",
    "9.  For the <b>three hidden units</b> (a<sub>1</sub><sup>(2)</sup>, a<sub>2</sub><sup>(2)</sup> and a<sub>3</sub><sup>(2)</sup>):\n",
    "\n",
    "    (a) Ɵ<sup>(2)</sup> is the matrix of parameters governing the mapping of the input units to hidden units.\n",
    "\n",
    "    (b) Ɵ<sup>(2)</sup> here is a 1 x 4 matrix (i.e. row vector).\n",
    "\n",
    "10. With regard to the Ɵ notation, this can be a little confusing as not explained in the videos very clearly.  Concretely, this comes together as follows:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=../Images\\neuralnetwork10.PNG width=100%>\n",
    "</p>\n",
    "\n",
    "## Why can't we simply use linear or logistic regression?\n",
    "\n",
    "Even for a relatively simple problem such as predicting house prices, if there are 100 features it may become incredibly complicated to fit a linear or logistic function to the data.  \n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=../Images\\neuralnetworks1.PNG width=100%>\n",
    "</p>\n",
    "\n",
    "## Why do we use neural networks?\n",
    "\n",
    "For instance, a computer vision problem to identify cars from non-cars:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=../Images\\neuralnetworks2.PNG width=100%>\n",
    "</p>\n",
    "\n",
    "In this type of problem, the number of features is huge.  In such circumstances, there is no simple way to build decent classifiers.  Instead, we need a non-linear hypothesis to separate the classes.  Neural networks perform much better in this regard than simply logistic regression.\n",
    "\n",
    "<b>Explanation re Quadratic Terms:</b>\n",
    "\n",
    "1. The ~3,000,000 quadratic features figure is the number of unique pairs of x<sub>i</sub> and x<sub>j</sub>.\n",
    "\n",
    "2. This can be calculated as follows: n(n+1)/2.\n",
    "\n",
    "3. Therefore, this equates to: 2,500 x (2501 + 1) / 2.\n",
    "\n",
    "4. 6,252,500 / 2 = 3,126,250.\n",
    "\n",
    "See also here:\n",
    "- https://www.coursera.org/learn/machine-learning/discussions/weeks/4/threads/zWn1oshFEeWRfg5WVr61Uw\n",
    "\n",
    "- https://www.mathsisfun.com/combinatorics/combinations-permutations.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
