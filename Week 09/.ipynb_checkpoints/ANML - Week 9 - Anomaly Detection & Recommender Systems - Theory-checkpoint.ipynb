{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Week 9 is in two parts:\n",
    "\n",
    "1. Part 1 is about <b>anomaly detection</b>.  Anomaly detection is another unsupervised technique, principally used to detect anomalous data, e.g.\n",
    "\n",
    "* Manufacturing defects\n",
    "\n",
    "\n",
    "* Fraud detection\n",
    "\n",
    "\n",
    "* Unusual user behaviour, e.g. terrorism\n",
    "\n",
    "\n",
    "2. Part 2 is about recommender systems, i.e. systems capable of recommending certain items or actions to users.  For instance, recommending movies to a user based on tastes of similar users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Anomaly Detection\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intuition\n",
    "\n",
    "For example, a system designed to detect anomalies in aircraft engines where:\n",
    "\n",
    "1. the dataset includes two features:\n",
    "\n",
    "    (a) $x_1$ = heat generated; and\n",
    "\n",
    "    (b) $x_2$ = vibration intensity.\n",
    "    \n",
    "\n",
    "2. The dataset includes $m$ examples.\n",
    "\n",
    "\n",
    "3. Given a new engine, $x_{test}$ what is the probability that it is anomalous, i.e. we determine some boundary ($\\epsilon$) outside of which there is a high probability the engine is anomalous: \n",
    "\n",
    "    (a) $p(x_{test}) \\leq \\epsilon$, flag as anomalous; or\n",
    "    \n",
    "    (b) $p(x_{test}) \\geq \\epsilon$, flag as OK.\n",
    "    \n",
    "    \n",
    "4. If the anomaly detection system:\n",
    "\n",
    "    (a) flags too many things as anomalous that are actually OK, try decreasing $\\epsilon$; or\n",
    "    \n",
    "    (b) flags too few things as anomalous that are actually anomalous, try increasing $\\epsilon$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gaussian Distribution\n",
    "\n",
    "## 2.1. What is it?\n",
    "\n",
    "In probability theory, <b>Gaussian</b> distribution is:\n",
    "\n",
    "1. A very common continuous probability distribution, i.e. a description of how likely a random variable (i.e. a variable whose outcome is the product of random phenomenon) will have a particular value or fall within a range of values.  For instance, a Gaussian distribution has the following helpful properties:\n",
    "\n",
    "    (a) 68% of data will be within $\\pm$ 1 standard deviation $\\sigma$ from the mean $\\mu$.\n",
    "    \n",
    "    (b) 95% of data will be within $\\pm$ 2 standard deviations $\\sigma$ from the mean $\\mu$.\n",
    "    \n",
    "    (c) 99.7% of data will be within $\\pm$ 3 standard deviations $\\sigma$ from the mean $\\mu$\n",
    "\n",
    "\n",
    "2. Identified by its iconic bell shaped curve.  It is also known as the <b>Normal</b> distribution.  \n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Gaussian.PNG\" width=\"80%\"/>\n",
    "</p>\n",
    "\n",
    "3. Described mathematically as follows:\n",
    "\n",
    "\\begin{align}\n",
    "x^{(i)} \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "\\end{align}\n",
    "\n",
    "4. Per the above, this means:\n",
    "\n",
    "    (a) The $x$ variables are distributed ($\\sim$);\n",
    "    \n",
    "    (b) According to some parameter (i.e. it is parameterized by) $\\mu$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Mathematical Formula for Gaussian Distribution\n",
    "\n",
    "### 2.2.1. Probability Density Function\n",
    "\n",
    "The mathematical equation to plot the Gaussian distribution function is as follows:\n",
    "\n",
    "\\begin{align}\n",
    "p(x; \\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-(x-\\mu)^2 \\text / \\ 2\\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\n",
    "1. $p(x;..)$ mean the probability that $x$ has a certain value based on a Gaussian distribution.\n",
    "\n",
    "\n",
    "2. $p(...; \\mu, \\sigma^2)$ refers to that probability being parameterized by $\\mu$ and $\\sigma$.\n",
    "\n",
    "\n",
    "3. $x$ is the continuous random variable.\n",
    "\n",
    "\n",
    "4. $e$ is the exponent.\n",
    "\n",
    "\n",
    "5. $\\mu$ is the mean of random variable $x$.  \n",
    "\n",
    "\n",
    "6. $\\sigma$ is the standard deviation of random variable $x$, i.e. specifying the width of the Gaussian probability density.\n",
    "\n",
    "\n",
    "7. $\\sigma^2$ is the variance of random variable $x$.  \n",
    "\n",
    "With regard to (5) and (6), see below for how these are identified on a typical Gaussian distribution curve:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Gaussian2.PNG\" width=\"50%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Estimating Parameters\n",
    "\n",
    "To estimate the parameters for Gaussian distribution, i.e. $\\mu$ and $\\sigma^2$ are as follows:\n",
    "\n",
    "* To calculate $\\mu$ use this formula: $\\mu = \\frac{1}{m}\\sum_{i = 1}^m x^{(i)}$\n",
    "\n",
    "\n",
    "* To calculate $\\sigma^2$ use this formula: $\\sigma^2 = \\frac{1}{m}\\sum_{i = 1}^{(i)}(x^{(i)} - \\mu)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gaussian Distribution & Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. The Anomaly Detection Algorithm\n",
    "\n",
    "Gaussian distribution can be used to detect anomalies.  The necessary algorithm is as follows:\n",
    "\n",
    "1. Choose features $x_j$ that you think might be indicative of anomalous examples, i.e. if unusually large or unusually small.\n",
    "\n",
    "\n",
    "2. Acquire an unlablled training set of examples {$x^{(1)}, ... x^{(m)}$}, where each sample $x^{(i)}$ is represented by a feature vector of features $x_j$..\n",
    "\n",
    "\n",
    "3. Fit to each feature parameters $\\mu_1,... \\mu_n$ and $\\sigma_1^2, ... \\sigma_n^2$, which can be calculated using the below formulas respectively:\n",
    "\n",
    "    (a) $\\mu_j = \\frac{1}{m}\\sum_{i = 1}^m x_j^{(i)}$, i.e. average value of the each feature.\n",
    "\n",
    "    (b) $\\sigma_j^2 = \\frac{1}{m}\\sum_{i = 1}^{(i)}(x_j^{(i)} - \\mu_j)^2$, i.e. variance of each feature.\n",
    "    \n",
    "    In other words, calculate the mean ($\\mu$) and variance ($\\sigma^2$) for each feature according to a Gaussian distribution.\n",
    "\n",
    "\n",
    "4. Given a new example $x$, we use Gaussian distribution to model the probability that each value will have a high or low value, which in turn helps us understand what is and what is not anomalous.  \n",
    "\n",
    "\n",
    "5. Mathematically (4) is described as: $p(x)$, i.e.\n",
    "\n",
    "\\begin{align}\n",
    "p(x) = p(x_1; \\mu_1, \\sigma_1^2) \\cdot p(x_2; \\mu_2, \\sigma_2^2)... p(x_n; \\mu_n, \\sigma_n^2)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "6. And this can be simplified to:\n",
    "\n",
    "\\begin{align}\n",
    "p(x) = \\Pi_{j = 1}^n p(x_j; \\mu_j, \\sigma_j^2) = \\Pi_{j = 1}^n \\frac{1}{\\sqrt{2\\pi\\sigma_j}} exp (-\\frac{x_j - \\mu_j)^2}{2\\sigma_j^2})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "7. Anomaly arises if $p(x) < \\epsilon$, i.e. if the example's features are anomalously distributed vs. a Gaussian distribution of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. An Example\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Gaussian3.PNG\" width=\"80%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Choosing $\\epsilon$\n",
    "\n",
    "Evaluate algorithm on the cross validation set to identify the ideal value of $\\epsilon$, i.e. similar to how we used cross validation to identify the optimal models and $\\lambda$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Evaluating an Anomaly Detection System\n",
    "\n",
    "### 3.4.1. Theory\n",
    "\n",
    "To evaluate an anomaly detection system (\"<b>ADS</b>\") we need:\n",
    "\n",
    "1. <b>A labelled dataset:</b> a dataset where the anomalous and non-anomalous data is labelled, e.g. $y = 0$ if normal and $y = 1$ if anomalous.\n",
    "\n",
    "\n",
    "2. <b>A training set:</b> a large collection of non-anomalous unlabelled examples.\n",
    "\n",
    "\n",
    "3. <b>A cross validation set:</b> including some that are <b>known</b> to be anomalous.\n",
    "\n",
    "\n",
    "4. <b>A test set:</b> including some that are <b>known</b> to be anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Example\n",
    "\n",
    "1. Dataset of 10,020 engine examples whereby:\n",
    "\n",
    "    (a) 10,000 are good, i.e. normal, engines; and\n",
    "    \n",
    "    (b) 20 are flawed, i.e. anomalous, engines.\n",
    "    \n",
    "    \n",
    "2. Training set: 6,000 good engines.\n",
    "\n",
    "\n",
    "3. Cross Validation set: \n",
    "\n",
    "    (a) 2,000 good engines; and\n",
    "    \n",
    "    (b) 10 anomalous engines.\n",
    "\n",
    "\n",
    "4. Test set: \n",
    "\n",
    "    (a) 2,000 good engines; and\n",
    "    \n",
    "    (b) 10 anomalous engines.\n",
    "    \n",
    "To evaluate we would then compare the predicted $y$ labels from the test set and cross-validation set, against their actual $y$ labels.  \n",
    "\n",
    "Evaluation metrics would include those covered earlier:\n",
    "\n",
    "* Confusion Matrix analysis\n",
    "\n",
    "\n",
    "* Precision / Recall\n",
    "\n",
    "\n",
    "* $F_1$ score\n",
    "\n",
    "Again, like before, simply evaluating based on \"classification accuracy\" (i.e. how many examples were correctly classified as a % of the total number of samples\") is unsufficient given it is easily affected by a skewed dataset, such as will be the case in most anomaly detection problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Anomaly Detection vs. Supervised Learning\n",
    "\n",
    "### 3.5.1. Suitability\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\AnomSupLearningComp.PNG\" width=\"80%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2. Examples of each\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\AnomSupLearningComp2.PNG\" width=\"80%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Choosing features\n",
    "\n",
    "### 3.6.1. The Objective\n",
    "\n",
    "In anomaly detection we want:\n",
    "\n",
    "* $p(x)$ to be <b>large</b> for normal examples $x$; and\n",
    "\n",
    "\n",
    "* $p(x)$ to be <b>small</b> for anomalous examples $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2. The Issue\n",
    "\n",
    "A common problem arises when $p(x)$ is comparable (e.g. both large) for both normal and anomalous examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.3. The Solution\n",
    "\n",
    "Use error analysis to identify the mistakes being made by the algorithm and identify additional features about the example and use that to make it easier to distinguish it from normal examples:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\AnomErrorAnalysis.PNG\" width=\"80%\"/>\n",
    "</p>\n",
    "\n",
    "In the above, Andrew suggests creating a new features:\n",
    "\n",
    "* $x_5$ equal to CPU Load / network traffic; and\n",
    "\n",
    "\n",
    "* $x_6$ equal to CPU Load$^2$ / network traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.4. When features don't have Gaussian distribution\n",
    "\n",
    "Before doing the above it's worth plotting a histogram of our data and checking for a bell-shaped curve with regard to each feature.  \n",
    "\n",
    "If a bell-shaped curve does not exist for one or more features then we need to play with out data for such features to <b>transform</b> it to a bell-shaped curve.  For instance, by trying the following on each offending feature:\n",
    "\n",
    "1. $log(x)$\n",
    "\n",
    "\n",
    "2. $log(x + 1)$\n",
    "\n",
    "\n",
    "3. $log(x + c)$, where $c$ is a constant\n",
    "\n",
    "\n",
    "4. $\\sqrt{x}$\n",
    "\n",
    "\n",
    "5. $x^{1/3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Recommender Systems\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Problem Statement\n",
    "\n",
    "## 1.1. Generally\n",
    "\n",
    "To generate a learning algorithm capable of recommending items or actions based on similar user behaviour.  Typically used by:\n",
    "\n",
    "* Online retailers, e.g. Amazon;\n",
    "\n",
    "\n",
    "* Online enterainment providers, e.g. Netflix and Amazon Prime;\n",
    "\n",
    "\n",
    "* Search engines, e.g. Google.\n",
    "\n",
    "Part 2 explores <b>three</b> methods:\n",
    "\n",
    "1. <b>Content Based Recommendations:</b> where we have features that capture the content (e.g. genres of movies) to predict dependent variables (e.g. ratings of movies).\n",
    "\n",
    "\n",
    "2. <b>Collaborative Filtering:</b> where we have, or can estimate parameters and use these to learn the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Example\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Recommender1.PNG\" width=\"90%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Content Based Recommendations\n",
    "\n",
    "The first approach to recommender systems is <b>content based recommendations</b>. It requires that the dataset includes some information regarding the content of the data, which we assume will impact a dependent variable.\n",
    "\n",
    "## 2.1. Example: Movie Recommendations cont\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Recommender2.PNG\" width=\"90%\"/>\n",
    "</p>\n",
    "\n",
    "<b>Note:</b> the vector $\\theta^{(1)}$ in Andrew's diagram is assumed to result from the algorithm already having arrived at those parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Notation\n",
    "\n",
    "In the above:\n",
    "\n",
    "1. $r(i, j) = 1$ if user $j$ has rated movie $i$; 0 if otherwise.\n",
    "\n",
    "\n",
    "2. $y^{(i, j)} =$ rating by user $j$ on movie $i$ (if defined).\n",
    "\n",
    "\n",
    "3. $\\theta^{(j)} =$ parameter vector for user $j$.\n",
    "\n",
    "\n",
    "4. $x^{(i)} =$ feature vector for movie $i$.\n",
    "\n",
    "\n",
    "5. For user $j$, movie $i$, the predicted rating is: $\\theta$ for $x^{(3)}$.\n",
    "\n",
    "\n",
    "6. $m^{(j)} = $ no. of movies rated by user $j$.\n",
    "\n",
    "\n",
    "7. $n =$ number of features per movie.\n",
    "\n",
    "\n",
    "8. $n_u =$ number of users.\n",
    "\n",
    "\n",
    "9. $n_m =$ number of movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Cost Function\n",
    "\n",
    "1. <b>Objective, per user:</b> to learn $\\theta^{(j)}$, i.e. the parameters vector that maps a movie's features to a predicted rating of a <i>particular</i> user for that movie.  \n",
    "\n",
    "    Mathematically we achieve this on a <i>per user</i> basis using the below formula, which is the familiar linear regression function:\n",
    "\n",
    "\\begin{align}\n",
    "min_{\\theta^{(j)}} = \\frac{1}{2} \\sum_{i:r(i,j) = 1}((\\theta^{(j)})^T  (x^{(i)}) - y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum_{k=1}^n (\\theta_k^{(j)})^2\n",
    "\\end{align}\n",
    "\n",
    "2. The base of the first summation in the above is choosing all movies $i$ such that $r(i, j) = 1$, i.e. all movies where that user has provided a rating.\n",
    "\n",
    "\n",
    "3. <b>Objective, all users:</b> to learn $\\theta^{(1)}, \\theta^{(2)}, ... \\theta^{n_u}$, i.e. the parameters vector that maps a movie's features to a predicted rating of each user for that movie.\n",
    "\n",
    "    Mathematically we achieve this on an <i>all user</i> basis using the below formula:\n",
    "    \n",
    "\\begin{align}\n",
    "min_{\\theta^{(1)},...\\theta^{(n_u)}} = \n",
    "\\frac{1}{2} \n",
    "\\sum_{j = 1}^{n_u}\n",
    "\\sum_{i:r(i,j) = 1}((\\theta^{(j)})^T  (x^{(i)}) - y^{(i,j)})^2 \n",
    "+ \\frac{\\lambda}{2}\n",
    "\\sum_{j = 1}^{n_u}\n",
    "\\sum_{k=1}^n (\\theta_k^{(j)})^2\n",
    "\\end{align}\n",
    "\n",
    "4. The above is are the familiar linear regression cost function. The only difference is that we eliminate the constant $\\frac{1}{m}$ by multiplying everything by $m$ as it makes no difference to the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Gradient Descent\n",
    "\n",
    "As with linear regression, we can apply gradient descent to update the parameters:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\RecommenderGradientDescent.PNG\" width=\"90%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Collaborative Filtering\n",
    "\n",
    "Unlike content based recommendations described above, we do not require data about its content. For instance if we have movie titles and (incomplete, i.e. where a user has not yet rated a movie) ratings but no features regarding the movie content.\n",
    "\n",
    "Collaborative filtering has an interesting property, namely that it does what is called <b>feature learning</b> using randomly initialised parameters and feature values.  \n",
    "\n",
    "It can therefore simultaneously learn features and parameters!  In doing so we can predict which ratings users will give to movies they are yet to view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Learning Features from Users\n",
    "\n",
    "### 3.1.1. Example\n",
    "\n",
    "We will work with the movie ratings example again:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\CollabFilt1.PNG\" width=\"90%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high level idea for collaborative filtering is to learn the features using the known data, i.e. in the above these are the known ratings by each user for given films.\n",
    "\n",
    "In the above, based on the known ratings for each user, we'd expect our algorithm to say:\n",
    "\n",
    "* That $\\theta^{(1)}$ with respect to Alice, $j_1$, should heavily weight romantic movies, hence $\\theta_1^{(1)} = 5$.\n",
    "\n",
    "\n",
    "* That $\\theta^{(2)}$ with respect to Bob, $j_2$, should also heavily weight romantic movies, hence $\\theta_1^{(1)} = 5$.\n",
    "\n",
    "\n",
    "* That $\\theta^{(3)}$ with respect to Carol, $j_3$, should heavily weight action movies, hence $\\theta_1^{(1)} = 0$\n",
    "\n",
    "\n",
    "* That $\\theta^{(4)}$ with respect to Dave, $j_4$, should heavily weight action movies, hence $\\theta_1^{(1)} = 0$\n",
    "\n",
    "\n",
    "* The above is based on working back as shown in the bottom right corner, e.g. $(\\theta^{(1)})^Tx^{(1)} \\approx 5$, $(\\theta^{(2)})^Tx^{(1)} \\approx 5$, $(\\theta^{(3)})^Tx^{(1)} \\approx 0$ and $(\\theta^{(4)})^Tx^{(1)} \\approx 0$.  In other words the expected $\\theta$ values for each user / movie should output a number approximately equal to the known movie rating for the given user / movie combo.\n",
    "\n",
    "\n",
    "* Therefore we'd expect $x_1^{(1)} = 1.0$ and $x_2^{(1)} = 0$.  Remember that $x_0 = 0$ as before with linear regression, logistic regression and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Cost Function\n",
    "\n",
    "The cost functions for feature learning are below:\n",
    "\n",
    "#### 3.2.1.1. Per Sample\n",
    "\n",
    "\\begin{align}\n",
    "min_{x^{(i)}} =\n",
    "\\frac{1}{2} \n",
    "\\sum_{j:r(i,j) = 1}\n",
    "((\\theta^{(j)})^T  (x^{(i)}) - y^{(i,j)})^2 \n",
    "+ \\frac{\\lambda}{2}\\sum_{k=1}^n (x_k^{(i)})^2\n",
    "\\end{align}\n",
    "\n",
    "In the above movie example, this tries to choose features that would predict something very close to the actual rating supplied by the user for a <b>particular</b> movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1.2. All Samples\n",
    "\n",
    "\\begin{align}\n",
    "min_{x^{(1)},...x^{(n_m)}} = \n",
    "\\frac{1}{2} \n",
    "\\sum_{i = 1}^{n_m}\n",
    "\\sum_{j:r(i,j) = 1}\n",
    "((\\theta^{(j)})^T  (x^{(i)}) - y^{(i,j)})^2 \n",
    "+ \\frac{\\lambda}{2}\n",
    "\\sum_{i = 1}^{n_m}\n",
    "\\sum_{k=1}^n (x_k^{(i)})^2\n",
    "\\end{align}\n",
    "\n",
    "In the above movie example, this tries to choose features that would predict something very close to the actual rating supplied by the users for <b>all</b> movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Collaborative Filtering Algorithm\n",
    "\n",
    "From section 2. we saw how to learn ratings from features and from section 3.1 we've seen how it is possible to learn the features based on ratings.  \n",
    "\n",
    "Collaborative Filtering allows us to combine these objectives, i.e. the cost function for $\\theta$ and the cost function for $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Optimization Objective\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\CollabFilt4.PNG\" width=\"90%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialise $x^{(i)}, ... x^{(n_m)}$ and $\\theta^{(1)},...\\theta^{(n_u)}$ to random values. This serves to break symmetry and ensures that the algorith learns features $x^{(i)},...x^{(n_m)}$ that are different from each other.\n",
    "\n",
    "\n",
    "2. Minimise $J(x^{(i)},...,x^{(n_m)}, \\theta^{(1)},...\\theta^{(n_u)})$, i.e. the above cost function, using gradient descent.\n",
    "\n",
    "\n",
    "3. For a user with parameters $\\theta$ and a movie with (learned) features $x$, predict a star rating of $\\theta^T x$.\n",
    "\n",
    "<b>Note:</b> because the algorithm can learn features by itself, the bias units where $x_0 = 1$ have been removed, therefore $x$ and $\\theta$ are $n$ dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Vectorised Collaborative Filtering\n",
    "\n",
    "Collaborative filtering is also known as \"<b>low rank matrix factorization</b> when the algorithm is implemented in vectorised form.\n",
    "\n",
    "To vectorize the above we need to do the following:\n",
    "\n",
    "1. Store the known movie ratings in a matrix $Y$.\n",
    "\n",
    "\n",
    "2. Store the predicted movie ratings of each user for each film, i.e. $(\\theta^{(1)})^T(x^{(1)})$, $(\\theta^{(1)})^T(x^{(1)})$, ... $(\\theta^{(n_u)})^T(x^{(1)})$ as $m$ rows in a matrix.  Like so:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\CollabFilt3.PNG\" width=\"90%\"/>\n",
    "</p>\n",
    "\n",
    "3. A simpler way of representing (2) is to split it into two separate matrices:\n",
    "\n",
    "    (a) $X$ = a matrix where each row represents the features for each movie; and\n",
    "    \n",
    "    (b) $\\theta$ = a matrix where each row represents the parameters for those features for a given user.\n",
    "    \n",
    "    \n",
    "4. Therefore the full matrix $Y$ of all predicted ratings of all movies by all users is given simply by: $Y = X\\Theta^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Finding Related Movies\n",
    "\n",
    "* For each film $i$, we learn a feature vector $x^{(i)}$.  \n",
    "\n",
    "\n",
    "* Using this information we can find movie vectors with the smallest distance between them to find related movies.\n",
    "\n",
    "\n",
    "* For instance, to find the 5 most similar movies $j$ to movie $i$ you look for the smallest distances between $\\|x^{(i)} - x^{(j)}\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Mean Normalization\n",
    "\n",
    "### 3.5.1. The issue\n",
    "\n",
    "If the ranking system for movies is used with regard to new users, then then new users (who have watched no movies), will be assigned new movies incorrectly. \n",
    "\n",
    "Specifically, they will be assigned $\\theta$ with all components equal to $0$ due to the minimization of the regularization term. That is, we assume that the new user will rank all movies $0$, which does not seem intuitively correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2. The Solution\n",
    "\n",
    "We rectify this problem by <b>normalizing</b> the data relative to the mean $\\mu$. \n",
    "\n",
    "We implement this as follows:\n",
    "\n",
    "1. We use a matrix $Y$ to store the data from previous ratings, where the $i^{th}$ row of $Y$ is the ratings for the $i^{th}$ movie and the $j^{th}$ column corresponds to the ratings for the $j^{th}$ user.\n",
    "\n",
    "\n",
    "2. We can now define a vector: $\\mu = \\big[ \\mu_1, \\mu_2,...\\mu_{n_m} \\big]$\n",
    "\n",
    "\n",
    "3. Such that: $\\mu_i = \\frac{\\sum_{j:r(i,j)=1} Y_{i, j}}{\\sum_j r(i, j)}$\n",
    "\n",
    "    Whereby (3) is effectively the mean of the previous ratings for the $i^{th}$ movie (where only moives that have been watched by users are counted, both regarding the numerator and denominator).  \n",
    "    \n",
    "\n",
    "4. We can normalize the data by subtracting $\\mu$, the mean rating, from the actual ratings for each user (column in matrix $Y$).\n",
    "    \n",
    "    \n",
    "5. Finally, we must slightly modify the linear regression prediction to include the mean normalization term: $(\\theta^{(j)})^Tx^{(i)} + \\mu_i$.  \n",
    "\n",
    "    <b>Note:</b> this is because we subtracted $\\mu$ when normalizing the data.\n",
    "\n",
    "\n",
    "6. Now, for a new user, the initial predicted values will be equal to the term $\\mu$ instead of simply being intialized to $0$, which is more accurate.\n",
    "\n",
    "    <b>Note:</b> the fact a new user has no ratings means the left side of $(\\theta^{(j)})^Tx^{(i)} + \\mu_i$ is cancelled out  (i.e. because it will be equal to $0$), which means the new user's predicted ratings will simply be the mean ratings for those movies $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3. An Example\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\CollabFiltMeanNorm.PNG\" width=\"90%\"/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
