{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Week 7 is all about Support Vector Machines (\"<b>SVM</b>\").  This is the final <b>supervised</b> learning algorithm explored in Andrew Ng's Introduction to Machine Learning course.  The previous supervised techniques were:\n",
    "\n",
    "* Linear Regression\n",
    "\n",
    "\n",
    "* Logistic Regression\n",
    "\n",
    "\n",
    "* Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Basics\n",
    "\n",
    "## 2.1. What is a SVM?\n",
    "\n",
    "SVMs:\n",
    "\n",
    "* Are supervised learning models for classification and regression problems.\n",
    "\n",
    "\n",
    "* Can solve linear and non-linear problems.\n",
    "\n",
    "\n",
    "* Are most commonly used in <b>classification</b> problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. When to use a SVM?\n",
    "\n",
    "SVMs can be used for:\n",
    "\n",
    "1. Regression problems; and\n",
    "\n",
    "\n",
    "2. Classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. How does a support vector machine work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM are based on the simple idea of finding a <b>hyperplane</b> that best divides a dataset into two classes.  \n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\SVMsimple.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "To better understand this, we need to first define some key SVM terminology:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. SVM Terminology\n",
    "\n",
    "* High level SVM components:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Hyperplanes.PNG\" width=\"60%\"/>\n",
    "</p>\n",
    "\n",
    "* <b>Hyperplane:</b> a multidimensional line (3+ dimensions) that separates and classifies a set of data.  By contrast, a <b>line</b> is a 2d line and a <b>plane</b> is a 3d surface.\n",
    "\n",
    "    Intuitively, the further from the hyperplane our data points the more confident we are that they have been correctly classified.  We therefore want our data points to be as far away from the hyperplane as possible, while still being on the correct side of it.\n",
    "    \n",
    "    \n",
    "\n",
    "* <b>Support Vector:</b> data points <b>nearest to the hyperplane</b> and are points that, if removed, would alter the position of the dividing hyperplane.  Because of this, the support vectors (\"<b>SV</b>\") can be considered critical elements of a dataset.  This is unlike linear regression and neural networks where <b>all</b> data points influence optimization.\n",
    "\n",
    "\n",
    "* <b>Margin:</b> the distance between the hyperplane and the nearest data point from either side.  The goal is to choose a hyperplane that is equidistant as far as possible for both sides, i.e. not too close to one class or the other:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Margin.PNG\" width=\"60%\"/>\n",
    "</p>\n",
    "\n",
    "* <b>Linear vs. Non-Linear Separable:</b> figure A shows separation possible with a linear plane (i.e. straight line) whereas figure B shows separation possible with a non-linear plane (i.e. bendy, in this case circular, line):\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\SVMLinSep.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "* <b>Gamma:</b> defines how far the influence of a single training example reaches, which low values meaning \"far\" and high values meaning \"close\".  In other words, with low gamma, data points far away from plausible hyperplanes are considered in calculation for the hyperplane.  Whereas high gamma means the data points close to the plausible hyperplanes are considered for the calculation.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\Gamma.PNG\" width=\"60%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. What if there is no clear hyperplane?\n",
    "\n",
    "In order to classify a messy (i.e. real world) dataset, we need to move away from a 2d view of the data to a 3d view.  \n",
    "\n",
    "For instance, imagine we are trying to classify green balls from blue balls but their distribution is like this:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\SVM2d.PNG\" width=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "We can imagine floating these balls in 3d space and using a sheet to separate them:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\SVM3d.PNG\" width=\"80%\"/>\n",
    "</p>\n",
    "\n",
    "This \"floating\" of the balls represents mapping the data from 2d to 3d, i.e. to a higher dimension.  This is known as \"<b>Kernelling</b>\".\n",
    "\n",
    "Once in 3d, the line can no longer be a line: it is a <b>plane</b>.  The idea is to then experiment with mapping the data to higher and higher dimensions until a hyperplane can be formed to adequately separate it into the correct classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. How is SVM's hyperplane different to linear classifiers (e.g. Logistic Regression)?\n",
    "\n",
    "Unlike other linear classifiers, e.g. Logistic Regression, the <i>motivation</i> of SVMs is to <b>maximise margin</b>.  In other words, an SVM tries to find the classifier whose decision boundary is furthest away from any data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Pros and Cons of SVMs\n",
    "\n",
    "### 2.3.1. Pros\n",
    "\n",
    "* Accuracy\n",
    "\n",
    "\n",
    "* Works well on smaller cleaner datasets\n",
    "\n",
    "\n",
    "* Can be more efficient because it uses a subset of training points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Cons\n",
    "\n",
    "* Is not suited to large datasets as the training time with SVMs can be high\n",
    "\n",
    "\n",
    "* Less effective on noisier datasets with overlapping classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. SVM Uses\n",
    "\n",
    "SVM is used for:\n",
    "\n",
    "* Text classification tasks such as category assignment, detecting spam and sentiment analysis. \n",
    "\n",
    "\n",
    "* Image recognition challenges, performing particularly well in aspect-based recognition and color-based classification. \n",
    "\n",
    "\n",
    "* Handwritten digit recognition, such as postal automation services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vectors (Brief) Recap\n",
    "\n",
    "## 3.1. What are vectors?\n",
    "\n",
    "A vector:\n",
    "\n",
    "* is an <b>n dimensional object</b>;\n",
    "\n",
    "\n",
    "* has <b>magnitude</b>, i.e. length; \n",
    "\n",
    "\n",
    "* has <b>amplitude</b>, i.e. direction; and\n",
    "\n",
    "\n",
    "* strts from the origin, (0, 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Components Explained\n",
    "\n",
    "### 3.2.1. Notation\n",
    "\n",
    "* Vectors usually identified by the letters of its <b>head</b> and <b>tail</b> like so: $\\overrightarrow{\\rm AB}$\n",
    "\n",
    "\n",
    "* Vector magnitude is denoted by: $\\| a \\|$.\n",
    "\n",
    "\n",
    "* Vector directon is denoted by: $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Calculations\n",
    "\n",
    "* The most common way is to first break up vectors into their constituent $x$ and $y$ parts, i.e. horiztonal and vertical vectors, like so:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"..\\Images\\vectorComponents.PNG\" width=\"20%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Magnitude (Length)\n",
    "\n",
    "* Magnitude or length of a vector is written as $\\| a \\|$, and calculated as follows: $\\|a \\| = \\sqrt{x^2, y^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Direction\n",
    "\n",
    "* Direction is written as: $\\theta = tan^{-1}\\begin{pmatrix}\\frac{y}{x}\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Operations Explained\n",
    "\n",
    "See [here](https://www.mathsisfun.com/algebra/vectors.html) via www.mathsisfun.co.uk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Useful Resources\n",
    "\n",
    "- https://www.mathsisfun.com/algebra/vectors.html\n",
    "\n",
    "\n",
    "- http://teachers.henrico.k12.va.us/math/ito_08/10AdditionalTrig/10LES4/vector_notate_n.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SVM Maths\n",
    "\n",
    "<i>To be covered</i> using [this article](https://www.jeremyjordan.me/support-vector-machines/) as its inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Useful Resources\n",
    "\n",
    "- https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html\n",
    "\n",
    "\n",
    "- https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners\n",
    "\n",
    "\n",
    "- https://maviccprp.github.io/a-support-vector-machine-in-just-a-few-lines-of-python-code/\n",
    "\n",
    "\n",
    "- https://medium.com/@LSchultebraucks/introduction-to-support-vector-machines-9f8161ae2fcb\n",
    "\n",
    "\n",
    "- http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf\n",
    "\n",
    "\n",
    "- https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72\n",
    "\n",
    "\n",
    "- https://www.jeremyjordan.me/support-vector-machines/\n",
    "\n",
    "\n",
    "- https://cling.csd.uwo.ca/cs860/papers/SVM_Explained.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
